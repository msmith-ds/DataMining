{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Classification and Regression -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 4/05/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Business Understanding\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for Lab 2 consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behavior.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this lab is to: 1)Fit clusters describing both customer and subscriber user types 2) Build a classification model for both customer and subscriber clusters 3) Predict customer clusters for subscriber observations and vice-versa providing a Classification-Ready dataset  for predicting whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, customer/subscriber cluster features, etc., and 4) provide a demonstration on how we would implement/deploy these clusters to a new data entry for use in a classification model real-time.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve these goals (as described further in the sections below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding 1\n",
    "***Describe the meaning and type of data***\n",
    "\n",
    "Before diving into each attribute in detail, one glaring facet of this data set that needs mentioning is its inherent time-series nature. By no means was this overlooked when we decided upon these particular data. To mitigate the effects of time on our analysis results, we have chosen to aggregate time-centric attributes such as dates and hours of the day by replacing them with simply the day of the week or period of the day (more on these details shortly). For example, by identifying trips occurring on July 1st, 2013, not by the date of occurrence but rather the day of the week, Monday, and identifying trips on July 2nd, 2013, as occurring on Tuesday, we will be able to obtain a \"big picture\" understanding of trends by day of the week instead of at the date-by-date level. We understand this is not a perfect solution since the time-series component is still an underlying factor in trip activity, but it is good enough to answer the types of questions we hope to target as described in the previous section as we will be comparing all Mondays against all Tuesdays, etc.\n",
    "\n",
    "As mentioned previously, the original data set ***from Citi Bike*** included 15 attributes. These 15 attributes and associated descriptions are provided below:\n",
    "1. **tripduration** - *Integer* - The total time (in seconds) a bike remains checked out, beginning with the start time and ending with the stop time\n",
    "2. **starttime** - *DateTime* - The date and time at which a bike was checked out, marking the start of a trip (i.e. 2/12/2014 8:16)\n",
    "3. **stoptime** - *DateTime* - The date and time at which a bike was checked back in, marking the end of a trip (i.e. 2/12/2014 8:16)\n",
    "4. **start_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station from which a bike is checked out\n",
    "5. **start_station_name** - *String* - The name of the station from which a bike is checked out; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "6. **start_station_latitude** - *Float* - The latitude coordinate for the station from which a bike is checked out (i.e. 40.74780373)\n",
    "7. **start_station_longitude** - *Float* - The longitude coordinate for the station from which a bike is checked out (i.e. -73.9900262)\n",
    "8. **end_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station in which a bike is checked in\n",
    "9. **end_station_name** - *String* - The name of the station at which a bike is checked in; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "10. **end_station_latitude** - *Float* - The latitude coordinate for the station at which a bike is checked in (i.e. 40.74780373)\n",
    "11. **end_station_longitude** - *Float* - The longitude coordinate for the station at which a bike is checked in (i.e. -73.9900262)\n",
    "12. **bikeid** - *String* - A categorical number value used to identify a particular bike; each bike in the bike share network has its own unique number\n",
    "13. **usertype** - *String* - A classifier attribute identifying a rider as a bike share subscriber or a one-time customer (i.e. Subscriber vs. Customer)\n",
    "14. **birth_year** - *Integer* - The year a rider was born (Only available for subscribed riders, however)\n",
    "15. **gender** - *String* - A categorical number value representing a rider's gender (i.e. 0 = unknown, 1 = male, 2 = female)\n",
    "\n",
    "\n",
    "It is important to note that birth year and gender details are not available for \"Customer\" user types but rather for \"Subscriber\" riders only. Fortunately, these are the only missing data values among all trips in the data set. Unfortunately, however, it means that we will not be able to identify the ratio of males-to-females that are not subscribed or use age to predict subcribers vs. non-subscribers (Customers). More to this end will be discussed in the next section.\n",
    "\n",
    "It is also worth mentioning that while attributes such as trip duration, start and end stations, bike ID, and basic rider details were collected and shared with the general public, care was taken by Citi Bike to remove trips taken by staff during system service appointments and inspections, trips to or from \"test\" stations which were employed during the data set's timeframe, and trips lasting less than 60 seconds which could indicate false checkout or re-docking efforts during checkin.\n",
    "\n",
    "Because some attributes may be deemed as duplicates (i.e. start_station_id, start_station_name, and start_station_latitude/longitude for identifying station locations), we chose to extract further attributes from the base attributes at hand. Further attributes were also extracted to mitigate the effects of time. In addition, we felt increased understanding could be obtained from combining weather data for the various trips as discussed in the previous section. These additional 10 attributes are described below:\n",
    "\n",
    "16. **LinearDistance** - *Integer* - The distance (miles) from a start station to an end station (as a crow flies); calculated from the latitude/longitude coordinates of start/end stations\n",
    "17. **DayOfWeek** - *String* - The day of the week a trip occurs regardless of time of day, month, etc.; extracted from the *starttime* attribute (i.e. Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)\n",
    "18. **TimeOfDay** - *String* - The portion of the day during which a bike was checked out; extracted from the *starttime* attribute (i.e. Morning, Midday, Afternoon, Evening, Night)\n",
    "19. **HolidayFlag** - *String* - A categorical binary value used to identify whether the day a trip occurred was on a holiday or not; extracted from the *starttime* attribute (i.e. 0 = Non-Holiday, 1 = Holiday)\n",
    "20. **Age** - *Integer* - The age of a rider at the time of a trip; calculated based on the *birth_year* attribute (Since only birth year is included in original Citi Bike data set, exact age at time of trip when considering birth month is not possible)\n",
    "21. **PRCP** - *Float* - The total recorded rainfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "22. **SNOW** - *Float* - The total recorded snowfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "23. **TAVE** - *Integer* - The average temperature throughout the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "24. **TMAX** - *Integer* - The maximum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "25. **TMIN** - *Integer* - The minimum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "\n",
    "After extracting our own attributes and merging weather data, the total number of attributes present in our final data set is 25. Only 15 are used throughout this lab, however, due to the duplicate nature of some attributes as discussed already. This final list of ***used*** attributes are tripduration, DayOfWeek, TimeOfDay, HolidayFlag, start_station_name, start_station_latitude, start_station_longitude, usertype, gender, Age, PRCP, SNOW, TAVE, TMAX, and TMIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250,000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gmaps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickleObject(objectname, filename, filepath = \"PickleFiles/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create a variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()\n",
    "    \n",
    "def unpickleObject(filename, filepath = \"PickleFiles/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "    \n",
    "    return unpickleObject\n",
    "    \n",
    "def clear_display():\n",
    "    from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Preparation 1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Part 1 - Define and prepare class variables\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4,881,384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680,909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to each other. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 hour period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however, we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding 2 - Visualize important attributes\n",
    "\n",
    "To re-iterate, our main objectives in analyzing these data are to determine which attributes have greatest bearing on predicting a rider's type (Customer vs. Subscriber) and to gain a better understanding of rider behavior as a function of external factors. Many attributes in this data set will eventually be used in subsequent labs to answer these questions. The primary attributes on which we will focus our attention in this section, however, are as follows:\n",
    "- Starting Location\n",
    "- Day of the Week\n",
    "- Time of Day\n",
    "- Trip Duration (both log and non-log)\n",
    "- Linear Distance\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "Over the course of this section, we will review these top attributes in some detail and discuss the value of using our chosen visualizations. Note also that merged weather data is of significant interest as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geophysical Start Stations HeatMap\n",
    "Before discussing the following heatmap in detail, it is worth noting some special steps required to use the gmaps module in Python in case the reader is interested in rendering our code to plot data on top of Google's maps (Note full instructions are available at https://media.readthedocs.org/pdf/jupyter-gmaps/latest/jupyter-gmaps.pdf)\n",
    "\n",
    "Besides having Jupyter Notebook installed on one's computer with extensions enabled (default if using Anaconda) and installing the gmaps module using pip, the following line should be run from within the command terminal. This is only to be done once and should be done when Jupyter Notebook is not running.\n",
    "```\n",
    "$ jupyter nbextension enable --py gmaps\n",
    "```\n",
    "In addition to running the above line in the command prompt, a Standard Google API user key will need obtained from https://developers.google.com/maps/documentation/javascript/get-api-key. This only needs done once and is necessary to pull the Google map data into the Jupyter Notebook environment. The key is entered in the *gmaps.configure()* line as shown in the below cell. We have provided our own private key in the meantime for the reader's convenience.\n",
    "\n",
    "Now on to the data visualization... This geological heatmap visualization is interactive; however, the kernel must run the code block each time our Jupyter Notebook file is opened due to the API key requirement. Therefore, we've captured some interesting views to aid in our discussion and have included them as embedded images.\n",
    "\n",
    "The start station heatmap represents the start station location data via attributes *start_station_latitude* and *start_station_longitude*. It identifies areas of highest and lowest concentration for trip starts. The location data is important as it helps us understand where the areas of highest activity are and, as will be seen in one of our later sections, will play an important role in identifying riders as regular customers or subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gmaps.configure(api_key=\"AIzaSyAsBi0MhgoQWfoGMSl5UcD-vR6H76cntxg\") # Load private Google API key\n",
    "\n",
    "locations = CitiBikeDataSampled_5050[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "m = gmaps.Map()\n",
    "heatmap_layer = gmaps.Heatmap(data = locations)\n",
    "m.add_layer(heatmap_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall view quickly reveals that station data was only provided for areas of NYC south of Manhattan and mostly north of Brooklyn. This could either mean that the bike share program had not yet expanded into these other areas at the time of data collection or that the data simply wasn't included (as mentioned previously, many test sites were being used during this time frame but CitiBike did not include them with this data set).\n",
    "\n",
    "Within the range of trip start frequency from the least number of trips (green) to the most trips (red), green and yellow indicate low to medium trip activity in most areas. However, higher pockets of concentration do exist in some places. We will attempt to put this visualization to good use by focusing in on one of these hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocations.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prominant heatspot occurs just east of Bryant Park and the Midtown map label. Zooming into this area (via regular Google Map controls as the rendered visual is interactive) allows for a closer look. A snapshot of this zoomed in image is embedded below. The hotspot seems slightly elongated and stands out from among the other stations. Zooming in further will help to understand why this is and may shed some light on the higher activity in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom1.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in to this area further helps us see that two stations are very close together. Even so, why might there be such high rider activity at these stations? This higher activity is likely affected by the stations' proximity to the famous Grand Central Station. As commuters and recreationalists alike arrive by train at Grand Central, it is natural that many of them may choose to continue their journey via the two closest bike share stations nearby. When the northernmost bike share station runs out of bikes, riders likely go to the next station to begin their ride instead.\n",
    "\n",
    "By understanding the dynamics of geographical activity within this data set and the amenities that surround each station, we will be able to more efficiently leverage the data to make our classification and regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom2.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geographic Heatmap Comparing Customer vs. Subscriber Start Station Activity\n",
    "\n",
    "After visualizing the overall dataset locations with a heatmap over NYC, we decided to take the visualization one step further. This time, we broke the dataset into two segments: Customer vs. Subscriber. Below is two separate gmap heatmaps containing geographic densities for each usertype. What we found assisted our theories on customer vs. subscriber usage tendencies. Seen first, the Customer heatmap overall contains much fewer dense regions. This helps to confirm our suspicions infering Customer bikers as less \"routine\" than subscribing bikers. When looking around for the most dense region in this heatmap, one point stuck out as particularly interesting: The Zoo. When comparing this region on the Subscriber gmap, we did not see the same type of traffic! This helps assist our theories that customer bikers use the service more for events, shopping,  or one-time use convenience. On the subscriber gmap, the most dense region, is that near the Grand Central Station as discussed earlier - assisting in the opposing theory for subscribing members as routine trips to work, groceries, etc. as they consistently use the bike share service as a means to reach the metro station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "customerData = CitiBikeDataSampled_5050.query('usertype == \"Customer\"')\n",
    "customerLoc = customerData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "cmap = gmaps.Map()\n",
    "customer_layer = gmaps.Heatmap(data=customerLoc)#, fill_color=\"red\", stroke_color=\"red\", scale=3)\n",
    "cmap.add_layer(customer_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/CMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "subscriberData = CitiBikeDataSampled_5050.query('usertype == \"Subscriber\"')\n",
    "subscriberLoc = subscriberData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "smap = gmaps.Map()\n",
    "subscriber_layer = gmaps.Heatmap(data=subscriberLoc)#, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "smap.add_layer(subscriber_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/SMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trip Duration and Linear Distance vs Weather by Customer/Subscriber\n",
    "\n",
    "Because we were able to bring together historical weather data for the dates we had in our records, we wanted to explore the relationship these variables had with our usertype status. If subscribers were regularly using the bikes for commuting as we've begun to see, then weather wouldn't impact their rental stastics as much as customers who appear to be primarily opportunistic in their usage.\n",
    "\n",
    "A quick cursory glance reveals a noticeable difference in bike rentals in regards to low temperatures, precipitation, and snowfall. While true, there are fewer customers than subscribers, we're concerned primarily with the spread or distribution of the plot points rather than the quantity. And we can see that on the customer pair plots that there are fewer points distributed across the lower temperature ranges and higher precipitation/snowfall ranges. The distributions pick back up at higher temperatures and lower precipitation points between the two usertypes.\n",
    "\n",
    "If stations consistently see use during \"bad\" weather, then those stations could be identified as subscriber stations. Further, if certain customers are found making the same trips consistently in all weather types, then they could be pushed for subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Customer'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Subscriber'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Trip Duration by Day of the Week Split Violin Plot\n",
    "\n",
    "Almost universally, across every day of the week, customers appear to have a higher trip duration than subscribers. While additional analysis will be required to confirm this, it's possible that one explanation is that subscribers can freely take and return their bikes which means that they're more willing to make shorter trips versus customers that pay each time they want to rent a bike in the first place. An alternate explanation, based on what we know in regards to the relationship between trip duration and linear distance traveled, is that subscribers are using the bikes for commuting to and from specific locations. This would result in a lower trip duration than customers that might use their bikes for general travel around the city. This possibility is corroborated by the decrease in activity on the weekends by subscribers.\n",
    "\n",
    "Identifying the point at which a customer might become a subscriber using this data would probably include monitoring weekday activity and trip duration. If a station has a lot of customers with trip durations similar to those of subscribers, then that station would be a good location to do a focused advertisement of the benefits of subscribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"tripdurationLog\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Linear Trip Distance by Day of the Week Split Violin Plot\n",
    "\n",
    "Unlike trip duration, the linear distance between start and end stations for both customers and subscribers appear to be similar in regards to means and are close in their quartiles. But what's noticeable here, is that customers are more widely distributed in how far or near they ride, with a significant increase in the number of customers that return their bikes to the station they started from.\n",
    "\n",
    "Further analysis will be necessary to explore the statistical significance of these differences, but it would be possible to identify those stations that are frequented by subscribers and assume that most stations within the first standard deviation of the linear distance found below to be considered \"subscriber stations\" and then seen which stations are outside of those zones to further build up the messaging encouraging subscription. Furthermore, by identifying those \"hot zones\" it's possible to rotate out bikes to increase their longevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"LinearDistance\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2 - Prepping for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "\n",
    "Initially including all start (and end) locations resulted in excessive system resource loading, later during randomized principal component computations, that froze our personal workstations and eventually ended with Python 'MemoryError' messaging. Therefore, due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size manually. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations) and including all of them resulted in system crashes during principal component analysis later in our lab. We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual integer columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these encodings complete, our final dataset to cross-validate on test/train datasets would appear to be complete. However, given the large number of attributes now present in our dataset, it would be wise to investigate a means of dimensionality reduction to not only speed up model generation, but to also improve accuracy by removing variable redundancy and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Set Summary\n",
    "\n",
    "At this stage, we've converted our original 30 variables into 107 attributes after creating dummy variables for categorical data such as day of the week, time of day, station names, gender, etc. These 107 attributes and their data types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(CitiBikeDataSampled_5050.columns):\n",
    "    data_type.append(CitiBikeDataSampled_5050.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(CitiBikeDataSampled_5050.columns, index = range(len(CitiBikeDataSampled_5050.columns))), 'Data Type' : pd.Series(data_type, index = range(len(CitiBikeDataSampled_5050.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE START\n",
    "\n",
    "Because our stratified, processed data is comprised of 105 various attributes ranging from weather and distance data to location and user type data across all 500,000 sample observations, and some variables such as weather attributes and even some start and end stations correlate to one another, we felt it would be wise during our previous Lab 2 analysis to reduce our number of attributes considered during Customer/Subscriber prediction. We proceeded to use Principal Component Analysis (PCA) to reduce the dimensionality of our dataset.\n",
    "\n",
    "Furthermore, during analysis of our principal components' loadings, we identified only 22 of the originally processed 105 attributes as being contextually important in identifying Customers and Subscribers. As the intent of our Lab 3 analysis is to further identify Customer users that should be Subscribers based on their behaviour, we deem it wise to only use these 22 attributes while clustering as well.\n",
    "\n",
    "These attributes are as follows:\n",
    "\n",
    "* start_station_latitude\n",
    "* start_station_longitude\n",
    "* end_station_latitude\n",
    "* end_station_longitude\n",
    "* PRCP\n",
    "* SNOW\n",
    "* TAVE\n",
    "* TMAX\n",
    "* TMIN\n",
    "* DayOfWeek_Friday\n",
    "* DayOfWeek_Monday\n",
    "* DayOfWeek_Saturday\n",
    "* DayOfWeek_Sunday\n",
    "* DayOfWeek_Thursday\n",
    "* DayOfWeek_Tuesday\n",
    "* DayOfWeek_Wednesday\n",
    "* TimeOfDay_Afternoon\n",
    "* TimeOfDay_Evening\n",
    "* TimeOfDay_Midday\n",
    "* TimeOfDay_Morning\n",
    "* TimeOfDay_Night\n",
    "* tripdurationLog\n",
    "\n",
    "In addition to using only these attributes while clustering, we chose to split our stratified sample data set of 500,000 transactions into separate Customer and Subscriber data sets while clustering. This will provide us the advantage of identifying clusters based on Customer data separately from Subscriber data - the advantage being that further granularity will be offered into the user sub-groups (based on transaction details) that comprise each user class. When later comparing these clusterings between each user class, we will be able to further classify one user type's data against the opposite user type's cluster IDs. This, and its implementation, will be described in much greater detail in the Deployment section. Currently, it suffices to say that clustering against each known user type is a necessary means of identifying would-be Subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Subset data set to only variables identified to have the greatest PCA loadings\n",
    "attr_clus = CitiBikeDataSampled_5050[['start_station_latitude',\n",
    "                                      'start_station_longitude',\n",
    "                                      'end_station_latitude',\n",
    "                                      'end_station_longitude',\n",
    "                                      'PRCP',\n",
    "                                      'SNOW',\n",
    "                                      'TAVE',\n",
    "                                      'TMAX',\n",
    "                                      'TMIN',\n",
    "                                      'DayOfWeek_Friday',\n",
    "                                      'DayOfWeek_Monday',\n",
    "                                      'DayOfWeek_Saturday',\n",
    "                                      'DayOfWeek_Sunday',\n",
    "                                      'DayOfWeek_Thursday',\n",
    "                                      'DayOfWeek_Tuesday',\n",
    "                                      'DayOfWeek_Wednesday',\n",
    "                                      'TimeOfDay_Afternoon',\n",
    "                                      'TimeOfDay_Evening',\n",
    "                                      'TimeOfDay_Midday',\n",
    "                                      'TimeOfDay_Morning',\n",
    "                                      'TimeOfDay_Night',\n",
    "                                      'tripdurationLog',\n",
    "                                      'usertype']]\n",
    "\n",
    "attr_scaled = attr_clus.ix[:,0:(len(attr_clus.columns)-1)] #Remove usertype from scaled columns\n",
    "scaler = StandardScaler().fit(attr_scaled)\n",
    "CitiBike_clus = scaler.transform(attr_scaled)\n",
    "\n",
    "CitiBike_clus = pd.DataFrame(CitiBike_clus)\n",
    "users = pd.DataFrame(CitiBikeDataSampled_5050.usertype)\n",
    "CitiBike_clus = pd.concat([CitiBike_clus.reset_index(), users.reset_index()], axis = 1)\n",
    "del CitiBike_clus['index']\n",
    "CitiBike_clus.columns = attr_clus.columns\n",
    "\n",
    "CitiBike_C = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Customer']\n",
    "CitiBike_S = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Subscriber']\n",
    "\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "#scaled = min_max_scaler.fit_transform(CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)])\n",
    "#cols = CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)].columns\n",
    "#CitiBike_C = pd.DataFrame(scaled, columns=cols)\n",
    "#\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "#scaled = min_max_scaler.fit_transform(CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)])\n",
    "#cols = CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)].columns\n",
    "#CitiBike_S = pd.DataFrame(scaled, columns=cols)\n",
    "#\n",
    "print('Customer data dimensions =', CitiBike_C.shape)\n",
    "print('Subscriber data dimensions =',CitiBike_S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to separting the data based on user type, we also scaled the data values to remove bias while clustering due to different attribute scales. Without scaling the data, attributes such as station coordinates and trip duration would carry heavier weights when compared against the OneHotEncoded attributes and precipitation data. This would cause unbalanced and improperly clustered groups. The first 5 standardized ride transactions are shown below for Customers and Subscribers as an example of what the data looks like after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(CitiBike_C.head())\n",
    "display(CitiBike_S.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 1 - Train and adjust parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run kmeans algorithm (this is the most traditional use of k-means)\n",
    "kmeans = KMeans(init='k-means++', # initialization\n",
    "        n_clusters=3,  # number of clusters\n",
    "        n_init=20,       # number of different times to run k-means\n",
    "        n_jobs=-1)\n",
    "\n",
    "kmeans.fit(CitiBike_S.ix[:,:21])\n",
    "\n",
    "# visualize the data\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.plot(CitiBike_S.ix[:, 1], CitiBike_S.ix[:, 0], 'r.', markersize=2) #plot the data\n",
    "plt.scatter(centroids[:, 1], centroids[:, 0],\n",
    "            marker='+', s=200, linewidths=3, color='k')  # plot the centroids\n",
    "plt.title('K-means clustering for X1')\n",
    "plt.xlabel('X1, Feature 1')\n",
    "plt.ylabel('X1, Feature 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X2_N = 10\n",
    "\n",
    "# create connectivity graphs before calcualting the hierarchy\n",
    "X2_knn_graph = kneighbors_graph(CitiBike_S.ix[:,:21], X2_N, mode='distance') # calculate distance to four nearest neighbors \n",
    "\n",
    "N2 = X2_knn_graph.shape[0]\n",
    "X2_4nn_distances = np.zeros((N2,1))\n",
    "for i in range(N2):\n",
    "    X2_4nn_distances[i] = X2_knn_graph[i,:].max()\n",
    "\n",
    "X2_4nn_distances = np.sort(X2_4nn_distances, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(N2), X2_4nn_distances, 'r.', markersize=2) #plot the data\n",
    "plt.title('Dataset name: X2, sorted by neighbor distance')\n",
    "plt.xlabel('X2, Instance Number')\n",
    "plt.ylabel('X2, Distance to {0}th nearest neighbor'.format(X2_N))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X1 = df_imputed[['Pclass','Fare']]\n",
    "\n",
    "params = []\n",
    "for link in ['ward', 'complete', 'average']:\n",
    "    for n_fam in range(13,20):\n",
    "\n",
    "        # append on the clustering\n",
    "        cls_fam = AgglomerativeClustering(n_clusters=n_fam, linkage=link)\n",
    "        cls_fam.fit(X2)\n",
    "        newfeature_fam = cls_fam.labels_ # the labels from kmeans clustering\n",
    "\n",
    "        y = df_imputed['Survived']\n",
    "        X = df_imputed[['IsMale','Pclass','Fare']]\n",
    "        X = np.column_stack((X,pd.get_dummies(newfeature_fam)))\n",
    "\n",
    "        acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "        params.append((n_fare,n_fam,acc.mean()*100,acc.std()*100)) # save state\n",
    "\n",
    "        print (\"C=\",n_fam,link,\"Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = AgglomerativeClustering(n_clusters=14, linkage='complete')\n",
    "cls.fit(data)\n",
    "hac_labels = cls.labels_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "\n",
    "Our next approach to clustering the CitiBike user data is to implement DBSCAN on the 22 attributes selected for clustering. However, initial clustering attempts while including longitude and latitude data resulted in processing failure due to memory errors. For this reason, we have chosen to remove start and end station coordinate data from the attributes identified for clustering.\n",
    "\n",
    "While this differs from our other approaches to clustering in this lab, treating coordinates as such is a necessary action given the processing problems encounted on our workstations. Our theory is that it is in part due to having so many transactions with the exact same geo-coordinate values. One solution might to OneHotEncode coordinate values, treating them as categorical data. The downside to this is that it would significantly increase the number of attributes in our data set. Given the problem and scope of this analysis, we've opted for their removal instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']]\n",
    "C_dbscan = CitiBike_C.ix[:,4:22]\n",
    "S_dbscan = CitiBike_S.ix[:,4:22]\n",
    "CitiBike_loc = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "print('Customer dbscan data dimensions =', C_dbscan.shape)\n",
    "print('Subscriber dbscan data dimensions =', S_dbscan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate removal aside, we do have some reservations about implementing DBSCAN clustering, period, due to the density method being based on Euclidean distance measures. Even with 18 attributes instead of 22, the high dimensionality within our Customer and Subscriber data sets is expected to minimize the effectiveness of using distance as the primary measure. Nevertheless, such an attempt is still worth while in order to compare the results against our other clustering methods.\n",
    "\n",
    "Of course, another expected limitation is that of size. DBSCAN is not very efficient with our large data set since a full pairwise similarity matrix must be constructed, amounting to $n^2$ computations each time the algorithm is run (See [Sci-Kit Learn's description](http://scikit-learn.org/stable/modules/clustering.html#dbscan)). So DBSCAN is expected to be slow to implement, making parameter adjustments difficult and time consuming.\n",
    "\n",
    "Now moving on with the analysis, we will later plot the cluster IDs against the coordinates in order to gain better insight into which cluster ID's appear at each station (Again, coordinate attributes are removed from the DBSCAN cluster data... plotting against coordinates is for marketing application purposes only as will be described in the deployment section). The initial coordinates, without clustering, are depicted in the following scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(15,5))\n",
    "#plt.plot(CitiBike_dbscan.start_station_longitude, CitiBike_dbscan.start_station_latitude, 'bo', markersize=3) #plot the data\n",
    "plt.plot(CitiBike_loc.start_station_longitude, CitiBike_loc.start_station_latitude, 'bo', markersize=4, markerfacecolor='b') #plot the data\n",
    "plt.title('Latitude/Longitude Data'.format(2))\n",
    "plt.xlabel('Scaled Longitude Coordinate'.format(2))\n",
    "plt.ylabel('Scaled Latitude Coordinate'.format(2))\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(-3,2.5)\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.grid()\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, customer and subscriber data will be clustered separately in order to help identify customers that should be subscribers but aren't, as will be discussed in greater detail in the Deployment section.\n",
    "\n",
    "Below are functions that will be utilized throughout the DBSCAN clustering process. The *getGraph* function simply generates a k-neighbors graph of the data in order to plot potential *eps* values based on a prescribed minimum number of samples to be used in the DBSCAN algorithm. Next, the *epsPlot* function plots these potential *eps* values. Finally, our *dbs* function clusters the data based on prescribed *eps* and minimum number of samples parameters. Because we plan to plot the clusters against station locations, we adopted and modified a couple functions from this [Stackoverflow post](http://stackoverflow.com/questions/8671808/matplotlib-avoiding-overlapping-datapoints-in-a-scatter-dot-beeswarm-plot) to jitter our data points while depicting a scatterplot. These functions are provided below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = arr.max()/100.\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "def jitter(x, y, s=20, c='b', marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs):\n",
    "    return plt.scatter(rand_jitter(x), rand_jitter(y), s=s, c=c, marker=marker, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs)\n",
    "\n",
    "def getGraph(N, data):\n",
    "    graph = kneighbors_graph(data, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    #CitiBike_knn_graph = kneighbors_graph(CitiBike_clus, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def epsPlot(N, graph):\n",
    "    N1 = graph.shape[0]\n",
    "    CitiBike_distances = np.zeros((N1,1))\n",
    "    for i in range(N1):\n",
    "        CitiBike_distances[i] = graph[i,:].max()\n",
    "\n",
    "    CitiBike_distances = np.sort(CitiBike_distances, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #plt.subplot(1,2,1)\n",
    "    plt.plot(range(N1), CitiBike_distances, 'r.', markersize=4) #plot the data\n",
    "    plt.title('Dataset name: CitiBike_clus, sorted by neighbor distance')\n",
    "    plt.xlabel('CitiBike_clus, Instance Number')\n",
    "    plt.ylabel('CitiBike_clus, Distance to {0}th nearest neighbor'.format(N))\n",
    "    #plt.xlim([400000,500000])\n",
    "    #plt.plot([0, 350], [0.0054, 0.0054], 'k--', lw=0.5)\n",
    "    #plt.grid()\n",
    "    \n",
    "def dbs(eps, minpts, data):\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_dbscan)\n",
    "    db = DBSCAN(eps=eps, min_samples=minpts, n_jobs=-1).fit(data)\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts, metric='cosine', algorithm='brute').fit(CitiBike_clus)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    # mark the samples that are considered \"core\"\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    plt.figure(figsize=(15,4))\n",
    "    unique_labels = set(labels) # the unique labels\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "        \n",
    "        if(data.equals(C_dbscan)):\n",
    "            xy = CitiBike_C[class_member_mask & core_samples_mask]\n",
    "        elif(data.equals(S_dbscan)):\n",
    "            xy = CitiBike_S[class_member_mask & core_samples_mask]\n",
    "        # plot the core points in this class\n",
    "        #plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "        #         markeredgecolor='w', markersize=6)\n",
    "        jitter(xy.start_station_longitude, xy.start_station_latitude, c=col, s=10)\n",
    "\n",
    "        # plot the remaining points that are edge points\n",
    "        if(data.equals(C_dbscan)):\n",
    "            xy = CitiBike_C[class_member_mask & ~core_samples_mask]\n",
    "        elif(data.equals(S_dbscan)):\n",
    "            xy = CitiBike_S[class_member_mask & ~core_samples_mask]\n",
    "        #plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "        #         markeredgecolor='w', markersize=3)\n",
    "        jitter(xy.start_station_longitude, xy.start_station_latitude, c=col, s=10)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    plt.xlim(-10,10)\n",
    "    plt.ylim(-3,2.5)\n",
    "    plt.grid()\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.show()\n",
    "    \n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer DBSCAN\n",
    "\n",
    "Before running the DBSCAN algorithm on our customer data set, we would first like to obtain a basic understanding of what types of eps and minimum number of samples to use. Due to the multi-dimensionality of our data, and the fact that we have 250,000 observations in our customer data set, it is difficult to define an appropriate mininum number of samples through visual inspection of the data. Therefore, we have chosen a preliminary sample count of 200 from which we will identify an eps value to use as our initial starting point.\n",
    "\n",
    "The eps plot below is configured such that customer transaction instances are ordered from least to greatest distance to their 200th nearest neighbor. To determine a good starting point, we identify a point where the distance starts to increase (looking for an inflection point upward). This is a bit challenging given the plotted results, but, again, this is only a starting point for our permutations. The arguable point identified in this case is at an eps value of about 0.5. This will be our starting value used in our first customer DBSCAN permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 200\n",
    "CitiBike_knn_graph = getGraph(N, C_dbscan)\n",
    "epsPlot(N, CitiBike_knn_graph)\n",
    "plt.annotate('Expected Eps value = approx. 0.5', xy=(225000, 0.7), xytext=(150000, 10),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, a minimal core sample count of 200 was selected rather arbitrarily since it is very difficult to identify a good starting point visually. Given that our customer data contains 250,000 transactions and we'd like to obtain DBSCAN results producing no more than ten to twenty clusters at most, 200 seemed like a decent starting point with the correct eps value. After a few DBSCAN runs, we decided to adjust this number based on results produced when adjusting the *eps* value.\n",
    "\n",
    "DBSCAN's inefficiencies with large data sets were quickly exemplified as we performed the following permutations:\n",
    "\n",
    "* Configuration 1: *eps* = 0.5, *minpts* = 200 ...... Wall time = 31min 16s\n",
    "* Configuration 2: *eps* = 0.7, *minpts* = 200 ...... Wall time = 35min 22s\n",
    "* Configuration 3: *eps* = 0.9, *minpts* = 200 ...... Wall time = 42min 47s\n",
    "* Configuration 4: *eps* = 1.0, *minpts* = 200 ...... Wall time = 45min 26s\n",
    "* Configuration 5: *eps* = 1.0, *minpts* = 250 ...... Wall time = 46min 55s\n",
    "* Configuration 6: *eps* = 1.0, *minpts* = 300 ...... Wall time = 47min 47s\n",
    "* Configuration 7: *eps* = 1.2, *minpts* = 200 ...... Wall time = 1h 14min 53s\n",
    "* Configuration 8: *eps* = 2.0, *minpts* = 200 ...... Wall time = 3h 39min 30s\n",
    "\n",
    "These permutations are performed below and their outputs pickled in order to save time each time our analysis code is run though again. The concept of pickling our results will be discussed further in the Deployment section.\n",
    "\n",
    "Cluster visualizations will be provided and discussed in the Modeling and Evaluation Part 3 section later in this writeup and, therefore, are not provided immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c = unpickleObject(\"db_c\")\n",
    "else: db_c = dbs(0.5, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c1 = unpickleObject(\"db_c1\")\n",
    "else: db_c1 = dbs(0.7, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c2 = unpickleObject(\"db_c2\")\n",
    "else: db_c2 = dbs(0.9, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c3 = unpickleObject(\"db_c3\")\n",
    "else: db_c3 = dbs(1.0, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c4 = unpickleObject(\"db_c4\")\n",
    "else: db_c4 = dbs(1.0, 250, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c5 = unpickleObject(\"db_c5\")\n",
    "else: db_c5 = dbs(1.0, 300, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c6 = unpickleObject(\"db_c6\")\n",
    "else: db_c6 = dbs(1.2, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c7 = unpickleObject(\"db_c7\")\n",
    "else: db_c7 = dbs(2.0, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c, \"db_c\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c1, \"db_c1\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c2, \"db_c2\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c3, \"db_c3\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c4, \"db_c4\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c5, \"db_c5\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c6, \"db_c6\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_c7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_c7, \"db_c7\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber DBSCAN\n",
    "\n",
    "Similar to what was done for our customer data above, we again chose a minimum core sample count of 100 as a preliminary starting point for subscriber data, not knowing exactly how many samples to define given our 22 dimension data set. We fully expect the results of the following *eps* plot be different from the customer data's plot because, as has been clearly identified in our previous labs and visually earlier in this lab, customers and subscribers behave differently. Therefore we expect them to cluster differently as well.\n",
    "\n",
    "The subscriber eps plot below is configured as was the plot for customer data above.The arguable point identified in this case is at an eps value of about 1.0. This will be our starting value used in our first subscriber DBSCAN permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 200\n",
    "CitiBike_knn_graph = getGraph(N, S_dbscan)\n",
    "epsPlot(N, CitiBike_knn_graph)\n",
    "plt.annotate('Expected Eps value = approx. 1.0', xy=(225000, 1.0), xytext=(150000, 5.5),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our starting points identified, we again planned to conduct 8 different permutations to narrow down to the best subscriber DBSCAN configuration. These configurations' parameters are outlined below along with their final run times (Note that subscriber permutations were run on a different workstation than the customer permutations to make efficient use of work time. So, these times are not meant to be an apples-to-apples comparison against customer times.):\n",
    "\n",
    "* Configuration 1: *eps* = 1.0, *minpts* = 200 ...... Wall time = 31min 48s\n",
    "* Configuration 2: *eps* = 0.8, *minpts* = 200 ...... Wall time = 25min 39s\n",
    "* Configuration 3: *eps* = 0.5, *minpts* = 200 ...... Wall time = 18min 13s\n",
    "* Configuration 4: *eps* = 0.5, *minpts* = 150 ...... Wall time = 18min 16s\n",
    "* Configuration 5: *eps* = 0.5, *minpts* = 250 ...... Wall time = 18min 21s\n",
    "* Configuration 6: *eps* = 0.5, *minpts* = 300 ...... Wall time = 18min 11s\n",
    "* Configuration 7: *eps* = 0.5, *minpts* = 350 ...... Wall time = 18min 17s\n",
    "* Configuration 8: *eps* = 0.5, *minpts* = 450 ...... Wall time = 18min 6s\n",
    "\n",
    "After reviewing results for these eight permutations, we decided it was worth also attempting two more permutations as outlined below:\n",
    "\n",
    "* Configuration 9: *eps* = 1.0, *minpts* = 450 ...... Wall time = 30min 31s\n",
    "* Configuration 10: *eps* = 1.2, *minpts* = 450 ...... Wall time = 35min 18s\n",
    "\n",
    "Again, these permutations are performed below but their outputs are pickled in order to save time with each run of our code. Cluster visualizations will be provided and discussed in the Modeling and Evaluation Part 3 section later in this writeup and, therefore, are not provided immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s = unpickleObject(\"db_s\")\n",
    "else: db_s = dbs(1.0, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s1 = unpickleObject(\"db_s1\")\n",
    "else: db_s1 = dbs(0.8, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s2 = unpickleObject(\"db_s2\")\n",
    "else: db_s2 = dbs(0.5, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s3 = unpickleObject(\"db_s3\")\n",
    "else: db_s3 = dbs(0.5, 150, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s4 = unpickleObject(\"db_s4\")\n",
    "else: db_s4 = dbs(0.5, 250, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s5 = unpickleObject(\"db_s5\")\n",
    "else: db_s5 = dbs(0.5, 300, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s6 = unpickleObject(\"db_s6\")\n",
    "else: db_s6 = dbs(0.5, 350, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s7 = unpickleObject(\"db_s7\")\n",
    "else: db_s7 = dbs(0.5, 450, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s8.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s8 = unpickleObject(\"db_s8\")\n",
    "else: db_s8 = dbs(1, 450, S_dbscan) ## This is our WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s9.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s9 = unpickleObject(\"db_s9\")\n",
    "else: db_s9 = dbs(1.2, 450, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s, \"db_s\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s1, \"db_s1\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s2, \"db_s2\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s3, \"db_s3\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s4, \"db_s4\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s5, \"db_s5\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s6, \"db_s6\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s7, \"db_s7\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s8.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s8, \"db_s8\", filepath = \"PickleFiles/\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/db_s9.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "else: pickleObject(db_s9, \"db_s9\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering\n",
    "\n",
    "The final clustering method we will apply to our data set is Spectral Clustering. Due to the multi-dimensional state of our data, our expectation is that spectral clustering will outperform K-means and DBSCAN clustering as it does not rely on distance measures but rather node connectivity. Another expected advantage to using Spectral Clustering is that it allows us to reduce the dimensionality of our data by clustering the data's first two Laplacian matrix eigenvectors only instead of all 22 attributes. As such, it does require a bit more data prep before clustering such as computing Similarity and Affinity, Degree, and Laplacian matrices, but the anticipated outcome is worth the effort. It is by calculating these Laplacian matrix eigenvectors that we are also able to identify an appropriate number of clusters to feed into the algorithm. Cluster count identification will be revealed and discussed in the Modeling and Evaluation Part 2 - Evaluate and Compare section.\n",
    "\n",
    "For Spectral Clustering we chose to use R not only for a change of pace but also because documentation for running the algorithm in R is plentiful and application using R overlapped with one of our group members' projects at work. A similar outcome could have been reached with Python just as well, but integrating R offered a fun, new challenge on our Windows OS machines.\n",
    "\n",
    "Before computing our required matrices, we must first intialize our **rpy2.ipython** session and pass to it our scaled Customer and Subscriber data. We also check to ensure the data imported correctly as an R dataframe and that attribute data types are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "#%R install.packages(\"kernlab\")\n",
    "%R library(kernlab)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%R -i CitiBike_C\n",
    "%R -i CitiBike_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = %R capture.output(str(CitiBike_C))\n",
    "\n",
    "for line in out:\n",
    "         print(line)\n",
    "print(\"\")\n",
    "out = %R capture.output(str(CitiBike_S))\n",
    "\n",
    "for line in out:\n",
    "         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial attempts to compute similarity matrices for all 250,000 transactions from each user type data set, it became quickly apparent we would need to sub-sample our data further. We discovered that the relationship between sample size and CPU time required to compute the Similarity, Affinity, Degree, and Laplacian matrices is not linear. For example, when random-sampling 500 transcations from each data set, matrix computations took 11 minutes to complete for each data set (22 minutes in total between both data sets). With 1000 samples randomly selected for matrix derivation, however, required CPU runtime significantly increased to 44 minutes per data set (that's 1.5 hours in total between both data sets). Given the significant amount of time spent running our DBSCAN code above, we opted to sample no more than 1000 transactions based on our project schedule. We later address this loss in granularity by predicting the remaining transactions' cluster IDs via classification methods in our Deployment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "CitiBike_miniC <- CitiBike_C[sample(1:nrow(CitiBike_C), 1000, replace=FALSE),]\n",
    "CitiBike_miniS <- CitiBike_S[sample(1:nrow(CitiBike_S), 1000, replace=FALSE),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In final preparation for matrix computation, we configure our Similarity and Affinity matrix functions which are a bit more extensive than our Degree and Laplacian matrix computations. These functions were adapted from a [tutorial](http://www.di.fc.ul.pt/~jpn/r/spectralclustering/spectralclustering.html) written by Joao Neto in December 2013. Similarly, we based our eigenvector derivations on this same tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Similarity matrix computation functions\n",
    "s <- function(x1, x2, alpha=1) {\n",
    "  exp(- alpha * norm(as.matrix(x1-x2), type=\"F\"))\n",
    "}\n",
    "\n",
    "make.similarity <- function(my.data, similarity) {\n",
    "  N <- nrow(my.data)\n",
    "  S <- matrix(rep(NA,N^2), ncol=N)\n",
    "  for(i in 1:N) {\n",
    "    for(j in 1:N) {\n",
    "      S[i,j] <- similarity(my.data[i,], my.data[j,])\n",
    "    }\n",
    "  }\n",
    "  S\n",
    "}\n",
    "\n",
    "# Affinity matrix computation functions\n",
    "make.affinity <- function(S, n.neighboors=2) {\n",
    "  N <- length(S[,1])\n",
    "\n",
    "  if (n.neighboors >= N) {  # fully connected\n",
    "    A <- S\n",
    "  } else {\n",
    "    A <- matrix(rep(0,N^2), ncol=N)\n",
    "    for(i in 1:N) { # for each line\n",
    "      # only connect to those points with larger similarity \n",
    "      best.similarities <- sort(S[i,], decreasing=TRUE)[1:n.neighboors]\n",
    "      for (s in best.similarities) {\n",
    "        j <- which(S[i,] == s)\n",
    "        A[i,j] <- S[i,j]\n",
    "        A[j,i] <- S[i,j] # to make an undirected graph, ie, the matrix becomes symmetric\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  A  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer Spectral Clustering\n",
    "\n",
    "We first tackle the Customer data set by calculating our Similarity, Affinity, Degree, and Laplacian matrices. Due to matrix computation CPU requirements (42min 43s), we save our Laplacian matrix object as a RDS file for readback, similar to our previous pickles.\n",
    "\n",
    "The Similarity and Affinity matrices serve a similar purpose to an Adjacency matrix by indicating node proximity with edges indicating links. Once the Affinity matrix is produced, it is used to compute the Degree matrix which is a diagonal matrix summing the number of edges for each node. Next, the Laplacian matrix is computed from both the Degree and Affinity matrices. We opted to implement the unnormalized graph Laplacian for our application as our data is already scaled. The formula for this is simply: $Laplacian = Degree - Affinity$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "if(!file.exists('PickleFiles/CustLap.rds')){\n",
    "    S <- make.similarity(CitiBike_miniC[,1:22], s)\n",
    "    A <- make.affinity(S, 3)  # use 3 neighboors (includes self)\n",
    "    D <- diag(apply(A, 1, sum)) # sum rows\n",
    "    U <- D - A\n",
    "    saveRDS(U, 'PickleFiles/CustLap.rds')\n",
    "}\n",
    "else{U <- readRDS('PickleFiles/CustLap.rds')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the final Laplacian matrix is complete, we move on to eigenvector computation. Specifically, we are interested in deriving the first eigenvector and the second eigenvector, or Fiedler Vector. These are required for us to cluster our data in a two dimensional space by identifying a location of normalized cut in the Fiedler Vector. This cut location is revealed by analyzing both the number of edges in each cluster that do not connect to other clusters and the number of edges which do connect each cluster. Similarly, relative connectivity and interconnectivity may be used to evaluate a proper cut: $RC*RI^a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "k   <- 2\n",
    "evL <- eigen(U, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]\n",
    "evLc <- evL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix and eigenvalue computations are the raw, manual methods used to transform the data set into a reduced dimensional space which can be easily clustered. Doing so manually is essential to identifying the correct number of clusters to feed into our clustering algorithm. This cluster count identification will be discussed in detail in the Modeling and Evaluation Part 2 - Evaluate and Compare section, but it suffices us to say for now that the appropriate number of clusters for customer data is 3.\n",
    "\n",
    "Fortunately, besides computing eigenvectors for cluster count identification, the *specc* R library can perform these computations for us and then cluster on the produced eigenvectors. All that is required of us beforehand is to convert our 1000 sample dataframe into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "mCitiC <- data.matrix(CitiBike_miniC)\n",
    "colnames(mCitiC) <- colnames(CitiBike_miniC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *specc* Spectral Clustering algorithm provides a number of parameters we can adjust to obtain varying cluster results. Permutations are provided below and are fast enough that no further RDS object storage is required.\n",
    "\n",
    "* Configuration 1: *centers* = 3, *iterations* = 200, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 2: *centers* = 3, *iterations* = 500, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 3: *centers* = 3, *iterations* = 1000, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 4: *centers* = 3, *iterations* = 1000, *kernel* = 'laplacedot', *nystrom.red* = FALSE\n",
    "* Configuration 5: *centers* = 3, *iterations* = 1000, *kernel* = 'polydot', *nystrom.red* = FALSE\n",
    "* Configuration 6: *centers* = 3, *iterations* = 1000, *kernel* = 'rbfdot', *nystrom.red* = TRUE\n",
    "\n",
    "*Centers* defines the number of clusters to produce as one would expect. *Iterations* in this case defines the maximum number of iterations allowed for the clustering algorithm to perform; the default number of iterations is 200. The *kernel* argument defines the kernel function used to compute the affinity matrix within the *specc* call; the default is 'rbfdot' which is the Radial Basis, or Gaussian, kernel function. Other options provided in our configurations include 'laplacedot' and 'polydot', where 'laplacedot' is the Laplacian kernel function and 'polydot' is the Polynomial kernel function. Finally, *nystrom.red* is a boolean argument signifying whether to use Nystrom Method to calculate eigenvectors. The default is False, but when changed to True, a sample of the dataset is used to compute the eigenvalues so that only an $n*m$ matrix ($n$ being the sample size) is stored in memory. We use the default number of Nystrom Method samples which is $\\frac{nrow}{6}$, or 167 given our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c <- specc(mCitiC, centers = 3)\n",
    "\n",
    "#iteration <- 1\n",
    "#jitplot(sc_c, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c1 <- specc(mCitiC, centers = 3, iterations = 500)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c1, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c1)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c2 <- specc(mCitiC, centers = 3, iterations = 1000)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c2, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c2)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c3 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'laplacedot')\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c3, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c3)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c4 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'polydot')\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c4, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c4)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "sc_c5 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'rbfdot', nystrom.red = TRUE)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c5, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_c5)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber Spectral Clustering\n",
    "\n",
    "We next tackle the Subscriber data set by calculating our Similarity, Affinity, Degree, and Laplacian matrices. Due to matrix computation CPU requirements (48min 29s), we save our Laplacian matrix object as a RDS file for readback, as was done with Customer spectral clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "if(!file.exists('PickleFiles/SubsLap.rds')){\n",
    "    S <- make.similarity(CitiBike_miniS[,1:22], s)\n",
    "    A <- make.affinity(S, 3)  # use 3 neighboors (includes self)\n",
    "    D <- diag(apply(A, 1, sum)) # sum rows\n",
    "    U <- D - A\n",
    "    saveRDS(U, 'PickleFiles/SubsLap.rds')\n",
    "}\n",
    "else{U <- readRDS('PickleFiles/SubsLap.rds')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the final Laplacian matrix is complete, we move on to eigenvector computation to obtain our first eigenvector and Fiedler Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "k   <- 2\n",
    "evL <- eigen(U, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]\n",
    "evLs <- evL\n",
    "\n",
    "signif(evL$values,2) # eigenvalues are in decreasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix and eigenvalue computations are the raw, manual methods used to transform the data set into a reduced dimensional space which can be easily clustered. Doing so manually is essential to identifying the correct number of clusters to feed into our clustering algorithm. This cluster count identification will be discussed in detail in the Modeling and Evaluation Part 2 - Evaluate and Compare section, but it suffices us to say for now that the appropriate number of clusters *to start with* for customer data is 4. 5 clusters were also identified as a possible candidate and will be explored here as well.\n",
    "\n",
    "Again, the *specc* R library will re-perform matrix computations for us and then cluster on the produced eigenvectors. All that is required of us beforehand is to convert our 1000 sample dataframe into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "mCitiS <- data.matrix(CitiBike_miniS)\n",
    "colnames(mCitiS) <- colnames(CitiBike_miniS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subscriber data permutations are provided below and are fast enough that no further RDS object storage is required. Notice that 4 centers were first defined but since the cluster sizes were not well distributed, 5 clusters was defined instead (we had some flexibility to use either 4 or 5 centers based on cluster count analysis as will be described later).\n",
    "\n",
    "* Configuration 1: *centers* = 4, *iterations* = 200, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 2: *centers* = 5, *iterations* = 200, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 3: *centers* = 5, *iterations* = 500, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 4: *centers* = 5, *iterations* = 1000, *kernel* = 'rbfdot', *nystrom.red* = FALSE\n",
    "* Configuration 5: *centers* = 5, *iterations* = 1000, *kernel* = 'laplacedot', *nystrom.red* = FALSE\n",
    "* Configuration 6: *centers* = 5, *iterations* = 1000, *kernel* = 'polydot', *nystrom.red* = FALSE\n",
    "* Configuration 7: *centers* = 5, *iterations* = 1000, *kernel* = 'rbfdot', *nystrom.red* = TRUE\n",
    "\n",
    "*Centers* defines the number of clusters to produce as one would expect. *Iterations* in this case defines the maximum number of iterations allowed for the clustering algorithm to perform; the default number of iterations is 200. The *kernel* argument defines the kernel function used to compute the affinity matrix within the *specc* call; the default is 'rbfdot' which is the Radial Basis, or Gaussian, kernel function. Other options provided in our configurations include 'laplacedot' and 'polydot', where 'laplacedot' is the Laplacian kernel function and 'polydot' is the Polynomial kernel function. Finally, *nystrom.red* is a boolean argument signifying whether to use Nystrom Method to calculate eigenvectors. The default is False, but when changed to True, a sample of the dataset is used to compute the eigenvalues so that only an $n*m$ matrix ($n$ being the sample size) is stored in memory. We use the default number of Nystrom Method samples which is $\\frac{nrow}{6}$, or 167 given our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s <- specc(mCitiS, centers = 4)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#iteration <- 1\n",
    "#jitplot(sc_s, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s6 <- specc(mCitiS, centers = 5)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s6, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s6)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s1 <- specc(mCitiS, centers = 5, iterations = 500)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s1, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s1)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s2 <- specc(mCitiS, centers = 5, iterations = 1000)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s2, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s2)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s3 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'laplacedot')\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s3, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s3)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s4 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'polydot')\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s4, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s4)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(1000)\n",
    "sc_s5 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'rbfdot', nystrom.red = TRUE)\n",
    "\n",
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s5, iteration)\n",
    "#iteration <- iteration + 1\n",
    "\n",
    "#out = %R capture.output(sc_s5)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 2 - Evaluate and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering\n",
    "\n",
    "As alluded to previously, DBSCAN is a poorly efficient algorithm for our application. We wanted so much to like it but couldn't compel ourselves to do so. Since the number of clusters are determined based on density metrics *eps* and *minimum core samples*, we endeavored to simply generate a reasonable number of customer and subscriber clusters for starters. Sadly enough, this turned into our primary metric for DBSCAN evaluation in the end due to rediculous computation times given our workstation limitations and stressed our project deliverables. Since we also knew the \"curse of dimensionality\" was not working in our favor in terms of producing valid DBSCAN results on our 22 attribute data sets, we deemed it more worthy of our time to move on to Spectral Clustering.\n",
    "\n",
    "##### Customer DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_customer = pd.DataFrame ({\"Cluster_ID_Customer\": db_c7.labels_, \"Cluster_ID_Subscriber\": np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cust_clus_DB = pd.concat([CitiBike_C.reset_index(), cluster_id_customer], axis=1)\n",
    "#CitiBike_C = pd.concat([CitiBike_C, cluster_id_customer], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portrayed in the table below are the cluster counts for each permutation of DBSCAN on our customer data. Our directive approach to *eps* and core sample changes was in the right direction with our first iteration producing a whopping 103 clusters and the last iteration producing 49 clusters (at the significant expense of computation time). Based on the number of clusters produced and our prior knowledge regarding the ride behaviors of customers, 49 clusters is still unreasonably large. Given the $n^2$ computations, time required to complete these computations, and observable increase in time required with the few number of clusters produced, we chose to stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_counts = [db_c.labels_.max() + 1,\n",
    "               db_c1.labels_.max() + 1,\n",
    "               db_c2.labels_.max() + 1,\n",
    "               db_c3.labels_.max() + 1,\n",
    "               db_c4.labels_.max() + 1,\n",
    "               db_c5.labels_.max() + 1,\n",
    "               db_c6.labels_.max() + 1,\n",
    "               db_c7.labels_.max() + 1]\n",
    "\n",
    "clus_counts = pd.DataFrame({'Cluster_Counts' : clus_counts})\n",
    "clus_counts.index.names = ['Permutation']\n",
    "clus_counts.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster size distributions for the final customer permutation are depicted in the table and histogram below. It is interesting to see that even with so many clusters, there is still significant imbalance in cluster size from one grouping to the next. Given our expectations for this data set, this further proves DBSCAN to be insufficient for our needs. Of good report is there were only 2,570 outliers represented in our customer data (average transactions per cluster were 5,000 when including outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agg = pd.DataFrame({'count' : Cust_clus_DB.groupby([\"Cluster_ID_Customer\"]).size()}).reset_index()\n",
    "\n",
    "display(Agg.T)\n",
    "print(\"Min. transactions per cluster (including outliers) =\", Agg['count'].min())\n",
    "print(\"Max. transactions per cluster (including outliers) =\", Agg['count'].max())\n",
    "print(\"Average transactions per cluster (including outliers) =\", Agg['count'].mean())\n",
    "Agg.hist('count', color = 'orange', bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_subscriber = pd.DataFrame ({\"Cluster_ID_Customer\": np.NaN, \"Cluster_ID_Subscriber\": db_s8.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sub_clus_DB = pd.concat([CitiBike_S.reset_index(), cluster_id_subscriber], axis=1)\n",
    "#CitiBike_C = pd.concat([CitiBike_C, cluster_id_customer], axis=1)\n",
    "#Sub_clus_DB.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portrayed in the table below are the cluster counts for each permutation of DBSCAN on our subscriber data. Our directive approach to *eps* and core sample changes this time was not in the best direction as our first permutation produced 71 clusters and our subsequent permutations increased the number of clusters produced. Though our *eps* and *minimum core samples* parameters were eventually corrected, we decided to implement a couple additional permutations as well in attempts to drive the number of clusters down further (our smallest count being 36). Based on the number of clusters produced and our prior knowledge regarding the ride behaviors of subscribers, even 36 clusters is still unreasonably large. Given the $n^2$ computations, time required to complete these computations, and observable increase in time required with the few number of clusters produced, we chose to stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_counts = [db_s.labels_.max() + 1,\n",
    "               db_s1.labels_.max() + 1,\n",
    "               db_s2.labels_.max() + 1,\n",
    "               db_s3.labels_.max() + 1,\n",
    "               db_s4.labels_.max() + 1,\n",
    "               db_s5.labels_.max() + 1,\n",
    "               db_s6.labels_.max() + 1,\n",
    "               db_s7.labels_.max() + 1,\n",
    "               db_s8.labels_.max() + 1,\n",
    "               db_s9.labels_.max() + 1]\n",
    "\n",
    "clus_counts = pd.DataFrame({'Cluster_Counts' : clus_counts})\n",
    "clus_counts.index.names = ['Permutation']\n",
    "clus_counts.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster size distributions for the permutation producing 36 clusters are depicted in the table and histogram below. It is interesting to see that even with so many clusters, there is still significant imbalance in cluster size from one grouping to the next. The disparity between sizes is quite large for subscriber DBSCAN results as well. For example, the 36th cluster only contains 465 transactions whereas the 5th cluster contains 13,392 and there are a total of 40,669 outliers! While this is the \"best\" subscriber configuration, it still falls short of our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Agg = pd.DataFrame({'count' : Sub_clus_DB.groupby([\"Cluster_ID_Subscriber\"]).size()}).reset_index()\n",
    "\n",
    "display(Agg.T)\n",
    "print(\"Min. transactions per cluster (including outliers) =\", Agg['count'].min())\n",
    "print(\"Max. transactions per cluster (including outliers) =\", Agg['count'].max())\n",
    "print(\"Average transactions per cluster (including outliers) =\", Agg['count'].mean())\n",
    "Agg.hist('count', color = 'violet', bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering\n",
    "\n",
    "Since we are trying to identify underlying traits exhibited within customer and subscriber classes across 22 attributes, we went into our spectral clustering methodology with high hopes. Fortunately, our results did not disappoint. By producing the Laplacian matrix and its associated eigenvectors, we were able to cluster data within a spectral space rather than the feature space.\n",
    "\n",
    "Unlike DBSCAN and more like K-means, we were required to identify an appropriate number of clusters to have the algorithm produce upfront. Unlike K-means, however, this was done by analyzing the eigenvalue spectrum for customer and subscriber data sets and locating the point at which the largest gap occurs between eigenvalues for each cluster count. This will be described in further detail in the respective customer/subscriber sections that follow.\n",
    "\n",
    "##### Customer Spectral Clustering\n",
    "\n",
    "Before implementing our Spectral Clustering permutations in the *Modeling and Evaluation Part 1 - Train and adjust parameters* section above, we calculated Similarity, Affinity, Degree, and Laplacian matrices manually. We also calculated Laplacian matrix eigenvalues at that time. This preparation was all meant to lead up to the eigenvalue spectrum plot displayed below.\n",
    "\n",
    "In this plot cluster counts are displayed on the x-axis with Laplacian eigenvalues on the y-axis. The dotted red line indicates the largest gap between consecutive eigenvalues and is generally the point at which the lower of the gap values is selected. Therefore, we identified 3 clusters as being the optimum count for our customer data and 3 is what we entered into our Spectral Clustering permutations earlier. More information regarding our approach is available in Ulrike von Luxburg's paper, [A Tutorial on Spectral Clustering](http://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evLc$values)[1:10], xlab = \"Customer Clusters\", ylab = \"Laplacian Eigenvalues\", main = \"Customer Eigenvalue Spectrum\")\n",
    "abline(v=3.5, col=\"red\", lty=2) # largest eigenvalue gap between 3 and 4 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping straight to our permutations, however, we did take interest in comparing within sum of squares while varying cluster counts. Below is a plot depicting the within sum of squares with each number of clusters. Note the only parameter that is being changed in our function call is the number of clusters.\n",
    "\n",
    "As can be seen, while 3 clusters does not produce the smallest within sum of squares, its Wss is the third smallest. This seems to be good compromise given the performance of the eignevalues above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "wss <- 0\n",
    "wss[1] <- ((length(mCitiC)/length(colnames(mCitiC)))-1)*sum(apply(mCitiC,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCitiC, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Customer Clusters\", ylab=\"Within groups sum of squares\", main = \"Customer WSS\", col = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- c(sum(withinss(sc_c)),\n",
    "         sum(withinss(sc_c1)),\n",
    "         sum(withinss(sc_c2)),\n",
    "         sum(withinss(sc_c3)),\n",
    "         sum(withinss(sc_c4)),\n",
    "         sum(withinss(sc_c5)))\n",
    "\n",
    "siz <- c(paste(size(sc_c), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c1), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c2), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c3), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c4), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c5), sep = '', collapse = ', '))\n",
    "\n",
    "its <- c(200,500,1000,1000,1000,1000)\n",
    "cen <- c(3,3,3,3,3,3)\n",
    "ker <- c(\"rbfdot\", \"rbfdot\", \"rbfdot\", \"laplacedot\", \"polydot\", \"rbfdot\")\n",
    "nys <- c(FALSE, FALSE, FALSE, FALSE, FALSE, TRUE)\n",
    "\n",
    "Cust_SC <- data.frame(cen, its, ker, nys, wss, siz)\n",
    "Cust_SC <- setNames(Cust_SC, c(\"Cluster.Count\", \"Iterations\", \"Kernel\", \"Nystrom.Method\", \"wss\", \"Sizes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theme of using within sum of squares continues as we now review the outcome of each tried configuration. Surprisingly, there was no difference in Wss or cluster size distributions as displayed in the table below (ordered by smallest to largest Wss). The only deviation from a Wss value of 24,927 was when choosing to implement the Nystrom Method (Wss = 22,842). Again, the Nystrom Method further samples the data when calculating the Laplacian eigenvectors. Interestingly enough, this approach not only produced the smallest Wss but also produced the most evenly distributed cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R Cust_SC[order(wss),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within sum of squares between each permutation are again depicted below in line chart form. Notice all Wss values are the same except the 6th permutation method's which utilizes Nystrom's Method for eigenvector calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cust_SC = %R Cust_SC\n",
    "fig = plt.figure()\n",
    "plt.plot(Cust_SC.index.values, Cust_SC.wss, 'ro-', linewidth=2)\n",
    "plt.title('Total Within Sum of Squares')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Total Wss')\n",
    "plt.annotate('Selected Configuration', xy=(6, 22841), xytext=(4.2, 22550),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05),)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another note about the Nystrom Method is its speed. While running our algorithm without Nystrom's Method enabled, our runtimes averaged about 30 seconds. Our algorithm with Nystrom's Method enabled averaged between only 400ms and 600ms. This is approximately a 98.3% reduction in runtime while still producing the best results! So 3 clusters, 1000 iterations, the Radial Basis kernel, and Nystrom Method enabled takes the clear win among customer Spectral Clustering permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber Spectral Clustering\n",
    "\n",
    "Similar to customer spectral clustering, we had calculated Similarity, Affinity, Degree, and Laplacian matrices manually in the *Modeling and Evaluation Part 1 - Train and adjust parameters* section along with the Laplacian matrix eigenvalues. This, too, led up to the eigenvalue spectrum plot displayed as follows.\n",
    "\n",
    "Subscriber Laplacian eigenvalues were a bit different from customer eigenvalues in that substantial gaps exist between both 4 and 5 clusters and 5 and 6 clusters. This means that either the first 5 or the first 6 eigenvectors contain all the cluster information needed to describe the subscriber data. For this reason we tried both counts in our subscriber permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evLs$values)[1:10], xlab = \"Subscriber Clusters\", ylab = \"Laplacian Eigenvalues\", log = \"y\", main = \"Subscriber Eigenvalue Spectrum\")\n",
    "abline(v=4.5, col=\"red\", lty=2) # largest eigenvalue gap between 4 and 5 clusters\n",
    "abline(v=5.5, col=\"blue\", lty=3) # Potential gap between 5 and 6 clusters too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we wanted to review within sum of squares while varying cluster counts before proceeding with trying our different parameters. Below is a plot depicting the within sum of squares with each number of clusters. Note the only parameter that is being changed in our function call is the number of clusters.\n",
    "\n",
    "As can be seen, both 4 and 5 clusters exemplify relatively small Wss values but are still larger than 2 or 3 clusters. It's also interesting to observe there is a dip in Wss values between 3 and 4 clusters before increasing again with 5 clusters. Again, while it is interesting to review this relationship between Wss and cluster count, our real strategy with Spectral Clustering is to select how many clusters to use based on the Laplacian eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "wss <- 0\n",
    "wss[1] <- ((length(mCitiS)/length(colnames(mCitiS)))-1)*sum(apply(mCitiS,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCitiS, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Subscriber Clusters\", ylab=\"Within groups sum of squares\", main = \"Subscriber WSS\", col = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- c(sum(withinss(sc_s)),\n",
    "         sum(withinss(sc_s6)),\n",
    "         sum(withinss(sc_s1)),\n",
    "         sum(withinss(sc_s2)),\n",
    "         sum(withinss(sc_s3)),\n",
    "         sum(withinss(sc_s4)),\n",
    "         sum(withinss(sc_s5)))\n",
    "\n",
    "siz <- c(paste(size(sc_s), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s6), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s1), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s2), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s3), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s4), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s5), sep = '', collapse = ', '))\n",
    "\n",
    "its <- c(200,200,500,1000,1000,1000,1000)\n",
    "cen <- c(4,5,5,5,5,5,5)\n",
    "ker <- c(\"rbfdot\", \"rbfdot\", \"rbfdot\", \"rbfdot\", \"laplacedot\", \"polydot\", \"rbfdot\")\n",
    "nys <- c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE)\n",
    "\n",
    "Sub_SC <- data.frame(cen, its, ker, nys, wss, siz)\n",
    "Sub_SC <- setNames(Sub_SC, c(\"Cluster.Count\", \"Iterations\", \"Kernel\", \"Nystrom.Method\", \"wss\", \"Sizes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, with an exception made for the Nystrom Method configuration, there was no difference in Wss or cluster size distributions as displayed in the table below for 5 clusters. There was, however, an expected difference between 4 and 5 cluster Wss values. What is unexpected is that the 4 cluster Wss value is larger than the 5 cluster value. This may be due to the randomized sampling method utilized by the R *specc* library. Regardless, the Nystrom Method still outperforms all other configurations in both Wss value and cluster size distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R Sub_SC[order(wss),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within sum of squares between each permutation are again depicted below in line chart form. Nystrom's Method clearly takes the win for smallest Wss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Sub_SC = %R Sub_SC\n",
    "fig = plt.figure()\n",
    "plt.plot(Sub_SC.index.values, Sub_SC.wss, 'ro-', linewidth=2)\n",
    "plt.title('Total Within Sum of Squares')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Total Wss')\n",
    "plt.annotate('Selected Configuration', xy=(7, 26926), xytext=(5.2, 26550),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05),)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again worth noting is the Nystrom Method's speed. While running our algorithm without Nystrom's Method enabled, our runtimes averaged about 30 seconds. Our algorithm with Nystrom's Method enabled averaged between only 400ms and 600ms. This is approximately a 98.3% reduction in runtime while still producing the best results! So 5 clusters, 1000 iterations, the Radial Basis kernel, and Nystrom Method enabled takes the clear win among subscriber Spectral Clustering permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Final Winner\n",
    "\n",
    "Based on the evaluations above, we have identified Spectral Clustering with 3 clusters, 1000 iterations, the Radial Basis kernel implemented, and Nystrom Method enabled as the clear winner for customer data and 5 clusters, 1000 iterations, the Radial Basis kernel implemented, and Nystrom Method enabled as the clear winner for subscriber data. It produces the smallest within sum of squares in the shortest amount of time. While all our other clustering methods suffer from the \"curse of dimensionality,\" the effects of large dimensionality in the feature space are significantly mitigated in the spectral space. All this is accomplished while producing a small, manageable amount of evenly sized clusters.\n",
    "\n",
    "Below, we merge the new cluster IDs with the scaled CitiBike customer and subscriber data and pickle the dataframe for further use as described in our Deployment section.\n",
    "\n",
    "As the final Spectral Clustering permutation is the only one we care about in the end, it will be our primary focus as we visualize cluster results in the *Modeling and Evaluation Part 3 - Visualize Results* section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R Cust_clus <- cbind(CitiBike_miniC, Cluster_ID_Customer = sc_c5, Cluster_ID_Subscriber = NaN)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cust_clus = %R Cust_clus\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/Cust_clus.pkl\"):\n",
    "    print(\"File already created\")\n",
    "else: pickleObject(Cust_clus, \"Cust_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R Sub_clus <- cbind(CitiBike_miniS, Cluster_ID_Customer = NaN, Cluster_ID_Subscriber = sc_s5)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sub_clus = %R Sub_clus\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/Sub_clus.pkl\"):\n",
    "    print(\"File already created\")\n",
    "else: pickleObject(Sub_clus, \"Sub_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 3 - Visualize Results\n",
    "\n",
    "With spectral clustering selected as our winner, we wanted visualize these identified clusters across the correlation between a few key attributes. Originally, during this process, we plotted all clusters at the same time with different colors representing clusters on the plot. When doing this, we quickly identified that this was not a feasible way to interpret the clusters because of how many overlapping data points we saw between clusters. (Example below)\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/NoisyVisualizations.png?raw=true\" width=\"300\">\n",
    "\n",
    "These overlapping data points and noise produced is likely due to the highly dimensional inputs utilized for our cluster model fit. Limitations of 3D visualizations do not give us nearly enough to truly see obvious clusters produced simply because we do not have the entire picture in the graph. \n",
    "\n",
    "Albeit we have encountered these challenges, we have decided to focus on 3D scatterplots for the relationship TripDurationLog has on Location and Weather data in respect to cluster assignments for both customer fit clusters and subscriber fit clusters. Given the noisy plots produced, we decided to subplot these visualizations by cluster id. This allows us to compare the clusters side by side instead of overlayed on top of each other. For each correlation 3d plot produced, we analyze the results from three different views by rotating x, y, and z axes. This helps us to see the data from different angles and elevations, potentially revealing additional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sub_clus = unpickleObject(\"Sub_clus\")\n",
    "Cust_clus = unpickleObject(\"Cust_clus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el):\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.gca(projection='3d')\n",
    "    ax.scatter(x, y, z, c=cs, cmap = 'prism', zdir = 'z')\n",
    "    ax.set_xlabel(xLab)\n",
    "    ax.set_ylabel(yLab)\n",
    "    ax.set_zlabel(zLab)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(xLim)\n",
    "    ax.set_ylim(yLim)\n",
    "    ax.set_zlim(zLim)\n",
    "    ax.view_init(azim=rot, elev = el) # Set rotation angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customers\n",
    "\n",
    "**Location by Trip Duration (Log) - Coordinate View** \n",
    "\n",
    "With X and Y axes as location coordinates, we were able to look at the data in a \"Top-down\" approach with high elevation in order to see if clusters appeared to target specific locations in NYC. We identified that clusters do not vary much in the northwest regions of the city, but we do see some slight variation in clusters in the southeast region. Cluster 1 appears to have data throughout the whole city, Cluster 2 has very sparse data in the southeast region and very dense data in the rest of the city, and finally Cluster 3 contains nearly no data at all in the southeast region of the city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custdistclusters = Cust_clus.Cluster_ID_Customer.drop_duplicates().sort_values().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "\n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_longitude\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_latitude\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.start_station_longitude.min(), Cust_clus.start_station_longitude.max())\n",
    "    yLim = (Cust_clus.start_station_latitude.min(),  Cust_clus.start_station_latitude.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(),         Cust_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -90\n",
    "    el = 80\n",
    "    \n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location by Trip Duration (Log) - Longitude View** \n",
    "\n",
    "Plotting the data at a 90 degree angle and very slight elevations lets us assess trip durations as start location coordinates move east to west across the map. We see pretty consistent variance in trip duration (log) from east coordinates to west coordinates *within* each cluster. This may suggest that location within these clusters does not play a huge role in relation to the trip duration. However, when looking at the variance of plotted points *across* clusters, there is a clear difference between the clusters. Cluster 1 contains trip duration (log) values ranging +/-4, Cluster 2 ranging +/-3, and Cluster 3 ranging +/-2. Interestingly, these differences in variance do not prevent overlapping data values ranges to occur. This suggests that other factors are at play, not plotted here which help to differentiate clusters. We also, once again, see the east coordinate data values dissapear in the third cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "    \n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_longitude\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_latitude\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.start_station_longitude.min(), Cust_clus.start_station_longitude.max())\n",
    "    yLim = (Cust_clus.start_station_latitude.min(),  Cust_clus.start_station_latitude.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(),         Cust_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -90\n",
    "    el = 10\n",
    "\n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location by Trip Duration (Log) - 3D View** \n",
    "\n",
    "This view, rotated at 60 degrees and an elevation of 45, was definitely the most difficult to interpret. We once again see the consistent variance seen within each cluster previously and the decreasing variance across clusters as you move from cluster 1 to 3. Further insights from this visualization are difficult to ascertain, given the missing dimensions utilized in the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "\n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_longitude\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].start_station_latitude\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.start_station_longitude.min(), Cust_clus.start_station_longitude.max())\n",
    "    yLim = (Cust_clus.start_station_latitude.min(),  Cust_clus.start_station_latitude.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(),         Cust_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -60\n",
    "    el = 45\n",
    "\n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather by Trip Duration (Log) - Weather View** \n",
    "\n",
    "We have chosen to assess Weather attributes of Average Temperature and Precipitation against trip duration, to identify interesting trends / features across clusters. In our first view, we plot high elevation in order to focus on the presence of data across clusters in varying weather circumstances. Given the fact that majority of rides are taken during non-rainy days, all three clusters contain dense regions around 0 precipitation and warmer temperatures. The interesting takeaway from this data, identifies the different \"tolerance\" levels of a customer rider. As you move from cluster 1 to cluster 3, you see the tolerance for bad weather (both cold temperatures and high precipitation tolerance) decrease. This may suggest that Customers within cluster 1 are more intense riders. One could speculate that the higher tolerance you have to bad weather as a customer, the more likely you may be to become a subscribing user. The correlation of weather attributes across clusters may be an important factor to identifying our target user segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "\n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].PRCP\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].TAVE\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.PRCP.min(),            Cust_clus.PRCP.max())\n",
    "    yLim = (Cust_clus.TAVE.min(),            Cust_clus.TAVE.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(), Cust_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Precipitation (scaled)'\n",
    "    yLab = 'Avg Temp (scaled)'\n",
    "    zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "    rot = -90\n",
    "    el = 80\n",
    "\n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather by Trip Duration (Log) - Rain View** \n",
    "\n",
    "Once again, we choose to plot the data at a 90 degree angle and very slight elevation. As discussed in the previous view, we see the same trend occuring with precipitation tolerance decreasing across clusters. In addition to this tolerance factor, we also see another interesting difference between clusters, as the range of trip duration values is much higher in clusters with larger weather tolerance. This supports the theory provided earlier, in which cluster 1 observations may be good target candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "\n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].PRCP\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].TAVE\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.PRCP.min(),            Cust_clus.PRCP.max())\n",
    "    yLim = (Cust_clus.TAVE.min(),            Cust_clus.TAVE.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(), Cust_clus.tripdurationLog.max())\n",
    "\n",
    "    xLab = 'Precipitation (scaled)'\n",
    "    yLab = 'Avg Temp (scaled)'\n",
    "    zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "    rot = -90\n",
    "    el = 10\n",
    "\n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather by Trip Duration (Log) - 3D View** \n",
    "\n",
    "Finally, our 3D rendered view, depicting all three attributes, provides further backing to our previous statements. The density around \"optimal weather\" conditions is definitely much tighter in cluster 3 in terms of both precipitation and temperatures than that of cluster 2 or cluster 1. The best way to describe these attributes accross clusters is simply levels of tolerance to weather conditions with 1 being highest tolerance(will ride no matter what) and 3 being lowest tolerance(will ride in optimal conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(custdistclusters)):\n",
    "\n",
    "    x  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].PRCP\n",
    "    y  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].TAVE\n",
    "    z  = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].tripdurationLog\n",
    "    cs = Cust_clus[Cust_clus[\"Cluster_ID_Customer\"] == i + 1].Cluster_ID_Customer\n",
    "\n",
    "    xLim = (Cust_clus.PRCP.min(),            Cust_clus.PRCP.max())\n",
    "    yLim = (Cust_clus.TAVE.min(),            Cust_clus.TAVE.max())\n",
    "    zLim = (Cust_clus.tripdurationLog.min(), Cust_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Precipitation (scaled)'\n",
    "    yLab = 'Avg Temp (scaled)'\n",
    "    zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "    rot = -60\n",
    "    el = 45\n",
    "\n",
    "    title = \"Customer Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscribers\n",
    "\n",
    "**Location by Trip Duration (Log) - Coordinate View** \n",
    "\n",
    "With X and Y axes as location coordinates, we were able to look at the data in a \"Top-down\" approach with high elevation in order to see if clusters appeared to target specific locations in NYC. With 5 different clusters, we thought we may see much more differentiation by location coordinates than that of the customer clusters. There seems to be a slightly noticable difference between location demographic within Cluster 2 in comparison to others, however the difference is very subtle and potentially caused by the difference in sample size within clusters. All other clusters resemble similar outcomes to one another and no true insights may be drawn as to what locations resemble which clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdistclusters = Sub_clus.Cluster_ID_Subscriber.drop_duplicates().sort_values().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(subdistclusters)):\n",
    "\n",
    "    x  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_longitude\n",
    "    y  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_latitude\n",
    "    z  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].tripdurationLog\n",
    "    cs = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].Cluster_ID_Subscriber\n",
    "\n",
    "    xLim = (Sub_clus.start_station_longitude.min(), Sub_clus.start_station_longitude.max())\n",
    "    yLim = (Sub_clus.start_station_latitude.min(),  Sub_clus.start_station_latitude.max())\n",
    "    zLim = (Sub_clus.tripdurationLog.min(),         Sub_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -90\n",
    "    el = 80\n",
    "    \n",
    "    title = \"Subscriber Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location by Trip Duration (Log) - Longitude View** \n",
    "\n",
    "Plotting the data at a 90 degree angle and very slight elevations lets us assess trip durations as start location coordinates move east to west across the map. Similar to the customer clusters, we see pretty consistent variance in trip duration (log) from east coordinates to west coordinates *within* each cluster. One may speculate that these location coordinates may be minimally impactful in our cluster fits and potentially in identifying usertypes as well. Variance in trip duration across clusters appears to change minimally, however it seems relatively unphased by the actual start location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(subdistclusters)):\n",
    "    \n",
    "    x  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_longitude\n",
    "    y  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_latitude\n",
    "    z  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].tripdurationLog\n",
    "    cs = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].Cluster_ID_Subscriber\n",
    "\n",
    "    xLim = (Sub_clus.start_station_longitude.min(), Sub_clus.start_station_longitude.max())\n",
    "    yLim = (Sub_clus.start_station_latitude.min(),  Sub_clus.start_station_latitude.max())\n",
    "    zLim = (Sub_clus.tripdurationLog.min(),         Sub_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -90\n",
    "    el = 10\n",
    "\n",
    "    title = \"Subscriber Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location by Trip Duration (Log) - 3D View** \n",
    "\n",
    "This view, rotated at 60 degrees and an elevation of 45, was once again the most difficult to interpret. We see the consistent variance seen within each cluster as was seen previously, as well as the differing trip duration variance across clusters as you move across clusters. Similar to our customer visualizations by location, further insights are difficult to ascertain, given the missing dimensions utilized in the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(subdistclusters)):\n",
    "\n",
    "    x  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_longitude\n",
    "    y  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].start_station_latitude\n",
    "    z  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].tripdurationLog\n",
    "    cs = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].Cluster_ID_Subscriber\n",
    "\n",
    "    xLim = (Sub_clus.start_station_longitude.min(), Sub_clus.start_station_longitude.max())\n",
    "    yLim = (Sub_clus.start_station_latitude.min(),  Sub_clus.start_station_latitude.max())\n",
    "    zLim = (Sub_clus.tripdurationLog.min(),         Sub_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Longitude (scaled)'\n",
    "    yLab = 'Latitude (scaled)'\n",
    "    zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "    rot = -60\n",
    "    el = 45\n",
    "\n",
    "    title = \"Subscriber Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather by Trip Duration (Log) - Weather View** \n",
    "\n",
    "Once again, we plot high elevation in order to focus on the presence of data across clusters in varying weather circumstances. Interestingly, there is a very clear differentiation between clusters in regards to temperatures. Both clusters 1 and 2, specifically target those subscribing members with a high tolerance to cold weather, whereas Clusters 3,4,5 all focus on warmer weather. This is much different resulting cluster features than what was found in the customer clusters, because of the hard break between clusters. As you look through the clusters targeting warmer weather, you can see a difference in precipitation tolerance with Cluster 4 having largest tolerance to rain and cluster 5 with the least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(subdistclusters)):\n",
    "\n",
    "    x  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].PRCP\n",
    "    y  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].TAVE\n",
    "    z  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].tripdurationLog\n",
    "    cs = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].Cluster_ID_Subscriber\n",
    "    \n",
    "    xLim = (Sub_clus.PRCP.min(),             Sub_clus.PRCP.max())\n",
    "    yLim = (Sub_clus.TAVE.min(),             Sub_clus.TAVE.max())\n",
    "    zLim = (Sub_clus.tripdurationLog.min(),  Sub_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Precipitation (scaled)'\n",
    "    yLab = 'Avg Temp (scaled)'\n",
    "    zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "    rot = -90\n",
    "    el = 80\n",
    "\n",
    "    title = \"Subscriber Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather by Trip Duration (Log) - 3D View** \n",
    "\n",
    "Although we have identified cluster trends by weather in our subscriber clusters, there does not appear to be much significance in trip duration across clusters. This may be due to the \"purpose\" subscribers are utilizing the service for. Regardless of rain or shine, our subscriber users are likely making the same routine trips which typically take the same amount of time each trip taken. As was determined by the customer cluster visualizations, we depict these clusters as identifying different tolerance levels amongst weather conditions. With the clear break in clusters present by temperature, and the different levels of precipitation tolerance amongst warmer weather rides - this appears to be a strong component in identifying these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(0,len(subdistclusters)):\n",
    "\n",
    "    x  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].PRCP\n",
    "    y  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].TAVE\n",
    "    z  = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].tripdurationLog\n",
    "    cs = Sub_clus[Sub_clus[\"Cluster_ID_Subscriber\"] == i + 1].Cluster_ID_Subscriber\n",
    "\n",
    "    xLim = (Sub_clus.PRCP.min(),             Sub_clus.PRCP.max())\n",
    "    yLim = (Sub_clus.TAVE.min(),             Sub_clus.TAVE.max())\n",
    "    zLim = (Sub_clus.tripdurationLog.min(),  Sub_clus.tripdurationLog.max())\n",
    "    \n",
    "    xLab = 'Precipitation (scaled)'\n",
    "    yLab = 'Avg Temp (scaled)'\n",
    "    zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "    rot = -60\n",
    "    el =  45\n",
    "\n",
    "    title = \"Subscriber Cluster {0}\".format(i+1)\n",
    "    ax = fig.add_subplot(2,3,i + 1, projection='3d')\n",
    "    scatter3d(ax,title,x,y,z,cs,xLab,yLab,zLab,xLim,yLim,zLim,rot,el)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Summary** \n",
    "\n",
    "Ultimately, given our highly dimensional cluster model fit inputs, it was very difficult to understand what makes a cluster unique from the others. There are many attributes left un-touched in these visualizations due to the limitations on our 3D plots. Within the attributes analyzed, we have come to the conclusion that location does not play a super large role in identifying cluster locations, with the exception of the southeast region of the city. Also, we have determined that one of the core components of the cluster segments is weather tolerance during a ride. Although this factor was deemed important, there are still several other attributes utilized when building these clusters and the interactions amongst these other factors could shed light on many other insights about our cluster segments produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 4 - Summarize the Ramifications\n",
    "\n",
    "As iterated several times over by this point, when pitting K-means, DBSCAN, and Spectral Clustering against one another, Spectral Clustering takes the win for both our customer and subscriber data sets. Reviewing the number of clusters alone, customer and subscriber counts were 7 and 11 for K-means, 49 and 36 for DBSCAN, and 3 and 5 clusters respectively for Spectral Clustering. DBSCAN was simply far too inefficient when including all clustering attributes (stations coordinates excluded) and time consuming, especially as the number of clusters produced was decreased. Then, when comparing the results of K-means against Spectral Clustering, clusters produced by Spectral Clustering exhibited the best cluster cohesion. Getting more specific still, implementing our Spectral Clustering algorithms using Nystrom's Method for eigenvector computation produced truly the best clustering results out of all configurations tested, from both a cohesion and resource perspective.\n",
    "\n",
    "But what are the ramifications of choosing this Spectral Clustering configuration? The biggest pitfall is that extreme sampling was required in order to make Spectral Clustering possible from a resource perspective. This means clustering for most transactions still remains. In addition to the sampling required, our decision to split the data set back into separate customer and subscriber data sets circumvents our business need to identify would-be/should-be subscribers among customer riders. We recognize the deficiency here but have fabricated a means for mitigating the effects thereof via classification on produced clusters (explored throughout the remainder of this section).\n",
    "\n",
    "In addition to executing cluster classification for missing values, we will also perform PCA analysis, similar to how we did during Lab 2, using these newly generated groupings. Throughout this additional analysis, we hope to extend description of our results and convey even more compelling findings than we have already throughout *Modeling and Evaluation Parts 1-3*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Clusters\n",
    "\n",
    "With our clusters fit and appended to the original data, we still do not know what customer cluster our subscriber observations are fit to and vice versa. In order to fit clusters for the inverse user type, we need to build a classification model that can predict the appropriate cluster with strong confidence. For these models, when focusing on accuracy well primarily use confusion matrices to explore our results alongside accuracy percentiles.\n",
    "\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 5 folds. This means, that from our original sample size of 1000, each \"fold\" will save off approximately 20% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst clusters. This process will occur through 5 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullunpicklepath = \"PickleFiles\\Sub_clus.pkl\"\n",
    "# Create an variable to pickle and open it in write mode\n",
    "unpicklefile = open(fullunpicklepath, 'rb')\n",
    "CitiBike_S_Clust = pickle.load(unpicklefile)\n",
    "unpicklefile.close()\n",
    "\n",
    "fullunpicklepath = \"PickleFiles\\Cust_clus.pkl\"\n",
    "# Create an variable to pickle and open it in write mode\n",
    "unpicklefile = open(fullunpicklepath, 'rb')\n",
    "CitiBike_C_Clust = pickle.load(unpicklefile)\n",
    "unpicklefile.close()\n",
    "\n",
    "    # Create CV Object for StratifiedKFold with 10 Folds, seeded at the length of our sample size\n",
    "seed = len(CitiBike_S_Clust)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. Values tested ranged from the default up until the maximum number of features passed in.\n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number of samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. Similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. **We tuned to provide output that completed all 10 iterations in under 10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def rfc_explor(ScaledData,\n",
    "               n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               y,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    accuracy = cross_val_score(rfc_clf, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,  \n",
    "                                         ScaledData,\n",
    "                                         y,\n",
    "                                         classes,\n",
    "                                         cv       = cv):\n",
    "    \n",
    "\n",
    "    X = ScaledData.as_matrix() \n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation\n",
    "\n",
    "    accuracy = []\n",
    "    \n",
    "    for (train, test) in cv.split(X, y):\n",
    "        clf.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "       \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "   \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(y[test], y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True)))   \n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =classes, \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Subscribers**\n",
    "\n",
    "After 14 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 87.6965 % with our worst parameter inputs of the random forest classification model to a value of 89.3968 % in the best tuned model fit. Also shown below is a stacked line chart of all accuracies across the 10 iterations, with different lines per parameter input. For all inputs provided, we saw less consistency than was desired - in our winning parameter inputs, we saw an accuracy metric ranging from 86 % - 94.0594 %. Part of this problem could be due to the very small sample size fit through spectral clustering. With more computational resources and time, we could have created a larger fit dataset to these clusters for training/testing operations. Given this report's focus on Clustering, and not on classification we have chosen to only train one model type. In a real-world setting, we would test various other classification methodologies to identify a model with the best consistent fit.\n",
    "\n",
    "Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "**Subscriber Parameters**\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>None</th>\n",
    "      <td>Auto</td>\n",
    "      <td>1</td>\n",
    "      <td>2</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#os.remove(\"PickleFiles/rfcdf_Subsc.pkl\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/rfcdf_Subsc.pkl\"):\n",
    "    print(\"File already created\")\n",
    "    rfcdf_Subsc = unpickleObject(\"rfcdf_Subsc\")\n",
    "else: \n",
    "    acclist = [] \n",
    "\n",
    "    n_estimators       =  [10    , 10    , 10   , 10  , 10   , 10  , 10  , 10      , 10     , 10     , 10    , 5       , 15      , 25     ]  \n",
    "    max_features       =  ['auto', 'auto', 8    , 12  , 16   , 20  , 22  , 'auto'  , 'auto' , 'auto' , 'auto', 'auto'  , 'auto'  , 'auto' ] \n",
    "    max_depth          =  [None  , None  , None , None, None , None, None, 100     , 50     , 25     , 10    , None    , None    , None   ] \n",
    "    min_samples_split  =  [2     , 8     , 2    , 2   , 2    , 2   , 2   , 2       , 2      , 2      , 2     , 2       , 2       , 2      ] \n",
    "    min_samples_leaf   =  [1     , 4     , 1    , 1   , 1    , 1   , 1   , 1       , 1      , 1      , 1     , 1       , 1       , 1      ]\n",
    "\n",
    "    for i in range(0,len(n_estimators)):\n",
    "        acclist.append(rfc_explor(ScaledData        = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                  n_estimators      = n_estimators[i],\n",
    "                                  max_features      = max_features[i],\n",
    "                                  max_depth         = max_depth[i],\n",
    "                                  min_samples_split = min_samples_split[i],\n",
    "                                  min_samples_leaf  = min_samples_leaf[i],\n",
    "                                  y = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].values\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "    rfcdf_Subsc = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                    \"n_estimators\": n_estimators,          \n",
    "                                                    \"max_features\": max_features,         \n",
    "                                                    \"max_depth\": max_depth,        \n",
    "                                                    \"min_samples_split\": min_samples_split,\n",
    "                                                    \"min_samples_leaf\": min_samples_leaf   \n",
    "                                                  }),\n",
    "                                   pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "    rfcdf_Subsc.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "\n",
    "    pickleObject(rfcdf_Subsc, \"rfcdf_Subsc\", filepath = \"PickleFiles/\")\n",
    "\n",
    "display(rfcdf_Subsc)\n",
    "\n",
    "\n",
    "plot = rfcdf_Subsc[[\"Iteration 0\",\"Iteration 1\",\"Iteration 2\",\"Iteration 3\",\"Iteration 4\",\"Iteration 5\",\"Iteration 6\",\"Iteration 7\",\"Iteration 8\",\"Iteration 9\"]].transpose().plot.line(title = \"Accuracies for Varying Parameters Across 10 Iterations\",rot=45)\n",
    "plot.set_xlabel(\"Iterations\")\n",
    "plot.set_ylabel(\"Accuracies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Customers**\n",
    "\n",
    "After 14 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 66.5054 % with our worst parameter inputs of the random forest classification model to a value of 73.7131 % in the best tuned model fit. Also shown below is a stacked line chart of all accuracies across the 10 iterations, with different lines per parameter input. For all inputs provided, we saw less consistency than was desired - in our winning parameter inputs, we saw an accuracy metric ranging from 63.7255 % - 81.1881 %. Part of this problem could be due to the very small sample size fit through spectral clustering. With more computational resources and time, we could have created a larger fit dataset to these clusters for training/testing operations. Given this report's focus on Clustering, and not on classification we have chosen to only train one model type. In a real-world setting, we would test various other classification methodologies to identify a model with the best consistent fit.\n",
    "\n",
    "Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "**Customer Parameters**\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>None</th>\n",
    "      <td>auto</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "      <td>25</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#os.remove(\"PickleFiles/rfcdf_Cust.pkl\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/rfcdf_Cust.pkl\"):\n",
    "    print(\"File already created\")\n",
    "    rfcdf_Cust = unpickleObject(\"rfcdf_Cust\")\n",
    "else: \n",
    "    acclist = [] \n",
    "\n",
    "    n_estimators       =  [10    , 10    , 10    , 10   , 10  , 10   , 10  , 10  , 10      , 10    , 10    , 10    , 5     , 15    , 25     ]  \n",
    "    max_features       =  ['auto', 'auto', 'auto', 8    , 12  , 16   , 20  , 22  , 'auto'  , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto' ] \n",
    "    max_depth          =  [None  , None  , None  , None , None, None , None, None, 100     , 50    , 25    , 10    , None  , None  , None   ] \n",
    "    min_samples_split  =  [2     , 8     , 12    , 8    , 8   , 8    , 8   , 8   , 8       , 8     , 8     , 8     , 8     , 8     , 8      ] \n",
    "    min_samples_leaf   =  [1     , 4     , 6     , 4    , 4   , 4    , 4   , 4   , 4       , 4     , 4     , 4     , 4     , 4     , 4      ]\n",
    "\n",
    "    for i in range(0,len(n_estimators)):\n",
    "        acclist.append(rfc_explor(ScaledData        = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                  n_estimators      = n_estimators[i],\n",
    "                                  max_features      = max_features[i],\n",
    "                                  max_depth         = max_depth[i],\n",
    "                                  min_samples_split = min_samples_split[i],\n",
    "                                  min_samples_leaf  = min_samples_leaf[i],\n",
    "                                  y = CitiBike_C_Clust[\"Cluster_ID_Customer\"].values\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "    rfcdf_Cust = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                    \"n_estimators\": n_estimators,          \n",
    "                                                    \"max_features\": max_features,         \n",
    "                                                    \"max_depth\": max_depth,        \n",
    "                                                    \"min_samples_split\": min_samples_split,\n",
    "                                                    \"min_samples_leaf\": min_samples_leaf   \n",
    "                                                  }),\n",
    "                                   pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "    rfcdf_Cust.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "\n",
    "    pickleObject(rfcdf_Cust, \"rfcdf_Cust\", filepath = \"PickleFiles/\")\n",
    "    \n",
    "display(rfcdf_Cust)\n",
    "\n",
    "plot = rfcdf_Cust[[\"Iteration 0\",\"Iteration 1\",\"Iteration 2\",\"Iteration 3\",\"Iteration 4\",\"Iteration 5\",\"Iteration 6\",\"Iteration 7\",\"Iteration 8\",\"Iteration 9\"]].transpose().plot.line(title = \"Accuracies for Varying Parameters Across 10 Iterations\",rot=45)\n",
    "plot.set_xlabel(\"Iterations\")\n",
    "plot.set_ylabel(\"Accuracies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Analyze the Results\n",
    "We have created a function to be used for our cross-validation Accuracy Scores. Model CLF object, original sample y values, a distinct list of classes, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results. \n",
    "\n",
    "With our tuned parameters identified we may now assess futher insights. **Subscribers:** As was discussed earlier, we see a fairly large range in accuracies, however the average accuracy rating of 89.589 % is fairly decent across all 10 iterations. In a confusion matrix of predicted results, we find that the most difficult cluster to predict and the cluster causing the most incorrect predictions for other classes is cluster 4. You can see this clearly in the gradient plot below as we have a larger density colors on cluster 4 false positives and incorrect cluster 4 predictions in comparison to other clusters. **Customers:** The customer demographic had much less accurate predictions despite the decrease in number of clusters. We once again see a large range in accuracies, with an average accuracy of 73.713 %. As is seen in the confusion matrix, we pretty consistently guess each cluster correct / incorrectly, and unlike the subscriber cluster set we do not see any potential trend for incorrect predictions off the bat. Although ~73% accuracy is not ideal, it is still more than 2x better than chance(33%), so we are comfortable with moving on with this model for this report.  \n",
    "\n",
    "Once again, in a real-world setting, we would test various other classification methodologies and utilize more resources to increase sample size in order to identify stronger cluster fits and a model with the best consistent fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15    ,\n",
    "                                 max_features       = 'auto',\n",
    "                                 max_depth          = None  , \n",
    "                                 min_samples_split  = 2     ,\n",
    "                                 min_samples_leaf   = 1     ,\n",
    "                                 n_jobs             = -1    , \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Subscriber, rfc_acc_Subscriber = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].values,\n",
    "                                                       classes     = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].drop_duplicates().sort_values().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 25    ,\n",
    "                                 max_features       = 'auto',\n",
    "                                 max_depth          = None  , \n",
    "                                 min_samples_split  = 8     ,\n",
    "                                 min_samples_leaf   = 4     ,\n",
    "                                 n_jobs             = -1    , \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Customer, rfc_acc_Customer = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_C_Clust[\"Cluster_ID_Customer\"].values,\n",
    "                                                       classes     = CitiBike_C_Clust[\"Cluster_ID_Customer\"].drop_duplicates().sort_values().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Predict Cluster Values for Inverse UserType Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_WithClusters = pd.concat([CitiBike_C, CitiBike_S])\n",
    "X = CitiBike_WithClusters.drop([\"usertype\"], axis=1)\n",
    "\n",
    "CitiBike_WithClusters[\"Cluster_ID_Customer\"]   = rfc_clf_Customer.predict(X)\n",
    "CitiBike_WithClusters[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)\n",
    "\n",
    "display(CitiBike_WithClusters.head())\n",
    "display(CitiBike_WithClusters.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA Loadings, do our newly added Cluster features add value?\n",
    "Our first objective is to identify the number of components to be used for user type classification. In order to do so, we first exclude redundant and non-value variables up front. Non-value variables include gender, birth year, and age since these data were missing for most Customer user types and were replaced with filler values as discussed in previous sections. We will exclude these since they misrepresent correlation with user type.\n",
    "\n",
    "*(Note: PCA code steps adapted and modified from https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    #Split Cluster_ID_Customer Values\n",
    "AttSplit = pd.get_dummies(CitiBike_WithClusters.Cluster_ID_Customer,prefix='Cluster_ID_Customer')\n",
    "CitiBike_WithClusters = pd.concat((CitiBike_WithClusters,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "    #Split Cluster_ID_Subscriber Values \n",
    "AttSplit = pd.get_dummies(CitiBike_WithClusters.Cluster_ID_Subscriber,prefix='Cluster_ID_Subscriber')\n",
    "CitiBike_WithClusters = pd.concat((CitiBike_WithClusters,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(CitiBike_WithClusters.head())\n",
    "\n",
    "myData_scaled_classification = CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).as_matrix()\n",
    "print(myData_scaled_classification.shape)\n",
    "\n",
    "maxcomp = 30\n",
    "\n",
    "pca_class = PCA(n_components=maxcomp, svd_solver='randomized')\n",
    "\n",
    "pca_class.fit(myData_scaled_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, We verify that 30 attributes exist in our dataset, after removed features for clustering and the addition of our cluster values. The maximum number of components to be produced will match this number. For this reason, we will identify 30 to be the number of components produced by our PCA and will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. Using full PCA required unacceptable lengths of time to compute.\n",
    "\n",
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Interestingly, the rate of change in explained variance among principal components has two \"stairs\", in the scree plot. This makes interpreting the number of components slightly more challenging, as one could argue either ~PC 5 or ~PC 17 for the point in which the plot levels off to marginal decreased explained variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,maxcomp+1), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, maxcomp+1, -0.1, 20])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance arguably begins to plateau around the 17th principal component. 17th principal component and that the first 17 components together explain 95.75% of variance in the data set. For this reason, 17 principal components may be selected as being the most appropriate for user type classification modeling given the variables among these data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "plt.plot(range(1,maxcomp+1), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, maxcomp+1, 10, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Loadings\n",
    "\n",
    "Now at the individual principal component level, each classification principal component has 30 loadings which may be esteemed as weights or coefficients representing each original attribute. These loadings represent to what extent each attribute fabricates a given principal component, and the relationship between these attributes in context of the principal component under review. Another perspective is that each principal component is describing an underlying factor which is comprised of the heaviest loadings.\n",
    "\n",
    "Rather than discuss all 30 principal components, we will instead focus on the two principal components with what we percieve as interesting loadings in respect to cluster values (PC1, PC4) within the first 17 Principal components. Each plot below has identified the top 15 attributes, both positive and negative, which describe the component analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 1 - Early Birds, Weekends, Mornings*\n",
    "\n",
    "We were happy to see 6 of the top 15 attributes within the first principal component, identified by our newly addeed customer / subscriber clusters. Interestingly, weather plays a big part in the relationship to these cluster values in PC1. With Temperature loadings approx. -0.51, we see negative correlations with SNOW(expected, as lower temperatures will produce a higher chance of snow) and our Subscriber/Customer Cluster 1 values. We found it interesting that we see consistent signs across the varying usertype clusters. Given the fact that Subscriber/Customer Cluster 1 are both positively signed at approx. .11, we may expect these clusters to react similarly in different weather conditions, mornings, and weekends. With negative correlation to Temperature, Mornings, and Weekend days - one might speculate this may be due to routine morning subscriber / Customer Cluster 1 routes in comparison to Subscriber Cluster 3,4 & Customer Cluster 2,3 excursions during good weather and / or weekends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[0], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 4 - Early Birds Vs Evening Rides*\n",
    "\n",
    "The second Principal Component we will discuss (PC4), identifies 6 of the top 15 attributes as our newly added cluster attributes. PC4 appears to rely heavily on the variance amongst ride start time. It appears as though the negative signs for Subscriber cluster 3 and Customer Clusters 3,4 match indicating their rider preference to be evening rides vs. mornings or nights. On the other side, we see that our positively signed Customer Cluster 1 and Subscriber clusters 4,5 all indicate preference towards morning and night rides over evening rides regardless of rain. This overlap, across cluster types, could be impactful towards identifying where customer behaviour resembles that of a subscriber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[3], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'SlateBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "### How useful is the model for interested parties?\n",
    "\n",
    "##### Classification of Usertype (Customer vs Subscriber)\n",
    "**How is this model useful?** Citibike aims to increase rider subscriptions, but may fear adverts to the wrong people could keep customers from enjoying their services. A classification model predicting usertype as customer or subscriber allows Citibike to identify those customers who meet the criteria for common subscribing members. False Positive predictions are extremely valueable to target this demographic.\n",
    "\n",
    "**How does the cluster features produced impact the model?**\n",
    "As previously discussed, we added two features to our dataset through spectral clustering. These features identified clusters based on both the subscriber and customer demographics. With the clusters identified, we were then capable of building classification models for each in order to append the cluster values to the entire dataset. Once these features were added to the dataset, running Principal Component Analysis identified several principal components which were heavily impacted by these new features. Our hopes, although not performed during this report, is that the addition of these cluster features will positively impact the accuracy of a classification model on predicting usertypes. \n",
    "\n",
    "**How would this model be deployed?** This model has the ability to be deployed as real-time predictions, or as a periodic corporate marketing alert if customer email addresses are readily available. Our preference would be a real-time prediction as a customer returns a bike. Upon return, if the model suggests a customer contains subcribing tendencies email alerts, pop-up promotions,etc. may be deployed in order to gain the customer's attention towards subscriber offerings. Alternatively, the marketing team could receive this information periodically, and implement custom strategies based on industry best practice techniques to this target group. This group of individuals are likely more apt to acknowledge these tactics positively, since their usage tendencies already align more closely to that of a subcribing member.\n",
    "\n",
    "**How often would the model need to be updated?** This model would definitely need to be updated periodically. As the citibike Share service grows, and new locations arrive, the model will need to be updated to account for the new sites. Also, as the population in NYC shifts over time(new businesses, schools, residential, etc.), trends may also fluctuate. These fluctuations will need to be accounted for in the model regularly to keep it up to date with current state NYC. Our recommendation for these updates would be periodic (monthly or quarterly) model fit updates in CitiBike systems to account for these possible changes.\n",
    "\n",
    "\n",
    "\n",
    "##### Additional Data to Collect:\n",
    "* **Event/Restaurant/Retail Data:** Given that we have detailed geocoordinate data and have already demonstrated powerful use of the Google Maps API, it would be possible to incorporate location details surrounding Citi Bike start and stop locations. There is potential for such data to be gathered automatically using API's such as Google's. Having this data would provide further insight into some of the reasons some bike share locations are more popular than others. Such data could even help Citi Bike predict changes in station demand based on changing surroundings and help determine where new stations should be installed.\n",
    "* **Special Events:** Similar to the previous idea, merging other public data based on geophysical coordinates and timeline could introduce other external factors such as the effects of parades, public concerts, festivals, and other events on ride activity for a given day or week. This would help identify/predict abnormal activity in this and future data sets. Additionally, it would provide insight to Citi Bike as to how to better plan and prepare for such events to boost rental count and increase trip duration.\n",
    "* **GPS Enabled Bike Computers:** Though not influenced by the data we have at hand, adding bicycle tracking hardware to each Citi Bike rental would provide substantial value to future data sets. Adding GPS tracking would enable Citi Bike to track specific routes followed by clients and could even aid NYC planners with transportation projects. Having route information means that true distance covered would be available, an attribute that would have far more meaning than our LinearDistance attribute. Incorporating GPS tracking with bike speed would provide insights into individual rider activity. For example, just because a rider's trip duration was 6 hours doesn't mean they actively rode for that amount of time. It is far more likely such a rider would have stopped for an extended period of time at least once during this period of time. Adding GPS and speed data would alleviate these existing gaps.\n",
    "\n",
    "### Deploying the Chosen Model on new data\n",
    "We have discussed above, what value our model holds, our preferred method of deployment, and frequency of model updates. Below we will walk through the process for prepping a real-time prediction model for deployment, and actually executing model predictions on new data inputs. \n",
    "\n",
    "##### Prepping the Model for Deployment\n",
    "A key component in deploying our model is the re-use of data transformation and / or Model fit objects created during this process. We need to be able to apply new data into the same constructs listed below, which were utilized in the Testing and Training process:\n",
    "* Standard Scaler\n",
    "* Model CLF Fit (RF decision trees)\n",
    "* Cluster Column List for Random Forest Prediction Input\n",
    "\n",
    "To do this, we must take our python objects currently stored in our active Kernel and permanently store them in a \"Pickled\" (.pkl) file. This .pkl file is a serializes version of our python object which can be accessed later on. To prove our pickled files are being utilized instead of previously created objects, we have executed the %reset jupyter command to clear all environment objects before proceding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "def pickleObject(objectname, filename, filepath = \"PickleFiles/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SubClusterCols = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() \n",
    "CusClusterCols = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objectlist   = [scaler,   rfc_clf_Subscriber,    rfc_clf_Customer,   SubClusterCols,   CusClusterCols   ]\n",
    "filenamelist = [\"scaler\", \"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(objectlist)):\n",
    "    pickleObject(objectlist[i], filenamelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from datetime import timedelta\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pyowm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions on New Data\n",
    "Below is a single data entry for a Customer upon the return of a bike rental. We have produces fake values, based on possible value ranges present in our original dataset from Citibike. For the purpose of this \"real-time\" prediction, the current date/time of execution is utilized for a user submitting a bike return. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "NewData = pd.DataFrame({\n",
    "                        \"tripduration\": [579],\n",
    "                        \"starttime\": [datetime.now() + timedelta(seconds = -579)],\n",
    "                        \"stoptime\": [datetime.now()],\n",
    "                        \"start_station_id\": [477],\n",
    "                        \"start_station_name\": [\"W 41 St & 8 Ave\"],\n",
    "                        \"start_station_latitude\": [40.756405],\n",
    "                        \"start_station_longitude\": [-73.990026],\n",
    "                        \"end_station_id\": [441],\n",
    "                        \"end_station_name\": [\"E 52 St & 2 Ave\"],\n",
    "                        \"end_station_latitude\": [40.756014],\n",
    "                        \"end_station_longitude\": [-73.967416],\n",
    "                        \"bikeid\": [16537],\n",
    "                        \"usertype\": [\"Customer\"],\n",
    "                        \"birthyear\": [\"\\\\N\"],\n",
    "                        \"gender\": [0]\n",
    "                      })\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformations and Additional Features**\n",
    "\n",
    "As was done previously during Data Understanding and Preparation, there are several attributes that we computed and / or obtained from third parties to complete our dataset. The following attributes are needed to be created:\n",
    "* LinearDistance\n",
    "* DayOfWeek\n",
    "* TimeOfDay\n",
    "* HolidayFlag\n",
    "* TMIN\n",
    "* TAVE\n",
    "* TMAX\n",
    "* PRCP\n",
    "* SNOW\n",
    "\n",
    "Since the current time was utilized for our bike return, we are able to utilize the openweathermap API for current weather forecasts for the current day. For this report, only the free features of this API are available. In a true production implementation, we recommend a subscription unlocking additional features for more accurate results and to increase the \"Calls per Minute\" limitations imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"LinearDistance\"] = NewData.apply(lambda x: vincenty((x[\"start_station_latitude\"], x[\"start_station_longitude\"]), \n",
    "                                                             (x[\"end_station_latitude\"],   x[\"end_station_longitude\"])).miles,\n",
    "                                          axis = 1)\n",
    "\n",
    "    \n",
    "    ## Compute DayOfWeek from Start Time\n",
    "        # starttime needs to be converted to a pandas datetime type before we can find the weekday name\n",
    "NewData['starttime'] = pd.to_datetime(NewData['starttime'])\n",
    "NewData[\"DayOfWeek\"] = NewData['starttime'].dt.weekday_name\n",
    "\n",
    "    ## Compute TimeOfDay from Start Time\n",
    "        ##Morning       5AM-10AM\n",
    "        ##Midday        10AM-2PM\n",
    "        ##Afternoon     2PM-5PM\n",
    "        ##Evening       5PM-10PM\n",
    "        ##Night         10PM-5AM\n",
    "\n",
    "\n",
    "NewData[\"TimeOfDay\"] = np.where((NewData['starttime'].dt.hour >= 5) & (NewData['starttime'].dt.hour < 10), 'Morning',\n",
    "                                np.where((NewData['starttime'].dt.hour >= 10) & (NewData['starttime'].dt.hour < 14), 'Midday',\n",
    "                                         np.where((NewData['starttime'].dt.hour >= 14) & (NewData['starttime'].dt.hour < 17), 'Afternoon',\n",
    "                                                  np.where((NewData['starttime'].dt.hour >= 17) & (NewData['starttime'].dt.hour < 22), 'Evening',\n",
    "                                                           'Night' ### ELSE case represents Night\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )\n",
    "                                                  \n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"HolidayFlag\"] = NewData['starttime'].isin(holidays.UnitedStates())\n",
    "NewData[\"HolidayFlag\"] = np.where(NewData[\"HolidayFlag\"] == False, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "owm = pyowm.OWM('462d2effa0ba127689b824b37efc9d12')  # You MUST provide a valid API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "forecaster = owm.three_hours_forecast_at_coords(lat = float(NewData[\"start_station_latitude\"]), lon = float(NewData[\"start_station_longitude\"]))\n",
    "forecast = forecaster.get_forecast()\n",
    "fweather_list = forecast.get_weathers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tlist = []\n",
    "plist = []\n",
    "slist = []\n",
    "\n",
    "\n",
    "for x in fweather_list:\n",
    "    date = datetime.date(datetime.strptime(x.get_reference_time('iso'),\"%Y-%m-%d %H:%M:%S+00\") + timedelta(hours = -5))\n",
    "    \n",
    "    if (date >= datetime.date(NewData[\"starttime\"].min())) \\\n",
    "    and (date < datetime.date(NewData[\"starttime\"].min())+ timedelta(days = 1)): \n",
    "        temp = x.get_temperature('fahrenheit')['temp']\n",
    "        prcp = x.get_rain()\n",
    "        snow = x.get_snow()\n",
    "        tlist.append(temp)\n",
    "        \n",
    "        if prcp == {}:\n",
    "            plist.append(0)\n",
    "        else:\n",
    "            plist.append(prcp)\n",
    "        \n",
    "        if snow == {}:\n",
    "            slist.append(0)\n",
    "        else:\n",
    "            slist.append(snow)\n",
    "        \n",
    "tempdata = pd.DataFrame(tlist)\n",
    "prcpdata = pd.DataFrame(plist)\n",
    "snowdata = pd.DataFrame(slist)\n",
    "\n",
    "NewData[\"TMIN\"] = tempdata.min()\n",
    "NewData[\"TAVE\"] = tempdata.mean()\n",
    "NewData[\"TMAX\"] = tempdata.max()\n",
    "NewData[\"PRCP\"] = prcpdata.sum()\n",
    "NewData[\"SNOW\"] = snowdata.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With additional features added, we have several missing values to scrub, the tripdurationlog attribute to compute, and data type conversions to apply to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "NewData[\"birthyear\"] = NewData[\"birthyear\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "NewData[['starttime', 'stoptime']] \\\n",
    "    = NewData[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "NewData[\"Age\"] = np.where(NewData[\"birthyear\"] == 0, 0,\n",
    "                                       NewData[\"starttime\"].dt.year - NewData[\"birthyear\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)\n",
    "\n",
    "# Log Transform Column Added\n",
    "NewData[\"tripdurationLog\"] = NewData[\"tripduration\"].apply(np.log)\n",
    "    \n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding categorical attributes**\n",
    "\n",
    "As was performed during our data preparation section of this report, we have several categorical data values which require encoding. Since we do not have the ability to execute dummy logic as before, we manually searched for the permutation of values assigning 1 if matched and 0 if not. The below categorical attributes have been encoded below:\n",
    "* DayOfWeek\n",
    "* TimeOfDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## DayOfWeek\n",
    "NewData[\"DayOfWeek_Monday\"] = np.where(NewData[\"DayOfWeek\"] == \"Monday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Tuesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Tuesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Wednesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Wednesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Thursday\"] = np.where(NewData[\"DayOfWeek\"] == \"Thursday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Friday\"] = np.where(NewData[\"DayOfWeek\"] == \"Friday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Saturday\"] = np.where(NewData[\"DayOfWeek\"] == \"Saturday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Sunday\"] = np.where(NewData[\"DayOfWeek\"] == \"Sunday\", 1, 0)\n",
    "\n",
    "## TimeOfDay\n",
    "NewData[\"TimeOfDay_Morning\"] = np.where(NewData[\"TimeOfDay\"] == \"Morning\", 1, 0)\n",
    "NewData[\"TimeOfDay_Midday\"] = np.where(NewData[\"TimeOfDay\"] == \"Midday\", 1, 0)\n",
    "NewData[\"TimeOfDay_Afternoon\"] = np.where(NewData[\"TimeOfDay\"] == \"Afternoon\", 1, 0)\n",
    "NewData[\"TimeOfDay_Evening\"] = np.where(NewData[\"TimeOfDay\"] == \"Evening\", 1, 0)\n",
    "NewData[\"TimeOfDay_Night\"] = np.where(NewData[\"TimeOfDay\"] == \"Night\", 1, 0)\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to deploying predictions on new data is to \"unpickle\" the .pkl file objects needed for our model. We need to unpickle the below constructs in order to re-produce our model, and apply it on new data:\n",
    "* Standard Scaler \n",
    "* Model CLF Fit (RF decision trees)\n",
    "* Cluster Column List for Random Forest Prediction Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def unpickleObject(filename, filepath = \"PickleFiles/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "    \n",
    "    return unpickleObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "unpickleObjectList = []\n",
    "filenamelist = [\"scaler\",\"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(filenamelist)):\n",
    "    unpickleObjectList.append(unpickleObject(filenamelist[i]))\n",
    "\n",
    "scaler = unpickleObjectList[0]\n",
    "rfc_clf_Subscriber = unpickleObjectList[1]\n",
    "rfc_clf_Customer = unpickleObjectList[2]\n",
    "SubClusterCols = unpickleObjectList[3]\n",
    "CusClusterCols = unpickleObjectList[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish the data in the standard scaler form it was in during clustering, we must transform our data similarly to how it was done during training. To do this, we utilize the transform operation under the unpickled scaler object in order to acheive similar results as if this record was part of the original trained dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NewData_Scaled = pd.DataFrame(scaler.transform(NewData[SubClusterCols]), columns = [SubClusterCols])\n",
    "display(NewData_Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scaled values transformed, we can create columns for \"Cluster_ID_Subscriber\" and \"Cluster_ID_Customer\" using the unpickled clf objects from the random forest classification models produced earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X = NewData_Scaled[SubClusterCols]\n",
    "\n",
    "NewData_Scaled[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X = NewData_Scaled[CusClusterCols]\n",
    "\n",
    "NewData_Scaled[\"Cluster_ID_Customer\"] = rfc_clf_Customer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "display(NewData_Scaled.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our clusters added to the dataset, the bike return data for this user is now finally ready to be utilzied for a classication / regression model of choice, including both Subscriber and Customer clusters. As discussed previously, given the purpose of this report and the focus on clustering - this model, and any data preparation steps for this model have not been prepared. \n",
    "\n",
    "Note that the run-time for all steps during the transformation/addition of new features to the original input complete in under 1 second during execution. This, of course, does not include the classification / regression model, but is a realistic timeframe up to this point for real-time application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "The following items were especially challenging during this project, we felt noteworthy to mention:\n",
    "* **High Dimensionality:** Most documentation found for clustering in general is based on 2D or 3D clustering. We wanted to produce clusters that attempted to describe the dataset in full. This produced several challenges when it came to runtimes(single iterations for fitting clusters taking 30 minutes to 3 hours each), and visualizing clusters in a way that made sense for contextual interpretations.    \n",
    "* **Utilization of R in jupyter: ** Only 1 of our 3 team members were able to correctly run rpy2 on their machines for this lab. This produced challenges as we tried to collaborate on work and use objects created from R code downstream in the project. \n",
    "* **Prediction of clusters: ** The rubric for this lab seemed to stop sooner than we thought relevant for a real-world application. We found that most cluster algorithms did not have a \"predict\" type of function. Once clusters were fit, you had the clusters for those observations, but not the cluster for new observations. Given the fact that our spectral clustering was so limited in terms of sample size, we had the majority of our dataset left unclustered. This, in application, would not be very useful. To mitigate this challenge, we decided to build a classification model utilizing random forest in order to train (10 fold) on the fit clusters - to fit a model ready to predict the missing cluster values efficiently for both subscriber and customer observations.\n",
    "* **Identifying the \"Value\" in our Clusters through PCA:** Once clusters were added to the dataset, we did not have peace of mind that the clusters were going to actually add \"value\" to Citibike. Without the time, nor project focus, on building a full classification model on predicting usertype - we still wanted to attempt to validate in some way our claims to valid clusters and uncover further insights in the process. We accomplished this through the utilization of PCA. After identifying the correct principal components within our dataset, we searched through the components looking for our newly added cluster columns. There were several Principal components which included our cluster columns, and two in particular that had 6 of the 8 cluster columns included within the top 15 attributes for the PC loading. This gave us peace of mind, that there was a strong possibility that these cluster columns could add value in a classification model downstream.\n",
    "* **Deployment:** In the last lab, we identified our recommendation for the value, deployment methodology, and frequency of updates for a classification model on usertype. The goals for this model remained the same during this lab, with the addition of our predicted clusters appended to the dataset - and the value added for these new attributes. Although, deployment methodology was identified, our team wanted to go one step further to produce code that could take our implemented data feature additions, transformations, and cluster predictions - and apply them to a new data observation. This code produces output, per our recommended real-time deployment methodology, that identifies all data needed(including cluster predictions) on a new data observation. This was done through utilization of .pkl files after a jupyter environment object reset to prove out the similar \"clean\" environment you may see in a Citibike return terminal and / or application. With these tasks produced, in addition to the classification work produced in the last lab, we feel as though this deployment area code would be close to ready for a production system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Py3]",
   "language": "python",
   "name": "conda-env-Py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
