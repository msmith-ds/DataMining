{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Classification and Regression -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 4/05/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Business Understanding\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for Lab 2 consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behavior.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this lab is to: 1)Fit clusters describing both customer and subscriber user types 2) Build a classification model for both customer and subscriber clusters 3) Predict customer clusters for subscriber observations and vice-versa providing a Classification-Ready dataset  for predicting whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, customer/subscriber cluster features, etc., and 4) provide a demonstration on how we would implement/deploy these clusters to a new data entry for use in a classification model real-time.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve these goals (as described further in the sections below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding 1\n",
    "***Describe the meaning and type of data***\n",
    "\n",
    "Before diving into each attribute in detail, one glaring facet of this data set that needs mentioning is its inherent time-series nature. By no means was this overlooked when we decided upon these particular data. To mitigate the effects of time on our analysis results, we have chosen to aggregate time-centric attributes such as dates and hours of the day by replacing them with simply the day of the week or period of the day (more on these details shortly). For example, by identifying trips occurring on July 1st, 2013, not by the date of occurrence but rather the day of the week, Monday, and identifying trips on July 2nd, 2013, as occurring on Tuesday, we will be able to obtain a \"big picture\" understanding of trends by day of the week instead of at the date-by-date level. We understand this is not a perfect solution since the time-series component is still an underlying factor in trip activity, but it is good enough to answer the types of questions we hope to target as described in the previous section as we will be comparing all Mondays against all Tuesdays, etc.\n",
    "\n",
    "As mentioned previously, the original data set ***from Citi Bike*** included 15 attributes. These 15 attributes and associated descriptions are provided below:\n",
    "1. **tripduration** - *Integer* - The total time (in seconds) a bike remains checked out, beginning with the start time and ending with the stop time\n",
    "2. **starttime** - *DateTime* - The date and time at which a bike was checked out, marking the start of a trip (i.e. 2/12/2014 8:16)\n",
    "3. **stoptime** - *DateTime* - The date and time at which a bike was checked back in, marking the end of a trip (i.e. 2/12/2014 8:16)\n",
    "4. **start_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station from which a bike is checked out\n",
    "5. **start_station_name** - *String* - The name of the station from which a bike is checked out; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "6. **start_station_latitude** - *Float* - The latitude coordinate for the station from which a bike is checked out (i.e. 40.74780373)\n",
    "7. **start_station_longitude** - *Float* - The longitude coordinate for the station from which a bike is checked out (i.e. -73.9900262)\n",
    "8. **end_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station in which a bike is checked in\n",
    "9. **end_station_name** - *String* - The name of the station at which a bike is checked in; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "10. **end_station_latitude** - *Float* - The latitude coordinate for the station at which a bike is checked in (i.e. 40.74780373)\n",
    "11. **end_station_longitude** - *Float* - The longitude coordinate for the station at which a bike is checked in (i.e. -73.9900262)\n",
    "12. **bikeid** - *String* - A categorical number value used to identify a particular bike; each bike in the bike share network has its own unique number\n",
    "13. **usertype** - *String* - A classifier attribute identifying a rider as a bike share subscriber or a one-time customer (i.e. Subscriber vs. Customer)\n",
    "14. **birth_year** - *Integer* - The year a rider was born (Only available for subscribed riders, however)\n",
    "15. **gender** - *String* - A categorical number value representing a rider's gender (i.e. 0 = unknown, 1 = male, 2 = female)\n",
    "\n",
    "\n",
    "It is important to note that birth year and gender details are not available for \"Customer\" user types but rather for \"Subscriber\" riders only. Fortunately, these are the only missing data values among all trips in the data set. Unfortunately, however, it means that we will not be able to identify the ratio of males-to-females that are not subscribed or use age to predict subcribers vs. non-subscribers (Customers). More to this end will be discussed in the next section.\n",
    "\n",
    "It is also worth mentioning that while attributes such as trip duration, start and end stations, bike ID, and basic rider details were collected and shared with the general public, care was taken by Citi Bike to remove trips taken by staff during system service appointments and inspections, trips to or from \"test\" stations which were employed during the data set's timeframe, and trips lasting less than 60 seconds which could indicate false checkout or re-docking efforts during checkin.\n",
    "\n",
    "Because some attributes may be deemed as duplicates (i.e. start_station_id, start_station_name, and start_station_latitude/longitude for identifying station locations), we chose to extract further attributes from the base attributes at hand. Further attributes were also extracted to mitigate the effects of time. In addition, we felt increased understanding could be obtained from combining weather data for the various trips as discussed in the previous section. These additional 10 attributes are described below:\n",
    "\n",
    "16. **LinearDistance** - *Integer* - The distance (miles) from a start station to an end station (as a crow flies); calculated from the latitude/longitude coordinates of start/end stations\n",
    "17. **DayOfWeek** - *String* - The day of the week a trip occurs regardless of time of day, month, etc.; extracted from the *starttime* attribute (i.e. Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)\n",
    "18. **TimeOfDay** - *String* - The portion of the day during which a bike was checked out; extracted from the *starttime* attribute (i.e. Morning, Midday, Afternoon, Evening, Night)\n",
    "19. **HolidayFlag** - *String* - A categorical binary value used to identify whether the day a trip occurred was on a holiday or not; extracted from the *starttime* attribute (i.e. 0 = Non-Holiday, 1 = Holiday)\n",
    "20. **Age** - *Integer* - The age of a rider at the time of a trip; calculated based on the *birth_year* attribute (Since only birth year is included in original Citi Bike data set, exact age at time of trip when considering birth month is not possible)\n",
    "21. **PRCP** - *Float* - The total recorded rainfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "22. **SNOW** - *Float* - The total recorded snowfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "23. **TAVE** - *Integer* - The average temperature throughout the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "24. **TMAX** - *Integer* - The maximum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "25. **TMIN** - *Integer* - The minimum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "\n",
    "After extracting our own attributes and merging weather data, the total number of attributes present in our final data set is 25. Only 15 are used throughout this lab, however, due to the duplicate nature of some attributes as discussed already. This final list of ***used*** attributes are tripduration, DayOfWeek, TimeOfDay, HolidayFlag, start_station_name, start_station_latitude, start_station_longitude, usertype, gender, Age, PRCP, SNOW, TAVE, TMAX, and TMIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250,000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gmaps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickleObject(objectname, filename, filepath = \"PickleFiles/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create a variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()\n",
    "    \n",
    "def unpickleObject(filename, filepath = \"PickleFiles/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "    \n",
    "    return unpickleObject\n",
    "    \n",
    "def clear_display():\n",
    "    from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Preparation 1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Part 1 - Define and prepare class variables\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4,881,384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680,909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to each other. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 hour period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however, we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding 2 - Visualize important attributes\n",
    "\n",
    "To re-iterate, our main objectives in analyzing these data are to determine which attributes have greatest bearing on predicting a rider's type (Customer vs. Subscriber) and to gain a better understanding of rider behavior as a function of external factors. Many attributes in this data set will eventually be used in subsequent labs to answer these questions. The primary attributes on which we will focus our attention in this section, however, are as follows:\n",
    "- Starting Location\n",
    "- Day of the Week\n",
    "- Time of Day\n",
    "- Trip Duration (both log and non-log)\n",
    "- Linear Distance\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "Over the course of this section, we will review these top attributes in some detail and discuss the value of using our chosen visualizations. Note also that merged weather data is of significant interest as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geophysical Start Stations HeatMap\n",
    "Before discussing the following heatmap in detail, it is worth noting some special steps required to use the gmaps module in Python in case the reader is interested in rendering our code to plot data on top of Google's maps (Note full instructions are available at https://media.readthedocs.org/pdf/jupyter-gmaps/latest/jupyter-gmaps.pdf)\n",
    "\n",
    "Besides having Jupyter Notebook installed on one's computer with extensions enabled (default if using Anaconda) and installing the gmaps module using pip, the following line should be run from within the command terminal. This is only to be done once and should be done when Jupyter Notebook is not running.\n",
    "```\n",
    "$ jupyter nbextension enable --py gmaps\n",
    "```\n",
    "In addition to running the above line in the command prompt, a Standard Google API user key will need obtained from https://developers.google.com/maps/documentation/javascript/get-api-key. This only needs done once and is necessary to pull the Google map data into the Jupyter Notebook environment. The key is entered in the *gmaps.configure()* line as shown in the below cell. We have provided our own private key in the meantime for the reader's convenience.\n",
    "\n",
    "Now on to the data visualization... This geological heatmap visualization is interactive; however, the kernel must run the code block each time our Jupyter Notebook file is opened due to the API key requirement. Therefore, we've captured some interesting views to aid in our discussion and have included them as embedded images.\n",
    "\n",
    "The start station heatmap represents the start station location data via attributes *start_station_latitude* and *start_station_longitude*. It identifies areas of highest and lowest concentration for trip starts. The location data is important as it helps us understand where the areas of highest activity are and, as will be seen in one of our later sections, will play an important role in identifying riders as regular customers or subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gmaps.configure(api_key=\"AIzaSyAsBi0MhgoQWfoGMSl5UcD-vR6H76cntxg\") # Load private Google API key\n",
    "\n",
    "locations = CitiBikeDataSampled_5050[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "m = gmaps.Map()\n",
    "heatmap_layer = gmaps.Heatmap(data = locations)\n",
    "m.add_layer(heatmap_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall view quickly reveals that station data was only provided for areas of NYC south of Manhattan and mostly north of Brooklyn. This could either mean that the bike share program had not yet expanded into these other areas at the time of data collection or that the data simply wasn't included (as mentioned previously, many test sites were being used during this time frame but CitiBike did not include them with this data set).\n",
    "\n",
    "Within the range of trip start frequency from the least number of trips (green) to the most trips (red), green and yellow indicate low to medium trip activity in most areas. However, higher pockets of concentration do exist in some places. We will attempt to put this visualization to good use by focusing in on one of these hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocations.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prominant heatspot occurs just east of Bryant Park and the Midtown map label. Zooming into this area (via regular Google Map controls as the rendered visual is interactive) allows for a closer look. A snapshot of this zoomed in image is embedded below. The hotspot seems slightly elongated and stands out from among the other stations. Zooming in further will help to understand why this is and may shed some light on the higher activity in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom1.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in to this area further helps us see that two stations are very close together. Even so, why might there be such high rider activity at these stations? This higher activity is likely affected by the stations' proximity to the famous Grand Central Station. As commuters and recreationalists alike arrive by train at Grand Central, it is natural that many of them may choose to continue their journey via the two closest bike share stations nearby. When the northernmost bike share station runs out of bikes, riders likely go to the next station to begin their ride instead.\n",
    "\n",
    "By understanding the dynamics of geographical activity within this data set and the amenities that surround each station, we will be able to more efficiently leverage the data to make our classification and regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom2.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geographic Heatmap Comparing Customer vs. Subscriber Start Station Activity\n",
    "\n",
    "After visualizing the overall dataset locations with a heatmap over NYC, we decided to take the visualization one step further. This time, we broke the dataset into two segments: Customer vs. Subscriber. Below is two separate gmap heatmaps containing geographic densities for each usertype. What we found assisted our theories on customer vs. subscriber usage tendencies. Seen first, the Customer heatmap overall contains much fewer dense regions. This helps to confirm our suspicions infering Customer bikers as less \"routine\" than subscribing bikers. When looking around for the most dense region in this heatmap, one point stuck out as particularly interesting: The Zoo. When comparing this region on the Subscriber gmap, we did not see the same type of traffic! This helps assist our theories that customer bikers use the service more for events, shopping,  or one-time use convenience. On the subscriber gmap, the most dense region, is that near the Grand Central Station as discussed earlier - assisting in the opposing theory for subscribing members as routine trips to work, groceries, etc. as they consistently use the bike share service as a means to reach the metro station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "customerData = CitiBikeDataSampled_5050.query('usertype == \"Customer\"')\n",
    "customerLoc = customerData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "cmap = gmaps.Map()\n",
    "customer_layer = gmaps.Heatmap(data=customerLoc)#, fill_color=\"red\", stroke_color=\"red\", scale=3)\n",
    "cmap.add_layer(customer_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/CMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "subscriberData = CitiBikeDataSampled_5050.query('usertype == \"Subscriber\"')\n",
    "subscriberLoc = subscriberData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "smap = gmaps.Map()\n",
    "subscriber_layer = gmaps.Heatmap(data=subscriberLoc)#, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "smap.add_layer(subscriber_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/SMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trip Duration and Linear Distance vs Weather by Customer/Subscriber\n",
    "\n",
    "Because we were able to bring together historical weather data for the dates we had in our records, we wanted to explore the relationship these variables had with our usertype status. If subscribers were regularly using the bikes for commuting as we've begun to see, then weather wouldn't impact their rental stastics as much as customers who appear to be primarily opportunistic in their usage.\n",
    "\n",
    "A quick cursory glance reveals a noticeable difference in bike rentals in regards to low temperatures, precipitation, and snowfall. While true, there are fewer customers than subscribers, we're concerned primarily with the spread or distribution of the plot points rather than the quantity. And we can see that on the customer pair plots that there are fewer points distributed across the lower temperature ranges and higher precipitation/snowfall ranges. The distributions pick back up at higher temperatures and lower precipitation points between the two usertypes.\n",
    "\n",
    "If stations consistently see use during \"bad\" weather, then those stations could be identified as subscriber stations. Further, if certain customers are found making the same trips consistently in all weather types, then they could be pushed for subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Customer'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Subscriber'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Trip Duration by Day of the Week Split Violin Plot\n",
    "\n",
    "Almost universally, across every day of the week, customers appear to have a higher trip duration than subscribers. While additional analysis will be required to confirm this, it's possible that one explanation is that subscribers can freely take and return their bikes which means that they're more willing to make shorter trips versus customers that pay each time they want to rent a bike in the first place. An alternate explanation, based on what we know in regards to the relationship between trip duration and linear distance traveled, is that subscribers are using the bikes for commuting to and from specific locations. This would result in a lower trip duration than customers that might use their bikes for general travel around the city. This possibility is corroborated by the decrease in activity on the weekends by subscribers.\n",
    "\n",
    "Identifying the point at which a customer might become a subscriber using this data would probably include monitoring weekday activity and trip duration. If a station has a lot of customers with trip durations similar to those of subscribers, then that station would be a good location to do a focused advertisement of the benefits of subscribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"tripdurationLog\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Linear Trip Distance by Day of the Week Split Violin Plot\n",
    "\n",
    "Unlike trip duration, the linear distance between start and end stations for both customers and subscribers appear to be similar in regards to means and are close in their quartiles. But what's noticeable here, is that customers are more widely distributed in how far or near they ride, with a significant increase in the number of customers that return their bikes to the station they started from.\n",
    "\n",
    "Further analysis will be necessary to explore the statistical significance of these differences, but it would be possible to identify those stations that are frequented by subscribers and assume that most stations within the first standard deviation of the linear distance found below to be considered \"subscriber stations\" and then seen which stations are outside of those zones to further build up the messaging encouraging subscription. Furthermore, by identifying those \"hot zones\" it's possible to rotate out bikes to increase their longevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"LinearDistance\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2 - Prepping for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "\n",
    "Initially including all start (and end) locations resulted in excessive system resource loading, later during randomized principal component computations, that froze our personal workstations and eventually ended with Python 'MemoryError' messaging. Therefore, due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size manually. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations) and including all of them resulted in system crashes during principal component analysis later in our lab. We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual integer columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these encodings complete, our final dataset to cross-validate on test/train datasets would appear to be complete. However, given the large number of attributes now present in our dataset, it would be wise to investigate a means of dimensionality reduction to not only speed up model generation, but to also improve accuracy by removing variable redundancy and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Set Summary\n",
    "\n",
    "At this stage, we've converted our original 30 variables into 107 attributes after creating dummy variables for categorical data such as day of the week, time of day, station names, gender, etc. These 107 attributes and their data types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(CitiBikeDataSampled_5050.columns):\n",
    "    data_type.append(CitiBikeDataSampled_5050.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(CitiBikeDataSampled_5050.columns, index = range(len(CitiBikeDataSampled_5050.columns))), 'Data Type' : pd.Series(data_type, index = range(len(CitiBikeDataSampled_5050.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put brief text in place of this referencing PCA in last lab and the columns that move forward\n",
    "Even with our data cleaned and prepped using OneHotEncoding, there is the innate need to reduce this number of attributes to a more manageable size before classification and regression predictions are made. For this reason, we've performed randomized PCA to compute linear combinations of the data and have chosen to use the first 14 principal components for the *n_components* parameter in our classification PCA and the first 15 principal components for regression PCA, based on explained variance and cumulative variance measures.\n",
    "\n",
    "Eigenvectors, proportions of explained variance, and cumulative proportions of explained variance are provided for the classification principal components and then again for regression principal components below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE START\n",
    "\n",
    "Because our stratified, processed data is comprised of 105 various attributes ranging from weather and distance data to location and user type data across all 500,000 sample observations, and some variables such as weather attributes and even some start and end stations correlate to one another, we felt it would be wise during our previous Lab 2 analysis to reduce our number of attributes considered during Customer/Subscriber prediction. We proceeded to use Principal Component Analysis (PCA) to reduce the dimensionality of our dataset.\n",
    "\n",
    "Furthermore, during analysis of our principal components' loadings, we identified only 22 of the originally processed 105 attributes as being contextually important in identifying Customers and Subscribers. As the intent of our Lab 3 analysis is to further identify Customer users that should be Subscribers based on their behaviour, we deem it wise to only use these 22 attributes while clustering as well.\n",
    "\n",
    "These attributes are as follows:\n",
    "\n",
    "* start_station_latitude\n",
    "* start_station_longitude\n",
    "* end_station_latitude\n",
    "* end_station_longitude\n",
    "* PRCP\n",
    "* SNOW\n",
    "* TAVE\n",
    "* TMAX\n",
    "* TMIN\n",
    "* DayOfWeek_Friday\n",
    "* DayOfWeek_Monday\n",
    "* DayOfWeek_Saturday\n",
    "* DayOfWeek_Sunday\n",
    "* DayOfWeek_Thursday\n",
    "* DayOfWeek_Tuesday\n",
    "* DayOfWeek_Wednesday\n",
    "* TimeOfDay_Afternoon\n",
    "* TimeOfDay_Evening\n",
    "* TimeOfDay_Midday\n",
    "* TimeOfDay_Morning\n",
    "* TimeOfDay_Night\n",
    "* tripdurationLog\n",
    "\n",
    "In addition to using only these attributes while clustering, we chose to split our stratified sample data set of 500,000 transactions into separate Customer and Subscriber data sets while clustering. This will provide us the advantage of identifying clusters based on Customer data separately from Subscriber data - the advantage being that further granularity will be offered into the user sub-groups (based on transaction details) that comprise each user class. When later comparing these clusterings between each user class, we will be able to further classify one user type's data against the opposite user type's cluster IDs. This, and its implementation, will be described in much greater detail in the Deployment section. Currently, it suffices to say that clustering against each known user type is a necessary means of identifying would-be Subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Subset data set to only variables identified to have the greatest PCA loadings\n",
    "attr_clus = CitiBikeDataSampled_5050[['start_station_latitude',\n",
    "                                      'start_station_longitude',\n",
    "                                      'end_station_latitude',\n",
    "                                      'end_station_longitude',\n",
    "                                      'PRCP',\n",
    "                                      'SNOW',\n",
    "                                      'TAVE',\n",
    "                                      'TMAX',\n",
    "                                      'TMIN',\n",
    "                                      'DayOfWeek_Friday',\n",
    "                                      'DayOfWeek_Monday',\n",
    "                                      'DayOfWeek_Saturday',\n",
    "                                      'DayOfWeek_Sunday',\n",
    "                                      'DayOfWeek_Thursday',\n",
    "                                      'DayOfWeek_Tuesday',\n",
    "                                      'DayOfWeek_Wednesday',\n",
    "                                      'TimeOfDay_Afternoon',\n",
    "                                      'TimeOfDay_Evening',\n",
    "                                      'TimeOfDay_Midday',\n",
    "                                      'TimeOfDay_Morning',\n",
    "                                      'TimeOfDay_Night',\n",
    "                                      'tripdurationLog',\n",
    "                                      'usertype']]\n",
    "\n",
    "attr_scaled = attr_clus.ix[:,0:(len(attr_clus.columns)-1)] #Remove usertype from scaled columns\n",
    "scaler = StandardScaler().fit(attr_scaled)\n",
    "CitiBike_clus = scaler.transform(attr_scaled)\n",
    "\n",
    "CitiBike_clus = pd.DataFrame(CitiBike_clus)\n",
    "users = pd.DataFrame(CitiBikeDataSampled_5050.usertype)\n",
    "CitiBike_clus = pd.concat([CitiBike_clus.reset_index(), users.reset_index()], axis = 1)\n",
    "del CitiBike_clus['index']\n",
    "CitiBike_clus.columns = attr_clus.columns\n",
    "\n",
    "CitiBike_C = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Customer']\n",
    "CitiBike_S = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Subscriber']\n",
    "\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "#scaled = min_max_scaler.fit_transform(CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)])\n",
    "#cols = CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)].columns\n",
    "#CitiBike_C = pd.DataFrame(scaled, columns=cols)\n",
    "#\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "#scaled = min_max_scaler.fit_transform(CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)])\n",
    "#cols = CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)].columns\n",
    "#CitiBike_S = pd.DataFrame(scaled, columns=cols)\n",
    "#\n",
    "print('Customer data dimensions =', CitiBike_C.shape)\n",
    "print('Subscriber data dimensions =',CitiBike_S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to separting the data based on user type, we also scaled the data values to remove bias while clustering due to different attribute scales. Without scaling the data, attributes such as station coordinates and trip duration would carry heavier weights when compared against the OneHotEncoded attributes and precipitation data. This would cause unbalanced and improperly clustered groups. The first 5 standardized ride transactions are shown below for Customers and Subscribers as an example of what the data looks like after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(CitiBike_C.head())\n",
    "display(CitiBike_S.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 1 - Train and adjust parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run kmeans algorithm (this is the most traditional use of k-means)\n",
    "kmeans = KMeans(init='k-means++', # initialization\n",
    "        n_clusters=3,  # number of clusters\n",
    "        n_init=20,       # number of different times to run k-means\n",
    "        n_jobs=-1)\n",
    "\n",
    "kmeans.fit(CitiBike_S.ix[:,:21])\n",
    "\n",
    "# visualize the data\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.plot(CitiBike_S.ix[:, 1], CitiBike_S.ix[:, 0], 'r.', markersize=2) #plot the data\n",
    "plt.scatter(centroids[:, 1], centroids[:, 0],\n",
    "            marker='+', s=200, linewidths=3, color='k')  # plot the centroids\n",
    "plt.title('K-means clustering for X1')\n",
    "plt.xlabel('X1, Feature 1')\n",
    "plt.ylabel('X1, Feature 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X2_N = 10\n",
    "\n",
    "# create connectivity graphs before calcualting the hierarchy\n",
    "X2_knn_graph = kneighbors_graph(CitiBike_S.ix[:,:21], X2_N, mode='distance') # calculate distance to four nearest neighbors \n",
    "\n",
    "N2 = X2_knn_graph.shape[0]\n",
    "X2_4nn_distances = np.zeros((N2,1))\n",
    "for i in range(N2):\n",
    "    X2_4nn_distances[i] = X2_knn_graph[i,:].max()\n",
    "\n",
    "X2_4nn_distances = np.sort(X2_4nn_distances, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(N2), X2_4nn_distances, 'r.', markersize=2) #plot the data\n",
    "plt.title('Dataset name: X2, sorted by neighbor distance')\n",
    "plt.xlabel('X2, Instance Number')\n",
    "plt.ylabel('X2, Distance to {0}th nearest neighbor'.format(X2_N))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X1 = df_imputed[['Pclass','Fare']]\n",
    "\n",
    "params = []\n",
    "for link in ['ward', 'complete', 'average']:\n",
    "    for n_fam in range(13,20):\n",
    "\n",
    "        # append on the clustering\n",
    "        cls_fam = AgglomerativeClustering(n_clusters=n_fam, linkage=link)\n",
    "        cls_fam.fit(X2)\n",
    "        newfeature_fam = cls_fam.labels_ # the labels from kmeans clustering\n",
    "\n",
    "        y = df_imputed['Survived']\n",
    "        X = df_imputed[['IsMale','Pclass','Fare']]\n",
    "        X = np.column_stack((X,pd.get_dummies(newfeature_fam)))\n",
    "\n",
    "        acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "        params.append((n_fare,n_fam,acc.mean()*100,acc.std()*100)) # save state\n",
    "\n",
    "        print (\"C=\",n_fam,link,\"Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = AgglomerativeClustering(n_clusters=14, linkage='complete')\n",
    "cls.fit(data)\n",
    "hac_labels = cls.labels_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "\n",
    "Our third approach to clustering the CitiBike user data is to implement DBSCAN on the 22 attributes selected for clustering. However, initial clustering attempts while including longitude and latitude data resulted in processing failure due to memory errors. For this reason, we have chosen to remove start and end station coordinate data from the attributes identified for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']]\n",
    "C_dbscan = CitiBike_C.ix[:,4:22]\n",
    "S_dbscan = CitiBike_S.ix[:,4:22]\n",
    "CitiBike_loc = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "print('Customer dbscan data dimensions =', C_dbscan.shape)\n",
    "print('Subscriber dbscan data dimensions =', S_dbscan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate removal aside, we do have some reservations about implementing DBSCAN clustering due to the density method being based on Euclidean distance measures. Even with 18 attributes instead of 22, the high dimensionality within our Customer and Subscriber data sets is expected to minimize the effectiveness of using distance as the primary measure. Nevertheless, such an attempt is still worth while in order to compare the results against our other clustering methods.\n",
    "\n",
    "By plotting cluster data against the geographical coordinates for each CitiBike station, we expect to be able to gain better insight into which cluster ID's appear at each station (Again, coordinate attributes are removed from the DBSCAN cluster data... plotting against coordinates is for marketing application purposes only). The initial coordinates, without clustering, are depicted in the following scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rand_jitter(arr):\n",
    "    stdev = arr.max()/100.\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "def jitter(x, y, s=20, c='b', marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs):\n",
    "    return plt.scatter(rand_jitter(x), rand_jitter(y), s=s, c=c, marker=marker, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(15,5))\n",
    "#plt.plot(CitiBike_dbscan.start_station_longitude, CitiBike_dbscan.start_station_latitude, 'bo', markersize=3) #plot the data\n",
    "plt.plot(CitiBike_loc.start_station_longitude, CitiBike_loc.start_station_latitude, 'bo', markersize=4, markerfacecolor='b') #plot the data\n",
    "plt.title('Latitude/Longitude Data'.format(2))\n",
    "plt.xlabel('Scaled Longitude Coordinate'.format(2))\n",
    "plt.ylabel('Scaled Latitude Coordinate'.format(2))\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(-3,2.5)\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.grid()\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously while K-Means and Hierarchical clustering, customer and subscriber data will be clustered separately in order to help identify customers that should be subscribers but aren't, as will be discussed in greater detail in the Deployment section.\n",
    "\n",
    "Below are functions that will be utilized throughout the DBSCAN clustering process. The *getGraph* function simply generates a k-neighbors graph of the data in order to plot potential *eps* values based on a prescribed minimum number of samples to be used in the DBSCAN algorithm. Next, the *epsPlot* function plots these potential *eps* values. Finally, the third function, *dbs*, clusters the data based on prescribed *eps* and minimum number of samples parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def getGraph(N, data):\n",
    "    graph = kneighbors_graph(data, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    #CitiBike_knn_graph = kneighbors_graph(CitiBike_clus, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def epsPlot(N, graph):\n",
    "    N1 = graph.shape[0]\n",
    "    CitiBike_distances = np.zeros((N1,1))\n",
    "    for i in range(N1):\n",
    "        CitiBike_distances[i] = graph[i,:].max()\n",
    "\n",
    "    CitiBike_distances = np.sort(CitiBike_distances, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #plt.subplot(1,2,1)\n",
    "    plt.plot(range(N1), CitiBike_distances, 'r.', markersize=4) #plot the data\n",
    "    plt.title('Dataset name: CitiBike_clus, sorted by neighbor distance')\n",
    "    plt.xlabel('CitiBike_clus, Instance Number')\n",
    "    plt.ylabel('CitiBike_clus, Distance to {0}th nearest neighbor'.format(N))\n",
    "    #plt.xlim([400000,500000])\n",
    "    #plt.annotate('Expected Eps value = approx. 0.0054', xy=(287, 0.0054), xytext=(200, 0.007),\n",
    "    #            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "    #plt.plot([0, 350], [0.0054, 0.0054], 'k--', lw=0.5)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def dbs(eps, minpts, data):\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_dbscan)\n",
    "    db = DBSCAN(eps=eps, min_samples=minpts, n_jobs=-1).fit(data)\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts, metric='cosine', algorithm='brute').fit(CitiBike_clus)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    # mark the samples that are considered \"core\"\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    plt.figure(figsize=(15,4))\n",
    "    unique_labels = set(labels) # the unique labels\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "        \n",
    "        if(data.equals(C_dbscan)):\n",
    "            xy = CitiBike_C[class_member_mask & core_samples_mask]\n",
    "        elif(data.equals(S_dbscan)):\n",
    "            xy = CitiBike_S[class_member_mask & core_samples_mask]\n",
    "        # plot the core points in this class\n",
    "        #plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "        #         markeredgecolor='w', markersize=6)\n",
    "        jitter(xy.start_station_longitude, xy.start_station_latitude, c=col, s=10)\n",
    "\n",
    "        # plot the remaining points that are edge points\n",
    "        if(data.equals(C_dbscan)):\n",
    "            xy = CitiBike_C[class_member_mask & ~core_samples_mask]\n",
    "        elif(data.equals(S_dbscan)):\n",
    "            xy = CitiBike_S[class_member_mask & ~core_samples_mask]\n",
    "        #plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "        #         markeredgecolor='w', markersize=3)\n",
    "        jitter(xy.start_station_longitude, xy.start_station_latitude, c=col, s=10)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    plt.xlim(-10,10)\n",
    "    plt.ylim(-3,2.5)\n",
    "    plt.grid()\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.show()\n",
    "    \n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer DBSCAN\n",
    "\n",
    "Before running the DBSCAN algorithm on our customer data set, we would first like to obtain a basic understanding of what types of eps and minimum number of samples to use. Due to the multi-dimensionality of our data, and the fact that we have 250,000 observations in our customer data set, it is difficult to define an appropriate mininum number of samples through visual inspection of the data. Therefore, we have chosen a preliminary sample count of 100 from which we will identify an eps value to use as our initial starting point.\n",
    "\n",
    "The plot below....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 100\n",
    "CitiBike_knn_graph = getGraph(N, C_dbscan)\n",
    "epsPlot(N, CitiBike_knn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c = unpickleObject(\"db_c\")\n",
    "else: db_c = dbs(0.5, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c1 = unpickleObject(\"db_c1\")\n",
    "else: db_c1 = dbs(0.7, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c2 = unpickleObject(\"db_c2\")\n",
    "else: db_c2 = dbs(0.9, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c3 = unpickleObject(\"db_c3\")\n",
    "else: db_c3 = dbs(1.0, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c4 = unpickleObject(\"db_c4\")\n",
    "else: db_c4 = dbs(1.0, 250, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c5 = unpickleObject(\"db_c5\")\n",
    "else: db_c5 = dbs(1.0, 300, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c6 = unpickleObject(\"db_c6\")\n",
    "else: db_c6 = dbs(1.2, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_c7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_c7 = unpickleObject(\"db_c7\")\n",
    "else: db_c7 = dbs(2.0, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pickleObject(db_c, \"db_c\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c1, \"db_c1\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c2, \"db_c2\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c3, \"db_c3\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c4, \"db_c4\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c5, \"db_c5\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c6, \"db_c6\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_c7, \"db_c7\", filepath = \"PickleFiles/\")\n",
    "#pickleObject(Cust_clus, \"Cust_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 100\n",
    "#CitiBike_knn_graph = kneighbors_graph(S_dbscan, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "#CitiBike_knn_graph = kneighbors_graph(CitiBike_clus, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "CitiBike_knn_graph = getGraph(N, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epsPlot(N, CitiBike_knn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s = unpickleObject(\"db_s\")\n",
    "else: db_s = dbs(1.0, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s1.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s1 = unpickleObject(\"db_s1\")\n",
    "else: db_s1 = dbs(0.8, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s2.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s2 = unpickleObject(\"db_s2\")\n",
    "else: db_s2 = dbs(0.5, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s3.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s3 = unpickleObject(\"db_s3\")\n",
    "else: db_s3 = dbs(0.5, 150, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s4.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s4 = unpickleObject(\"db_s4\")\n",
    "else: db_s4 = dbs(0.5, 250, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s5.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s5 = unpickleObject(\"db_s5\")\n",
    "else: db_s5 = dbs(0.5, 300, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s6.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s6 = unpickleObject(\"db_s6\")\n",
    "else: db_s6 = dbs(0.5, 350, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s7.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s7 = unpickleObject(\"db_s7\")\n",
    "else: db_s7 = dbs(0.5, 450, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s8.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s8 = unpickleObject(\"db_s8\")\n",
    "else: db_s8 = dbs(1, 450, S_dbscan) ## This is our WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile(\"PickleFiles/db_s9.pkl\"):\n",
    "    print(\"Found the File!\")\n",
    "    db_s9 = unpickleObject(\"db_s9\")\n",
    "else: db_s9 = dbs(1.2, 450, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pickleObject(db_s, \"db_s\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s1, \"db_s1\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s2, \"db_s2\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s3, \"db_s3\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s4, \"db_s4\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s5, \"db_s5\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s6, \"db_s6\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s7, \"db_s7\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s8, \"db_s8\", filepath = \"PickleFiles/\")\n",
    "pickleObject(db_s9, \"db_s9\", filepath = \"PickleFiles/\")\n",
    "#pickleObject(Sub_clus, \"Sub_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering\n",
    "\n",
    "The final clustering method we will apply to our data set is Spectral Clustering. Due to the multi-dimensional state of our data, our expectation is that spectral clustering will outperform K-means, Hierarchical, and DBSCAN clustering as it does not rely on distance measures but rather node connectivity. Another expected advantage to using Spectral Clustering is that it allows us to reduce the dimensionality of our data by clustering the data's first two Laplacian matrix eigenvectors only instead of all 22 attributes. As such, it does require a bit more data prep before clustering such as computing Similarity and Affinity, Degree, and Laplacian matrices (determining an appropriate number of clusters in the process), but the anticipated outcome is worth the effort.\n",
    "\n",
    "For Spectral Clustering we chose to use R not only for a change of pace but also because documentation for running the algorithm in R is plentiful and application using R overlapped with one of our group members' projects at work. A similar outcome could have been reached with Python just as well, but integrating R offered a fun, new challenge on our Windows OS machines.\n",
    "\n",
    "Before computing our required matrices, we must first intialize our **rpy2.ipython** session and pass to it our scaled Customer and Subscriber data. We also check to ensure the data imported correctly as an R dataframe and that attribute data types are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "#%R install.packages(\"kernlab\")\n",
    "%R library(kernlab)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%R -i CitiBike_C\n",
    "%R -i CitiBike_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = %R capture.output(str(CitiBike_C))\n",
    "\n",
    "for line in out:\n",
    "         print(line)\n",
    "print(\"\")\n",
    "out = %R capture.output(str(CitiBike_S))\n",
    "\n",
    "for line in out:\n",
    "         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial attempts to compute similarity matrices for all 250,000 transactions from each user type data set, it became quickly apparent we would need to sub-sample our data further. We discovered the relationship between sample size and CPU time required to compute the similarity, affinity, degree, and laplacian matrices is not linear. For example, when random-sampling 500 transcations from each data set, matrix computations took 11 minutes to complete for each data set (22 minutes in total between both data sets). With 1000 samples randomly selected for matrix derivation, however, required CPU runtime significantly increased to 44 minutes per data set (that's an 1.5 hours in total between both data sets). Given the significant amount of time spent running our DBSCAN code above, we opted to sample no more than 1000 transactions for purpose of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "set.seed(100)\n",
    "CitiBike_miniC <- CitiBike_C[sample(1:nrow(CitiBike_C), 1000, replace=FALSE),]\n",
    "CitiBike_miniS <- CitiBike_S[sample(1:nrow(CitiBike_S), 1000, replace=FALSE),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In final preparation for matrix computation, we setup our similarity and affinity matrix functions which are a bit more extensive than our degree and Laplacian matrices. These functions were adapted from http://www.di.fc.ul.pt/~jpn/r/spectralclustering/spectralclustering.html. Similarly, we based our eigenvector derivations on this same tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "# Similarity matrix computation functions\n",
    "s <- function(x1, x2, alpha=1) {\n",
    "  exp(- alpha * norm(as.matrix(x1-x2), type=\"F\"))\n",
    "}\n",
    "\n",
    "make.similarity <- function(my.data, similarity) {\n",
    "  N <- nrow(my.data)\n",
    "  S <- matrix(rep(NA,N^2), ncol=N)\n",
    "  for(i in 1:N) {\n",
    "    for(j in 1:N) {\n",
    "      S[i,j] <- similarity(my.data[i,], my.data[j,])\n",
    "    }\n",
    "  }\n",
    "  S\n",
    "}\n",
    "\n",
    "# Affinity matrix computation functions\n",
    "make.affinity <- function(S, n.neighboors=2) {\n",
    "  N <- length(S[,1])\n",
    "\n",
    "  if (n.neighboors >= N) {  # fully connected\n",
    "    A <- S\n",
    "  } else {\n",
    "    A <- matrix(rep(0,N^2), ncol=N)\n",
    "    for(i in 1:N) { # for each line\n",
    "      # only connect to those points with larger similarity \n",
    "      best.similarities <- sort(S[i,], decreasing=TRUE)[1:n.neighboors]\n",
    "      for (s in best.similarities) {\n",
    "        j <- which(S[i,] == s)\n",
    "        A[i,j] <- S[i,j]\n",
    "        A[j,i] <- S[i,j] # to make an undirected graph, ie, the matrix becomes symmetric\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  A  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer Spectral Clustering\n",
    "\n",
    "We first tackle the Customer data set by calculating our similarity, affinity, degree, and Laplacian matrices. We opted to implement the unnormalized graph Laplacian for our application as our data is already scaled. Due to matrix computation CPU requirements (Averaging 44 minutes), we set our Laplacian matrix object aside as a RDS file in this case similar to our previous pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "if(!file.exists('PickleFiles/CustLap.rds')){\n",
    "    S <- make.similarity(CitiBike_miniC, s)\n",
    "    A <- make.affinity(S, 3)  # use 3 neighboors (includes self)\n",
    "    D <- diag(apply(A, 1, sum)) # sum rows\n",
    "    U <- D - A\n",
    "    saveRDS(U, 'PickleFiles/CustLap.rds')\n",
    "}\n",
    "else{U <- readRDS('PickleFiles/CustLap.rds')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the final Laplacian matrix is comlete, we move on to eigenvector computation. Specifically, we are interested in deriving the first eigenvector and the second eigenvector, or Fiedler Vector. These are required for us to cluster our data in a two dimensional space by identifying a location of normalized cut in the Fiedler Vector. This cut location is revealed by analyzing both the number of edges in each cluster that do not connect to other clusters and the number of edges which do connect each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "k   <- 2\n",
    "evL <- eigen(U, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]\n",
    "\n",
    "signif(evL$values,2) # eigenvalues are in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "mCitiC <- data.matrix(CitiBike_miniC)\n",
    "colnames(mCitiC) <- colnames(CitiBike_miniC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- 0\n",
    "wss[1] <- ((length(mCitiC)/length(colnames(mCitiC)))-1)*sum(apply(mCitiC,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCitiC, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Customer Clusters\", ylab=\"Within groups sum of squares\", main = \"Customer WSS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%R colnames(mCiti) <- colnames(CitiBike_clus[5:22])\n",
    "#%R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c <- specc(mCitiC, centers = 3)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "jitplot <- function(clusts, iteration){\n",
    "    plot(jitter(mCitiC[,2],200), jitter(mCitiC[,1],200), col = clusts, pch = 19, xlim = c(-3,4), ylim = c(-3, 2), xlab = 'Customer Longitude Coordinates', ylab = 'Customer Latitude Coordinates', main = paste('Config', iteration, 'Coordinates'))\n",
    "    legend('topright', title = 'Cluster ID', legend = sort(unique(clusts)), col = sort(unique(clusts)), pch = 19)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#iteration <- 1\n",
    "#jitplot(sc_c, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c1 <- specc(mCitiC, centers = 3, iterations = 500)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c1, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c1)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c2 <- specc(mCitiC, centers = 3, iterations = 1000)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c2, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c2)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c3 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'laplacedot')\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c3, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c3)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c4 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'polydot')\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c4, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c4)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_c5 <- specc(mCitiC, centers = 3, iterations = 1000, kernel = 'rbfdot', nystrom.red = TRUE)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_c5, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_c5)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R Cust_clus <- cbind(CitiBike_miniC, Cluster_ID_Customer = sc_c5, Cluster_ID_Subscriber = NaN)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cust_clus = %R Cust_clus\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/Cust_clus.pkl\"):\n",
    "    print(\"File already created\")\n",
    "else: pickleObject(Cust_clus, \"Cust_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "for (i in sort(unique(sc_c5))){\n",
    "    #plot(Cust_clus[Cust_clus$Cluster_ID_Customer == i,]$`start_station_longitude`, Cust_clus[Cust_clus$Cluster_ID_Customer == i,]$`star_station_latitude`, col=sc_c5, pch=19)\n",
    "    plot(mCitiC[Cust_clus$Cluster_ID_Customer == i,2], mCitiC[Cust_clus$Cluster_ID_Customer == i,1], col = i, pch = 19, xlim = c(-3,4), ylim = c(-3, 2), xlab = 'Customer Longitude Coordinates', ylab = 'Customer Latitude Coordinates', main = paste('Customer Cluster', i, 'Coordinates'))\n",
    "    legend('topright', title = 'Cluster ID', legend = sort(unique(sc_c5)), col = sort(unique(sc_c5)), pch = 19)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "if(!file.exists('PickleFiles/SubsLap.rds')){\n",
    "    S <- make.similarity(CitiBike_miniS, s)\n",
    "    A <- make.affinity(S, 3)  # use 3 neighboors (includes self)\n",
    "    D <- diag(apply(A, 1, sum)) # sum rows\n",
    "    U <- D - A\n",
    "    saveRDS(U, 'PickleFiles/SubsLap.rds')\n",
    "}\n",
    "else{U <- readRDS('PickleFiles/SubsLap.rds')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall time: 48min 29s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "k   <- 2\n",
    "evL <- eigen(U, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]\n",
    "\n",
    "signif(evL$values,2) # eigenvalues are in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "mCitiS <- data.matrix(CitiBike_miniS)\n",
    "colnames(mCitiS) <- colnames(CitiBike_miniS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- 0\n",
    "wss[1] <- ((length(mCitiS)/length(colnames(mCitiS)))-1)*sum(apply(mCitiS,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCitiS, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Subscriber Clusters\", ylab=\"Within groups sum of squares\", main = \"Subscriber WSS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%R colnames(mCiti) <- colnames(CitiBike_clus[5:22])\n",
    "#%R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s <- specc(mCitiS, centers = 4)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#iteration <- 1\n",
    "#jitplot(sc_s, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s6 <- specc(mCitiS, centers = 5)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s6, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s6)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s1 <- specc(mCitiS, centers = 5, iterations = 500)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s1, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s1)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s2 <- specc(mCitiS, centers = 5, iterations = 1000)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s2, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s2)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s3 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'laplacedot')\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s3, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s3)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s4 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'polydot')\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s4, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s4)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R sc_s5 <- specc(mCitiS, centers = 5, iterations = 1000, kernel = 'rbfdot', nystrom.red = TRUE)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#jitplot(sc_s5, iteration)\n",
    "#iteration <- iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out = %R capture.output(sc_s5)\n",
    "#\n",
    "#for line in out:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R Sub_clus <- cbind(CitiBike_miniS, Cluster_ID_Customer = NaN, Cluster_ID_Subscriber = sc_s5)\n",
    "clear_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sub_clus = %R Sub_clus\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/Sub_clus.pkl\"):\n",
    "    print(\"File already created\")\n",
    "else: pickleObject(Sub_clus, \"Sub_clus\", filepath = \"PickleFiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "for (i in sort(unique(sc_s5))){\n",
    "    #plot(Sub_clus[Sub_clus$Cluster_ID_Customer == i,]$`start_station_longitude`, Sub_clus[Sub_clus$Cluster_ID_Customer == i,]$`star_station_latitude`, col=sc_s5, pch=19)\n",
    "    plot(mCitiS[Sub_clus$Cluster_ID_Subscriber == i,2], mCitiS[Sub_clus$Cluster_ID_Subscriber == i,1], col = i, pch = 19, xlim = c(-3,4), ylim = c(-3, 2), xlab = 'Customer Longitude Coordinates', ylab = 'Customer Latitude Coordinates', main = paste('Customer Cluster', i, 'Coordinates'))\n",
    "    legend('topright', title = 'Cluster ID', legend = sort(unique(sc_s5)), col = sort(unique(sc_s5)), pch = 19)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 2 - Evaluate and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_customer = pd.DataFrame ({\"Cluster_ID_Customer\": db_c7.labels_, \"Cluster_ID_Subscriber\": np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cust_clus_DB = pd.concat([CitiBike_C.reset_index(), cluster_id_customer], axis=1)\n",
    "#CitiBike_C = pd.concat([CitiBike_C, cluster_id_customer], axis=1)\n",
    "#Cust_clus_DB.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_counts = [db_c.labels_.max() + 1,\n",
    "               db_c1.labels_.max() + 1,\n",
    "               db_c2.labels_.max() + 1,\n",
    "               db_c3.labels_.max() + 1,\n",
    "               db_c4.labels_.max() + 1,\n",
    "               db_c5.labels_.max() + 1,\n",
    "               db_c6.labels_.max() + 1,\n",
    "               db_c7.labels_.max() + 1]\n",
    "\n",
    "pd.DataFrame({'Cluster_Counts' : clus_counts}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agg = pd.DataFrame({'count' : Cust_clus_DB.groupby([\"Cluster_ID_Customer\"]).size()}).reset_index()\n",
    "\n",
    "display(Agg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_subscriber = pd.DataFrame ({\"Cluster_ID_Customer\": np.NaN, \"Cluster_ID_Subscriber\": db_s.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sub_clus_DB = pd.concat([CitiBike_S.reset_index(), cluster_id_subscriber], axis=1)\n",
    "#CitiBike_C = pd.concat([CitiBike_C, cluster_id_customer], axis=1)\n",
    "#Sub_clus_DB.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_counts = [db_s.labels_.max() + 1,\n",
    "               db_s1.labels_.max() + 1,\n",
    "               db_s2.labels_.max() + 1,\n",
    "               db_s3.labels_.max() + 1,\n",
    "               db_s4.labels_.max() + 1,\n",
    "               db_s5.labels_.max() + 1,\n",
    "               db_s6.labels_.max() + 1,\n",
    "               db_s7.labels_.max() + 1,\n",
    "               db_s8.labels_.max() + 1,\n",
    "               db_s9.labels_.max() + 1]\n",
    "\n",
    "pd.DataFrame({'Cluster_Counts' : clus_counts}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Agg = pd.DataFrame({'count' : Sub_clus_DB.groupby([\"Cluster_ID_Subscriber\"]).size()}).reset_index()\n",
    "\n",
    "display(Agg.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering\n",
    "\n",
    "##### Customer Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evL$values)[1:10], xlab = \"Customer Clusters\", ylab = \"Laplacian Eigenvalues\", log = \"y\", main = \"Customer Eigenvalue Spectrum\")\n",
    "abline(v=3.5, col=\"red\", lty=2) # largest eigenvalue gap between 3 and 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- c(sum(withinss(sc_c)),\n",
    "         sum(withinss(sc_c1)),\n",
    "         sum(withinss(sc_c2)),\n",
    "         sum(withinss(sc_c3)),\n",
    "         sum(withinss(sc_c4)),\n",
    "         sum(withinss(sc_c5)))\n",
    "\n",
    "siz <- c(paste(size(sc_c), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c1), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c2), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c3), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c4), sep = '', collapse = ', '),\n",
    "         paste(size(sc_c5), sep = '', collapse = ', '))\n",
    "\n",
    "its <- c(200,500,1000,1000,1000,1000)\n",
    "cen <- c(3,3,3,3,3,3)\n",
    "ker <- c(\"rbfdot\", \"rbfdot\", \"rbfdot\", \"laplacedot\", \"polydot\", \"rbfdot\")\n",
    "nys <- c(FALSE, FALSE, FALSE, FALSE, FALSE, TRUE)\n",
    "\n",
    "Cust_SC <- data.frame(cen, its, ker, nys, wss, siz)\n",
    "Cust_SC <- setNames(Cust_SC, c(\"Cluster.Count\", \"Iterations\", \"Kernel\", \"Nystrom.Method\", \"wss\", \"Sizes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R Cust_SC[order(wss),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cust_SC = %R Cust_SC\n",
    "fig = plt.figure()\n",
    "plt.plot(Cust_SC.index.values, Cust_SC.wss, 'ro-', linewidth=2)\n",
    "plt.title('Total Within Sum of Squares')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Total Wss')\n",
    "plt.annotate('Selected Configuration', xy=(6, 25250), xytext=(4.2, 25550),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05),)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evL$values)[1:10], xlab = \"Subscriber Clusters\", ylab = \"Laplacian Eigenvalues\", log = \"y\", main = \"Subscriber Eigenvalue Spectrum\")\n",
    "abline(v=4.5, col=\"red\", lty=2) # largest eigenvalue gap between 4 and 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- c(sum(withinss(sc_s)),\n",
    "         sum(withinss(sc_s6)),\n",
    "         sum(withinss(sc_s1)),\n",
    "         sum(withinss(sc_s2)),\n",
    "         sum(withinss(sc_s3)),\n",
    "         sum(withinss(sc_s4)),\n",
    "         sum(withinss(sc_s5)))\n",
    "\n",
    "siz <- c(paste(size(sc_s), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s6), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s1), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s2), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s3), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s4), sep = '', collapse = ', '),\n",
    "         paste(size(sc_s5), sep = '', collapse = ', '))\n",
    "\n",
    "its <- c(200,200,500,1000,1000,1000,1000)\n",
    "cen <- c(4,5,5,5,5,5,5)\n",
    "ker <- c(\"rbfdot\", \"rbfdot\", \"rbfdot\", \"rbfdot\", \"laplacedot\", \"polydot\", \"rbfdot\")\n",
    "nys <- c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE)\n",
    "\n",
    "Sub_SC <- data.frame(cen, its, ker, nys, wss, siz)\n",
    "Sub_SC <- setNames(Sub_SC, c(\"Cluster.Count\", \"Iterations\", \"Kernel\", \"Nystrom.Method\", \"wss\", \"Sizes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R Sub_SC[order(wss),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Sub_SC = %R Sub_SC\n",
    "fig = plt.figure()\n",
    "plt.plot(Sub_SC.index.values, Sub_SC.wss, 'ro-', linewidth=2)\n",
    "plt.title('Total Within Sum of Squares')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Total Wss')\n",
    "#plt.annotate('Selected Configuration', xy=(7, 24296), xytext=(5.2, 24550),\n",
    "#            arrowprops=dict(facecolor='blue', shrink=0.05),)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 3 - Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.scatter(x, y, z, c=cs, cmap = 'prism', zdir = 'z')\n",
    "    ax.set_xlabel(xLab)\n",
    "    ax.set_ylabel(yLab)\n",
    "    ax.set_zlabel(zLab)\n",
    "    ax.view_init(azim=rot, elev = el) # Set rotation angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = Cust_clus.start_station_longitude\n",
    "y = Cust_clus.start_station_latitude\n",
    "z = Cust_clus.tripdurationLog\n",
    "cs = Cust_clus.Cluster_ID_Customer\n",
    "\n",
    "xLab = 'Longitude (scaled)'\n",
    "yLab = 'Latitude (scaled)'\n",
    "zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "rot = -90\n",
    "el = 80\n",
    "\n",
    "scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Cust_clus.start_station_longitude\n",
    "y = Cust_clus.start_station_latitude\n",
    "z = Cust_clus.tripdurationLog\n",
    "cs = Cust_clus.Cluster_ID_Customer\n",
    "\n",
    "xLab = 'Longitude (scaled)'\n",
    "yLab = 'Latitude (scaled)'\n",
    "zLab = 'Trip Duration Log  (scaled)'\n",
    "\n",
    "rot = -90\n",
    "el = 10\n",
    "\n",
    "scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Cust_clus.PRCP\n",
    "y = Cust_clus.TAVE\n",
    "z = Cust_clus.tripdurationLog\n",
    "cs = Cust_clus.Cluster_ID_Customer\n",
    "\n",
    "xLab = 'Precipitation (scaled)'\n",
    "yLab = 'Avg Temp (scaled)'\n",
    "zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "rot = -90\n",
    "el = 10\n",
    "\n",
    "scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Cust_clus.PRCP\n",
    "y = Cust_clus.TAVE\n",
    "z = Cust_clus.tripdurationLog\n",
    "cs = Cust_clus.Cluster_ID_Customer\n",
    "\n",
    "xLab = 'Precipitation (scaled)'\n",
    "yLab = 'Avg Temp (scaled)'\n",
    "zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "rot = -90\n",
    "el = 80\n",
    "\n",
    "scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Cust_clus.PRCP\n",
    "y = Cust_clus.TAVE\n",
    "z = Cust_clus.tripdurationLog\n",
    "cs = Cust_clus.Cluster_ID_Customer\n",
    "\n",
    "xLab = 'Precipitation (scaled)'\n",
    "yLab = 'Avg Temp (scaled)'\n",
    "zLab = 'Trip Duration Log (scaled))'\n",
    "\n",
    "rot = -60\n",
    "el = 45\n",
    "\n",
    "scatter3d(x,y,z,cs,xLab,yLab,zLab,rot,el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 4 - Summarize the Ramifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE END\n",
    "# ALEX'S CLASSIFICATION MODEL CODE START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Clusters\n",
    "\n",
    "With our clusters fit and appended to the original data, we still do not know what customer cluster our subscriber observations are fit to and vice versa. In order to fit clusters for the inverse user type, we need to build a classification model that can predict the appropriate cluster with strong confidence. For these models, when focusing on accuracy we’ll primarily use confusion matrices to explore our results alongside accuracy percentiles.\n",
    "\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 5 folds. This means, that from our original sample size of 1000, each \"fold\" will save off approximately 20% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst clusters. This process will occur through 5 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullunpicklepath = \"PickleFiles\\Sub_clus.pkl\"\n",
    "# Create an variable to pickle and open it in write mode\n",
    "unpicklefile = open(fullunpicklepath, 'rb')\n",
    "CitiBike_S_Clust = pickle.load(unpicklefile)\n",
    "unpicklefile.close()\n",
    "\n",
    "fullunpicklepath = \"PickleFiles\\Cust_clus.pkl\"\n",
    "# Create an variable to pickle and open it in write mode\n",
    "unpicklefile = open(fullunpicklepath, 'rb')\n",
    "CitiBike_C_Clust = pickle.load(unpicklefile)\n",
    "unpicklefile.close()\n",
    "\n",
    "    # Create CV Object for StratifiedKFold with 10 Folds, seeded at the length of our sample size\n",
    "seed = len(CitiBike_S_Clust)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. Values tested ranged from the default up until the maximum number of features passed in.\n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number of samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. Similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. **We tuned to provide output that completed all 10 iterations in under 10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def rfc_explor(ScaledData,\n",
    "               n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               y,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    accuracy = cross_val_score(rfc_clf, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,  \n",
    "                                         ScaledData,\n",
    "                                         y,\n",
    "                                         classes,\n",
    "                                         cv       = cv):\n",
    "    \n",
    "\n",
    "    X = ScaledData.as_matrix() \n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation\n",
    "\n",
    "    accuracy = []\n",
    "    \n",
    "    for (train, test) in cv.split(X, y):\n",
    "        clf.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "       \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "   \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(y[test], y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True)))   \n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =classes, \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Subscribers**\n",
    "\n",
    "After 14 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 82.7292 % with our worst parameter inputs of the random forest classification model to a value of 86.6774 % in the best tuned model fit. Also shown below is a stacked line chart of all accuracies across the 10 iterations, with different lines per parameter input. For all inputs provided, we saw less consistency than was desired - in our winning parameter inputs, we saw an accuracy metric ranging from 79.7980 % - 92.3077 %. Part of this problem could be due to the very small sample size fit through spectral clustering. With more computational resources and time, we could have created a larger fit dataset to these clusters for training/testing operations. Given this report's focus on Clustering, and not on classification we have chosen to only train one model type. In a real-world setting, we would test various other classification methodologies to identify a model with the best consistent fit.\n",
    "\n",
    "Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "**Subscriber Parameters**\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>None</th>\n",
    "      <td>Auto</td>\n",
    "      <td>1</td>\n",
    "      <td>2</td>\n",
    "      <td>10</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#os.remove(\"PickleFiles/rfcdf_Subsc.pkl\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/rfcdf_Subsc.pkl\"):\n",
    "    print(\"File already created\")\n",
    "    rfcdf_Subsc = unpickleObject(\"rfcdf_Subsc\")\n",
    "else: \n",
    "    acclist = [] \n",
    "\n",
    "    n_estimators       =  [10    , 10    , 10   , 10  , 10   , 10  , 10  , 10      , 10     , 10     , 10    , 5       , 15      , 25     ]  \n",
    "    max_features       =  ['auto', 'auto', 8    , 12  , 16   , 20  , 22  , 'auto'  , 'auto' , 'auto' , 'auto', 'auto'  , 'auto'  , 'auto' ] \n",
    "    max_depth          =  [None  , None  , None , None, None , None, None, 100     , 50     , 25     , 10    , None    , None    , None   ] \n",
    "    min_samples_split  =  [2     , 8     , 2    , 2   , 2    , 2   , 2   , 2       , 2      , 2      , 2     , 2       , 2       , 2      ] \n",
    "    min_samples_leaf   =  [1     , 4     , 1    , 1   , 1    , 1   , 1   , 1       , 1      , 1      , 1     , 1       , 1       , 1      ]\n",
    "\n",
    "    for i in range(0,len(n_estimators)):\n",
    "        acclist.append(rfc_explor(ScaledData        = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                  n_estimators      = n_estimators[i],\n",
    "                                  max_features      = max_features[i],\n",
    "                                  max_depth         = max_depth[i],\n",
    "                                  min_samples_split = min_samples_split[i],\n",
    "                                  min_samples_leaf  = min_samples_leaf[i],\n",
    "                                  y = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].values\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "    rfcdf_Subsc = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                    \"n_estimators\": n_estimators,          \n",
    "                                                    \"max_features\": max_features,         \n",
    "                                                    \"max_depth\": max_depth,        \n",
    "                                                    \"min_samples_split\": min_samples_split,\n",
    "                                                    \"min_samples_leaf\": min_samples_leaf   \n",
    "                                                  }),\n",
    "                                   pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "    rfcdf_Subsc.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "\n",
    "    pickleObject(rfcdf_Subsc, \"rfcdf_Subsc\", filepath = \"PickleFiles/\")\n",
    "\n",
    "display(rfcdf_Subsc)\n",
    "\n",
    "\n",
    "plot = rfcdf_Subsc[[\"Iteration 0\",\"Iteration 1\",\"Iteration 2\",\"Iteration 3\",\"Iteration 4\",\"Iteration 5\",\"Iteration 6\",\"Iteration 7\",\"Iteration 8\",\"Iteration 9\"]].transpose().plot.line(title = \"Accuracies for Varying Parameters Across 10 Iterations\",rot=45)\n",
    "plot.set_xlabel(\"Iterations\")\n",
    "plot.set_ylabel(\"Accuracies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Customers**\n",
    "\n",
    "After 14 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 65.6700 % with our worst parameter inputs of the random forest classification model to a value of 70.6806 % in the best tuned model fit. Also shown below is a stacked line chart of all accuracies across the 10 iterations, with different lines per parameter input. For all inputs provided, we saw less consistency than was desired - in our winning parameter inputs, we saw an accuracy metric ranging from 61.6162 % - 84.1584 %. Part of this problem could be due to the very small sample size fit through spectral clustering. With more computational resources and time, we could have created a larger fit dataset to these clusters for training/testing operations. Given this report's focus on Clustering, and not on classification we have chosen to only train one model type. In a real-world setting, we would test various other classification methodologies to identify a model with the best consistent fit.\n",
    "\n",
    "Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "**Customer Parameters**\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>None</th>\n",
    "      <td>auto</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "      <td>25</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#os.remove(\"PickleFiles/rfcdf_Cust.pkl\")\n",
    "\n",
    "if os.path.isfile(\"PickleFiles/rfcdf_Cust.pkl\"):\n",
    "    print(\"File already created\")\n",
    "    rfcdf_Cust = unpickleObject(\"rfcdf_Cust\")\n",
    "else: \n",
    "    acclist = [] \n",
    "\n",
    "    n_estimators       =  [10    , 10    , 10    , 10   , 10  , 10   , 10  , 10  , 10      , 10    , 10    , 10    , 5     , 15    , 25     ]  \n",
    "    max_features       =  ['auto', 'auto', 'auto', 8    , 12  , 16   , 20  , 22  , 'auto'  , 'auto', 'auto', 'auto', 'auto', 'auto', 'auto' ] \n",
    "    max_depth          =  [None  , None  , None  , None , None, None , None, None, 100     , 50    , 25    , 10    , None  , None  , None   ] \n",
    "    min_samples_split  =  [2     , 8     , 12    , 8    , 8   , 8    , 8   , 8   , 8       , 8     , 8     , 8     , 8     , 8     , 8      ] \n",
    "    min_samples_leaf   =  [1     , 4     , 6     , 4    , 4   , 4    , 4   , 4   , 4       , 4     , 4     , 4     , 4     , 4     , 4      ]\n",
    "\n",
    "    for i in range(0,len(n_estimators)):\n",
    "        acclist.append(rfc_explor(ScaledData        = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                  n_estimators      = n_estimators[i],\n",
    "                                  max_features      = max_features[i],\n",
    "                                  max_depth         = max_depth[i],\n",
    "                                  min_samples_split = min_samples_split[i],\n",
    "                                  min_samples_leaf  = min_samples_leaf[i],\n",
    "                                  y = CitiBike_C_Clust[\"Cluster_ID_Customer\"].values\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "    rfcdf_Cust = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                    \"n_estimators\": n_estimators,          \n",
    "                                                    \"max_features\": max_features,         \n",
    "                                                    \"max_depth\": max_depth,        \n",
    "                                                    \"min_samples_split\": min_samples_split,\n",
    "                                                    \"min_samples_leaf\": min_samples_leaf   \n",
    "                                                  }),\n",
    "                                   pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "    rfcdf_Cust.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "\n",
    "    pickleObject(rfcdf_Cust, \"rfcdf_Cust\", filepath = \"PickleFiles/\")\n",
    "    \n",
    "display(rfcdf_Cust)\n",
    "\n",
    "plot = rfcdf_Cust[[\"Iteration 0\",\"Iteration 1\",\"Iteration 2\",\"Iteration 3\",\"Iteration 4\",\"Iteration 5\",\"Iteration 6\",\"Iteration 7\",\"Iteration 8\",\"Iteration 9\"]].transpose().plot.line(title = \"Accuracies for Varying Parameters Across 10 Iterations\",rot=45)\n",
    "plot.set_xlabel(\"Iterations\")\n",
    "plot.set_ylabel(\"Accuracies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Analyze the Results\n",
    "We have created a function to be used for our cross-validation Accuracy Scores. Model CLF object, original sample y values, a distinct list of classes, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results. \n",
    "\n",
    "With our tuned parameters identified we may now assess futher insights. **Subscribers:** As was discussed earlier, we see a fairly large range in accuracies, however the average accuracy rating of 86.677 % is farly decent across all 10 iterations. In a confusion matrix of predicted results, we find that the most difficult cluster to predict and the cluster causing the most incorrect predictions for other classes is cluster 4. This may be due to the distribution of observations identified as cluster 4 in comparison to the rest of the clusters, you can see this clearly in the gradient plot below as we have a high density marker on cluster 4 true positives. **Customers:** The customer demographic had much less accurate predictions despite the decrease in number of clusters. We once again see a large range in accuracies, with an average accuracy of 70.681 %. As is seen in the confusion matrix, we pretty consistently guess each cluster incorrectly, and unlike the subscriber cluster set we do not see any potential causes for this off the bat. Although ~70% accuracy is not ideal, it is still better than chance(33%), so we are comfortable with moving on with this model for this report.  \n",
    "\n",
    "Once again, in a real-world setting, we would test various other classification methodologies and utilize more resources to increase sample size in order to identify stronger cluster fits and a model with the best consistent fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 10    ,\n",
    "                                 max_features       = 'auto',\n",
    "                                 max_depth          = None  , \n",
    "                                 min_samples_split  = 2     ,\n",
    "                                 min_samples_leaf   = 1     ,\n",
    "                                 n_jobs             = -1    , \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Subscriber, rfc_acc_Subscriber = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].values,\n",
    "                                                       classes     = CitiBike_S_Clust[\"Cluster_ID_Subscriber\"].drop_duplicates().sort_values().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 25    ,\n",
    "                                 max_features       = 'auto',\n",
    "                                 max_depth          = None  , \n",
    "                                 min_samples_split  = 8     ,\n",
    "                                 min_samples_leaf   = 4     ,\n",
    "                                 n_jobs             = -1    , \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Customer, rfc_acc_Customer = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_C_Clust[\"Cluster_ID_Customer\"].values,\n",
    "                                                       classes     = CitiBike_C_Clust[\"Cluster_ID_Customer\"].drop_duplicates().sort_values().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Predict Cluster Values for Inverse UserType Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_WithClusters = pd.concat([CitiBike_C, CitiBike_S])\n",
    "X = CitiBike_WithClusters.drop([\"usertype\"], axis=1)\n",
    "\n",
    "CitiBike_WithClusters[\"Cluster_ID_Customer\"]   = rfc_clf_Customer.predict(X)\n",
    "CitiBike_WithClusters[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)\n",
    "\n",
    "display(CitiBike_WithClusters.head())\n",
    "display(CitiBike_WithClusters.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA Loadings, do our newly added Cluster features add value?\n",
    "Our first objective is to identify the number of components to be used for user type classification. In order to do so, we first exclude redundant and non-value variables up front. Non-value variables include gender, birth year, and age since these data were missing for most Customer user types and were replaced with filler values as discussed in previous sections. We will exclude these since they misrepresent correlation with user type.\n",
    "\n",
    "*(Note: PCA code steps adapted and modified from https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    #Split Cluster_ID_Customer Values\n",
    "AttSplit = pd.get_dummies(CitiBike_WithClusters.Cluster_ID_Customer,prefix='Cluster_ID_Customer')\n",
    "CitiBike_WithClusters = pd.concat((CitiBike_WithClusters,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "    #Split Cluster_ID_Subscriber Values \n",
    "AttSplit = pd.get_dummies(CitiBike_WithClusters.Cluster_ID_Subscriber,prefix='Cluster_ID_Subscriber')\n",
    "CitiBike_WithClusters = pd.concat((CitiBike_WithClusters,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "display(CitiBike_WithClusters.head())\n",
    "\n",
    "myData_scaled_classification = CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).as_matrix()\n",
    "print(myData_scaled_classification.shape)\n",
    "\n",
    "maxcomp = 28\n",
    "\n",
    "pca_class = PCA(n_components=maxcomp, svd_solver='randomized')\n",
    "\n",
    "pca_class.fit(myData_scaled_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, We verify that 28 attributes exist in our dataset, after removed features for clustering and the addition of our cluster values. The maximum number of components to be produced will match this number. For this reason, we will identify 28 to be the number of components produced by our PCA and will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. Using full PCA required unacceptable lengths of time to compute.\n",
    "\n",
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first **14 principal components – the change is rather steep through the 14th component. After the 1% drop between components 14 and 15, the rate of decreasing explained variance begins to somewhat flatten out, reducing to a 0.2% change or less.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,maxcomp+1), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, maxcomp+1, -0.1, 20])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance arguably begins to plateau around the **14th principal component and that the first 14 components together explain 77.92% of variance in the data set. For this reason, 14 principal components may be selected as being the most appropriate for user type classification modeling given the variables among these data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "plt.plot(range(1,maxcomp+1), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, maxcomp+1, 10, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Loadings\n",
    "\n",
    "Now at the individual principal component level, each classification principal component has **89** loadings which may be esteemed as weights or coefficients representing each original attribute. These loadings represent to what extent each attribute fabricates a given principal component, and the relationship between these attributes in context of the principal component under review. Another perspective is that each principal component is describing an underlying factor which is comprised of the heaviest loadings.\n",
    "\n",
    "Rather than discuss all **14 principal components, we will instead focus on the three principal components with the largest positive coefficients (PC3, PC10, and PC4) as described in the PCA importance plot above as well as the component with the largest coefficient value in the negative direction (PC8).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 1 - xxx*\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[0], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[1], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[2], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'DeepSkyBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[3], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'SlateBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[4], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[5], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[6], index=CitiBike_WithClusters.drop([\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[7], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[8], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[9], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[10], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[11], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.Series(pca_class.components_[12], index=CitiBike_WithClusters.drop([\"index\",\"usertype\", \"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis = 1).columns)\n",
    "maxcomponentsix = pd.Series(pd.DataFrame(abs(components).sort_values(ascending=False).head(15)).index)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "weightsplot = pd.Series(components, index=maxcomponentsix)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALEX'S CLASSIFICATION MODEL CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "### How useful is the model for interested parties?\n",
    "\n",
    "##### Classification of Usertype (Customer vs Subscriber)\n",
    "**How is this model useful?** Citibike aims to increase rider subscriptions, but may fear adverts to the wrong people could keep customers from enjoying their services. A classification model predicting usertype as customer or subscriber allows Citibike to identify those customers who meet the criteria for common subscribing members. False Positive predictions are extremely valueable to target this demographic.\n",
    "\n",
    "**How does the cluster features produced impact the model?**\n",
    "As previously discussed, we added two features to our dataset through XXX clustering. These features identified clusters based on both the subscriber and customer demographics. Once these features were added to the dataset, running Principal Component Analysis identified several principal components which were heavily impacted by these new features. Specifically Components XX, XX, and XX depicted highly weighted features for XX, XX, and XX. Our hopes, although not performed during this report, is that the addition of these cluster features will positively impact the accuracy of a classification model on predicting usertypes.\n",
    "\n",
    "**How would this model be deployed?** This model has the ability to be deployed as real-time predictions, or as a periodic corporate marketing alert if customer email addresses are readily available. Our preference would be a real-time prediction as a customer returns a bike. Upon return, if the model suggests a customer contains subcribing tendencies email alerts, pop-up promotions,etc. may be deployed in order to gain the customer's attention towards subscriber offerings. Alternatively, the marketing team could receive this information periodically, and implement custom strategies based on industry best practice techniques to this target group. This group of individuals are likely more apt to acknowledge these tactics positively, since their usage tendencies already align more closely to that of a subcribing member.\n",
    "\n",
    "**How often would the model need to be updated?** This model would definitely need to be updated periodically. As the citibike Share service grows, and new locations arrive, the model will need to be updated to account for the new sites. Also, as the population in NYC shifts over time(new businesses, schools, residential, etc.), trends may also fluctuate. These fluctuations will need to be accounted for in the model regularly to keep it up to date with current state NYC. Our recommendation for these updates would be periodic (monthly or quarterly) model fit updates in CitiBike systems to account for these possible changes.\n",
    "\n",
    "\n",
    "\n",
    "##### Additional Data to Collect:\n",
    "* **Event/Restaurant/Retail Data:** Given that we have detailed geocoordinate data and have already demonstrated powerful use of the Google Maps API, it would be possible to incorporate location details surrounding Citi Bike start and stop locations. There is potential for such data to be gathered automatically using API's such as Google's. Having this data would provide further insight into some of the reasons some bike share locations are more popular than others. Such data could even help Citi Bike predict changes in station demand based on changing surroundings and help determine where new stations should be installed.\n",
    "* **Special Events:** Similar to the previous idea, merging other public data based on geophysical coordinates and timeline could introduce other external factors such as the effects of parades, public concerts, festivals, and other events on ride activity for a given day or week. This would help identify/predict abnormal activity in this and future data sets. Additionally, it would provide insight to Citi Bike as to how to better plan and prepare for such events to boost rental count and increase trip duration.\n",
    "* **GPS Enabled Bike Computers:** Though not influenced by the data we have at hand, adding bicycle tracking hardware to each Citi Bike rental would provide substantial value to future data sets. Adding GPS tracking would enable Citi Bike to track specific routes followed by clients and could even aid NYC planners with transportation projects. Having route information means that true distance covered would be available, an attribute that would have far more meaning than our LinearDistance attribute. Incorporating GPS tracking with bike speed would provide insights into individual rider activity. For example, just because a rider's trip duration was 6 hours doesn't mean they actively rode for that amount of time. It is far more likely such a rider would have stopped for an extended period of time at least once during this period of time. Adding GPS and speed data would alleviate these existing gaps.\n",
    "\n",
    "### Deploying the Chosen Model on new data\n",
    "We have discussed above, what value our model holds, our preferred method of deployment, and frequency of model updates. Below we will walk through the process for prepping a real-time prediction model for deployment, and actually executing model predictions on new data inputs. \n",
    "\n",
    "##### Prepping the Model for Deployment\n",
    "A key component in deploying our model is the re-use of data transformation and / or Model fit objects created during this process. We need to be able to apply new data into the same constructs listed below, which were utilized in the Testing and Training process:\n",
    "* Standard Scaler\n",
    "* Model CLF Fit (RF decision trees)\n",
    "* Cluster Column List for Random Forest Prediction Input\n",
    "\n",
    "To do this, we must take our python objects currently stored in our active Kernel and permanently store them in a \"Pickled\" (.pkl) file. This .pkl file is a serializes version of our python object which can be accessed later on. To prove our pickled files are being utilized instead of previously created objects, we have executed the %reset jupyter command to clear all environment objects before proceding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "def pickleObject(objectname, filename, filepath = \"PickleFiles/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SubClusterCols = CitiBike_S_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() \n",
    "CusClusterCols = CitiBike_C_Clust.drop([\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objectlist   = [[1, 2, 3, 4, 5], rfc_clf_Subscriber, rfc_clf_Customer, SubClusterCols, CusClusterCols   ]\n",
    "filenamelist = [\"list\",          \"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(objectlist)):\n",
    "    pickleObject(objectlist[i], filenamelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from datetime import timedelta\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pyowm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions on New Data\n",
    "Below is a single data entry for a Customer upon the return of a bike rental. We have produces fake values, based on possible value ranges present in our original dataset from Citibike. For the purpose of this \"real-time\" prediction, the current date/time of execution is utilized for a user submitting a bike return. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "NewData = pd.DataFrame({\n",
    "                        \"tripduration\": [579],\n",
    "                        \"starttime\": [datetime.now() + timedelta(seconds = -579)],\n",
    "                        \"stoptime\": [datetime.now()],\n",
    "                        \"start_station_id\": [477],\n",
    "                        \"start_station_name\": [\"W 41 St & 8 Ave\"],\n",
    "                        \"start_station_latitude\": [40.756405],\n",
    "                        \"start_station_longitude\": [-73.990026],\n",
    "                        \"end_station_id\": [441],\n",
    "                        \"end_station_name\": [\"E 52 St & 2 Ave\"],\n",
    "                        \"end_station_latitude\": [40.756014],\n",
    "                        \"end_station_longitude\": [-73.967416],\n",
    "                        \"bikeid\": [16537],\n",
    "                        \"usertype\": [\"Customer\"],\n",
    "                        \"birthyear\": [\"\\\\N\"],\n",
    "                        \"gender\": [0]\n",
    "                      })\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformations and Additional Features**\n",
    "\n",
    "As was done previously during Data Understanding and Preparation, there are several attributes that we computed and / or obtained from third parties to complete our dataset. The following attributes are needed to be created:\n",
    "* LinearDistance\n",
    "* DayOfWeek\n",
    "* TimeOfDay\n",
    "* HolidayFlag\n",
    "* TMIN\n",
    "* TAVE\n",
    "* TMAX\n",
    "* PRCP\n",
    "* SNOW\n",
    "\n",
    "Since the current time was utilized for our bike return, we are able to utilize the openweathermap API for current weather forecasts for the current day. For this report, only the free features of this API are available. In a true production implementation, we recommend a subscription unlocking additional features for more accurate results and to increase the \"Calls per Minute\" limitations imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"LinearDistance\"] = NewData.apply(lambda x: vincenty((x[\"start_station_latitude\"], x[\"start_station_longitude\"]), \n",
    "                                                             (x[\"end_station_latitude\"],   x[\"end_station_longitude\"])).miles,\n",
    "                                          axis = 1)\n",
    "\n",
    "    \n",
    "    ## Compute DayOfWeek from Start Time\n",
    "        # starttime needs to be converted to a pandas datetime type before we can find the weekday name\n",
    "NewData['starttime'] = pd.to_datetime(NewData['starttime'])\n",
    "NewData[\"DayOfWeek\"] = NewData['starttime'].dt.weekday_name\n",
    "\n",
    "    ## Compute TimeOfDay from Start Time\n",
    "        ##Morning       5AM-10AM\n",
    "        ##Midday        10AM-2PM\n",
    "        ##Afternoon     2PM-5PM\n",
    "        ##Evening       5PM-10PM\n",
    "        ##Night         10PM-5AM\n",
    "\n",
    "\n",
    "NewData[\"TimeOfDay\"] = np.where((NewData['starttime'].dt.hour >= 5) & (NewData['starttime'].dt.hour < 10), 'Morning',\n",
    "                                np.where((NewData['starttime'].dt.hour >= 10) & (NewData['starttime'].dt.hour < 14), 'Midday',\n",
    "                                         np.where((NewData['starttime'].dt.hour >= 14) & (NewData['starttime'].dt.hour < 17), 'Afternoon',\n",
    "                                                  np.where((NewData['starttime'].dt.hour >= 17) & (NewData['starttime'].dt.hour < 22), 'Evening',\n",
    "                                                           'Night' ### ELSE case represents Night\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )\n",
    "                                                  \n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"HolidayFlag\"] = NewData['starttime'].isin(holidays.UnitedStates())\n",
    "NewData[\"HolidayFlag\"] = np.where(NewData[\"HolidayFlag\"] == False, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "owm = pyowm.OWM('462d2effa0ba127689b824b37efc9d12')  # You MUST provide a valid API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "forecaster = owm.three_hours_forecast_at_coords(lat = float(NewData[\"start_station_latitude\"]), lon = float(NewData[\"start_station_longitude\"]))\n",
    "forecast = forecaster.get_forecast()\n",
    "fweather_list = forecast.get_weathers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tlist = []\n",
    "plist = []\n",
    "slist = []\n",
    "\n",
    "\n",
    "for x in fweather_list:\n",
    "    date = datetime.date(datetime.strptime(x.get_reference_time('iso'),\"%Y-%m-%d %H:%M:%S+00\") + timedelta(hours = -5))\n",
    "    \n",
    "    if (date >= datetime.date(NewData[\"starttime\"].min())) \\\n",
    "    and (date < datetime.date(NewData[\"starttime\"].min())+ timedelta(days = 1)): \n",
    "        temp = x.get_temperature('fahrenheit')['temp']\n",
    "        prcp = x.get_rain()\n",
    "        snow = x.get_snow()\n",
    "        tlist.append(temp)\n",
    "        \n",
    "        if prcp == {}:\n",
    "            plist.append(0)\n",
    "        else:\n",
    "            plist.append(prcp)\n",
    "        \n",
    "        if snow == {}:\n",
    "            slist.append(0)\n",
    "        else:\n",
    "            slist.append(snow)\n",
    "        \n",
    "tempdata = pd.DataFrame(tlist)\n",
    "prcpdata = pd.DataFrame(plist)\n",
    "snowdata = pd.DataFrame(slist)\n",
    "\n",
    "NewData[\"TMIN\"] = tempdata.min()\n",
    "NewData[\"TAVE\"] = tempdata.mean()\n",
    "NewData[\"TMAX\"] = tempdata.max()\n",
    "NewData[\"PRCP\"] = prcpdata.sum()\n",
    "NewData[\"SNOW\"] = snowdata.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With additional features added, we have several missing values to scrub, the tripdurationlog attribute to compute, and data type conversions to apply to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "NewData[\"birthyear\"] = NewData[\"birthyear\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "NewData[['starttime', 'stoptime']] \\\n",
    "    = NewData[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "NewData[\"Age\"] = np.where(NewData[\"birthyear\"] == 0, 0,\n",
    "                                       NewData[\"starttime\"].dt.year - NewData[\"birthyear\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)\n",
    "\n",
    "# Log Transform Column Added\n",
    "NewData[\"tripdurationLog\"] = NewData[\"tripduration\"].apply(np.log)\n",
    "    \n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding categorical attributes**\n",
    "\n",
    "As was performed during our data preparation section of this report, we have several categorical data values which require encoding. Since we do not have the ability to execute dummy logic as before, we manually searched for the permutation of values assigning 1 if matched and 0 if not. The below categorical attributes have been encoded below:\n",
    "* DayOfWeek\n",
    "* TimeOfDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## DayOfWeek\n",
    "NewData[\"DayOfWeek_Monday\"] = np.where(NewData[\"DayOfWeek\"] == \"Monday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Tuesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Tuesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Wednesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Wednesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Thursday\"] = np.where(NewData[\"DayOfWeek\"] == \"Thursday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Friday\"] = np.where(NewData[\"DayOfWeek\"] == \"Friday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Saturday\"] = np.where(NewData[\"DayOfWeek\"] == \"Saturday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Sunday\"] = np.where(NewData[\"DayOfWeek\"] == \"Sunday\", 1, 0)\n",
    "\n",
    "## TimeOfDay\n",
    "NewData[\"TimeOfDay_Morning\"] = np.where(NewData[\"TimeOfDay\"] == \"Morning\", 1, 0)\n",
    "NewData[\"TimeOfDay_Midday\"] = np.where(NewData[\"TimeOfDay\"] == \"Midday\", 1, 0)\n",
    "NewData[\"TimeOfDay_Afternoon\"] = np.where(NewData[\"TimeOfDay\"] == \"Afternoon\", 1, 0)\n",
    "NewData[\"TimeOfDay_Evening\"] = np.where(NewData[\"TimeOfDay\"] == \"Evening\", 1, 0)\n",
    "NewData[\"TimeOfDay_Night\"] = np.where(NewData[\"TimeOfDay\"] == \"Night\", 1, 0)\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to deploying predictions on new data is to \"unpickle\" the .pkl file objects needed for our model. We need to unpickle the below constructs in order to re-produce our model, and apply it on new data:\n",
    "* Standard Scaler \n",
    "* Model CLF Fit (RF decision trees)\n",
    "* Cluster Column List for Random Forest Prediction Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickleObject(filename, filepath = \"PickleFiles/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "    \n",
    "    return unpickleObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unpickleObjectList = []\n",
    "filenamelist = [\"list\",\"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(filenamelist)):\n",
    "    unpickleObjectList.append(unpickleObject(filenamelist[i]))\n",
    "\n",
    "list = unpickleObjectList[0]\n",
    "rfc_clf_Subscriber = unpickleObjectList[1]\n",
    "rfc_clf_Customer = unpickleObjectList[2]\n",
    "SubClusterCols = unpickleObjectList[3]\n",
    "CusClusterCols = unpickleObjectList[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = NewData[SubClusterCols]\n",
    "\n",
    "NewData[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = NewData[CusClusterCols]\n",
    "\n",
    "NewData[\"Cluster_ID_Customer\"] = rfc_clf_Customer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(NewData.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our clusters added to the dataset, the bike return data for this user is now finally ready to be utilzied for a classication / regression model of choice, including both Subscriber and Customer clusters. As discussed previously, given the purpose of this report and the focus on clustering - this model, and any data preparation steps for this model have not been prepared. \n",
    "\n",
    "Note that the run-time for transforming/adding new features to the original input completes within XX seconds. This, of course, does not include the classification / regression model, but is a realistic timeframe up to this point for real-time application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "This lab was the most challenging and time consuming yet, partly because we chose to implement PCA in order to reduce the dimensionality of our data set and partly because we chose to explore new model types such as Neural Networks MLP Regression for trip duration prediction. The additional research and work put toward these records seemed to balloon at times, especially when considering MLP interpretations and ranking attributes through the haze of PCA's linear combinations. We hope, however, that our efforts did not go unnoticed given the added time we spent ensuring we utilized and interpreted these tools correctly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0dadc60f47f04508a5c1e59ccb5e758b": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "95c6e7ee3d874cab9c7e154996a80a5b": {
     "views": [
      {
       "cell_index": 49
      }
     ]
    },
    "fffa21f8017c460d9a27461647c0b505": {
     "views": [
      {
       "cell_index": 36
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
