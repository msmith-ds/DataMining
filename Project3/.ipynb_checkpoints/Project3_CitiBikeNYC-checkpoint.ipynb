{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Classification and Regression -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 4/05/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Business Understanding\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for Lab 2 consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behavior.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this lab is to: 1) Predict whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, etc., and 2) predict a rider's trip duration as a function of the aforementioned variables.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve these goals (as described further in the sections below). By leveraging Stratified 10-Fold Cross Validation, we expect to be able to derive dependable and accurate user type and ride duration prediction models for which accuracy and performance will be discussed in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding 1\n",
    "***Describe the meaning and type of data***\n",
    "\n",
    "Before diving into each attribute in detail, one glaring facet of this data set that needs mentioning is its inherent time-series nature. By no means was this overlooked when we decided upon these particular data. To mitigate the effects of time on our analysis results, we have chosen to aggregate time-centric attributes such as dates and hours of the day by replacing them with simply the day of the week or period of the day (more on these details shortly). For example, by identifying trips occurring on July 1st, 2013, not by the date of occurrence but rather the day of the week, Monday, and identifying trips on July 2nd, 2013, as occurring on Tuesday, we will be able to obtain a \"big picture\" understanding of trends by day of the week instead of at the date-by-date level. We understand this is not a perfect solution since the time-series component is still an underlying factor in trip activity, but it is good enough to answer the types of questions we hope to target as described in the previous section as we will be comparing all Mondays against all Tuesdays, etc.\n",
    "\n",
    "As mentioned previously, the original data set ***from Citi Bike*** included 15 attributes. These 15 attributes and associated descriptions are provided below:\n",
    "1. **tripduration** - *Integer* - The total time (in seconds) a bike remains checked out, beginning with the start time and ending with the stop time\n",
    "2. **starttime** - *DateTime* - The date and time at which a bike was checked out, marking the start of a trip (i.e. 2/12/2014 8:16)\n",
    "3. **stoptime** - *DateTime* - The date and time at which a bike was checked back in, marking the end of a trip (i.e. 2/12/2014 8:16)\n",
    "4. **start_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station from which a bike is checked out\n",
    "5. **start_station_name** - *String* - The name of the station from which a bike is checked out; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "6. **start_station_latitude** - *Float* - The latitude coordinate for the station from which a bike is checked out (i.e. 40.74780373)\n",
    "7. **start_station_longitude** - *Float* - The longitude coordinate for the station from which a bike is checked out (i.e. -73.9900262)\n",
    "8. **end_station_id** - *String* - A categorical number value used to identify Citi Bike stations, in this case the station in which a bike is checked in\n",
    "9. **end_station_name** - *String* - The name of the station at which a bike is checked in; most often the name of an intersection (i.e. E 39 St & 2 Ave)\n",
    "10. **end_station_latitude** - *Float* - The latitude coordinate for the station at which a bike is checked in (i.e. 40.74780373)\n",
    "11. **end_station_longitude** - *Float* - The longitude coordinate for the station at which a bike is checked in (i.e. -73.9900262)\n",
    "12. **bikeid** - *String* - A categorical number value used to identify a particular bike; each bike in the bike share network has its own unique number\n",
    "13. **usertype** - *String* - A classifier attribute identifying a rider as a bike share subscriber or a one-time customer (i.e. Subscriber vs. Customer)\n",
    "14. **birth_year** - *Integer* - The year a rider was born (Only available for subscribed riders, however)\n",
    "15. **gender** - *String* - A categorical number value representing a rider's gender (i.e. 0 = unknown, 1 = male, 2 = female)\n",
    "\n",
    "\n",
    "It is important to note that birth year and gender details are not available for \"Customer\" user types but rather for \"Subscriber\" riders only. Fortunately, these are the only missing data values among all trips in the data set. Unfortunately, however, it means that we will not be able to identify the ratio of males-to-females that are not subscribed or use age to predict subcribers vs. non-subscribers (Customers). More to this end will be discussed in the next section.\n",
    "\n",
    "It is also worth mentioning that while attributes such as trip duration, start and end stations, bike ID, and basic rider details were collected and shared with the general public, care was taken by Citi Bike to remove trips taken by staff during system service appointments and inspections, trips to or from \"test\" stations which were employed during the data set's timeframe, and trips lasting less than 60 seconds which could indicate false checkout or re-docking efforts during checkin.\n",
    "\n",
    "Because some attributes may be deemed as duplicates (i.e. start_station_id, start_station_name, and start_station_latitude/longitude for identifying station locations), we chose to extract further attributes from the base attributes at hand. Further attributes were also extracted to mitigate the effects of time. In addition, we felt increased understanding could be obtained from combining weather data for the various trips as discussed in the previous section. These additional 10 attributes are described below:\n",
    "\n",
    "16. **LinearDistance** - *Integer* - The distance (miles) from a start station to an end station (as a crow flies); calculated from the latitude/longitude coordinates of start/end stations\n",
    "17. **DayOfWeek** - *String* - The day of the week a trip occurs regardless of time of day, month, etc.; extracted from the *starttime* attribute (i.e. Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)\n",
    "18. **TimeOfDay** - *String* - The portion of the day during which a bike was checked out; extracted from the *starttime* attribute (i.e. Morning, Midday, Afternoon, Evening, Night)\n",
    "19. **HolidayFlag** - *String* - A categorical binary value used to identify whether the day a trip occurred was on a holiday or not; extracted from the *starttime* attribute (i.e. 0 = Non-Holiday, 1 = Holiday)\n",
    "20. **Age** - *Integer* - The age of a rider at the time of a trip; calculated based on the *birth_year* attribute (Since only birth year is included in original Citi Bike data set, exact age at time of trip when considering birth month is not possible)\n",
    "21. **PRCP** - *Float* - The total recorded rainfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "22. **SNOW** - *Float* - The total recorded snowfall in inches on the day of a trip; merged from the CDIAC weather data set\n",
    "23. **TAVE** - *Integer* - The average temperature throughout the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "24. **TMAX** - *Integer* - The maximum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "25. **TMIN** - *Integer* - The minimum temperature on the day on which a trip occurs; merged from the CDIAC weather data set\n",
    "\n",
    "After extracting our own attributes and merging weather data, the total number of attributes present in our final data set is 25. Only 15 are used throughout this lab, however, due to the duplicate nature of some attributes as discussed already. This final list of ***used*** attributes are tripduration, DayOfWeek, TimeOfDay, HolidayFlag, start_station_name, start_station_latitude, start_station_longitude, usertype, gender, Age, PRCP, SNOW, TAVE, TMAX, and TMIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250,000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gmaps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "import pickle\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Preparation 1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1 - Define and prepare class variables\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4,881,384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680,909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to each other. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 hour period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however, we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding 2 - Visualize important attributes\n",
    "\n",
    "To re-iterate, our main objectives in analyzing these data are to determine which attributes have greatest bearing on predicting a rider's type (Customer vs. Subscriber) and to gain a better understanding of rider behavior as a function of external factors. Many attributes in this data set will eventually be used in subsequent labs to answer these questions. The primary attributes on which we will focus our attention in this section, however, are as follows:\n",
    "- Starting Location\n",
    "- Day of the Week\n",
    "- Time of Day\n",
    "- Trip Duration (both log and non-log)\n",
    "- Linear Distance\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "Over the course of this section, we will review these top attributes in some detail and discuss the value of using our chosen visualizations. Note also that merged weather data is of significant interest as well. As we desire to heavily compare weather conditions against various rider habits, however, we will refrain from focusing on weather-related attributes until the subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geophysical Start Stations HeatMap\n",
    "Before discussing the following heatmap in detail, it is worth noting some special steps required to use the gmaps module in Python in case the reader is interested in rendering our code to plot data on top of Google's maps (Note full instructions are available at https://media.readthedocs.org/pdf/jupyter-gmaps/latest/jupyter-gmaps.pdf)\n",
    "\n",
    "Besides having Jupyter Notebook installed on one's computer with extensions enabled (default if using Anaconda) and installing the gmaps module using pip, the following line should be run from within the command terminal. This is only to be done once and should be done when Jupyter Notebook is not running.\n",
    "```\n",
    "$ jupyter nbextension enable --py gmaps\n",
    "```\n",
    "In addition to running the above line in the command prompt, a Standard Google API user key will need obtained from https://developers.google.com/maps/documentation/javascript/get-api-key. This only needs done once and is necessary to pull the Google map data into the Jupyter Notebook environment. The key is entered in the *gmaps.configure()* line as shown in the below cell. We have provided our own private key in the meantime for the reader's convenience.\n",
    "\n",
    "Now on to the data visualization... This geological heatmap visualization is interactive; however, the kernel must run the code block each time our Jupyter Notebook file is opened due to the API key requirement. Therefore, we've captured some interesting views to aid in our discussion and have included them as embedded images.\n",
    "\n",
    "The start station heatmap represents the start station location data via attributes *start_station_latitude* and *start_station_longitude*. It identifies areas of highest and lowest concentration for trip starts. The location data is important as it helps us understand where the areas of highest activity are and, as will be seen in one of our later sections, will play an important role in identifying riders as regular customers or subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gmaps.configure(api_key=\"AIzaSyAsBi0MhgoQWfoGMSl5UcD-vR6H76cntxg\") # Load private Google API key\n",
    "\n",
    "locations = CitiBikeDataSampled_5050[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "m = gmaps.Map()\n",
    "heatmap_layer = gmaps.Heatmap(data = locations)\n",
    "m.add_layer(heatmap_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall view quickly reveals that station data was only provided for areas of NYC south of Manhattan and mostly north of Brooklyn. This could either mean that the bike share program had not yet expanded into these other areas at the time of data collection or that the data simply wasn't included (as mentioned previously, many test sites were being used during this time frame but CitiBike did not include them with this data set).\n",
    "\n",
    "Within the range of trip start frequency from the least number of trips (green) to the most trips (red), green and yellow indicate low to medium trip activity in most areas. However, higher pockets of concentration do exist in some places. We will attempt to put this visualization to good use by focusing in on one of these hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocations.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prominant heatspot occurs just east of Bryant Park and the Midtown map label. Zooming into this area (via regular Google Map controls as the rendered visual is interactive) allows for a closer look. A snapshot of this zoomed in image is embedded below. The hotspot seems slightly elongated and stands out from among the other stations. Zooming in further will help to understand why this is and may shed some light on the higher activity in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom1.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in to this area further helps us see that two stations are very close together. Even so, why might there be such high rider activity at these stations? This higher activity is likely affected by the stations' proximity to the famous Grand Central Station. As commuters and recreationalists alike arrive by train at Grand Central, it is natural that many of them may choose to continue their journey via the two closest bike share stations nearby. When the northernmost bike share station runs out of bikes, riders likely go to the next station to begin their ride instead.\n",
    "\n",
    "By understanding the dynamics of geographical activity within this data set and the amenities that surround each station, we will be able to more efficiently leverage the data to make our classification and regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/All_StartLocationsZoom2.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geographic Heatmap Comparing Customer vs. Subscriber Start Station Activity\n",
    "\n",
    "After visualizing the overall dataset locations with a heatmap over NYC, we decided to take the visualization one step further. This time, we broke the dataset into two segments: Customer vs. Subscriber. Below is two separate gmap heatmaps containing geographic densities for each usertype. What we found assisted our theories on customer vs. subscriber usage tendencies. Seen first, the Customer heatmap overall contains much fewer dense regions. This helps to confirm our suspicions infering Customer bikers as less \"routine\" than subscribing bikers. When looking around for the most dense region in this heatmap, one point stuck out as particularly interesting: The Zoo. When comparing this region on the Subscriber gmap, we did not see the same type of traffic! This helps assist our theories that customer bikers use the service more for events, shopping,  or one-time use convenience. On the subscriber gmap, the most dense region, is that near the Grand Central Station as discussed earlier - assisting in the opposing theory for subscribing members as routine trips to work, groceries, etc. as they consistently use the bike share service as a means to reach the metro station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "customerData = CitiBikeDataSampled_5050.query('usertype == \"Customer\"')\n",
    "customerLoc = customerData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "cmap = gmaps.Map()\n",
    "customer_layer = gmaps.Heatmap(data=customerLoc)#, fill_color=\"red\", stroke_color=\"red\", scale=3)\n",
    "cmap.add_layer(customer_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/CMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "subscriberData = CitiBikeDataSampled_5050.query('usertype == \"Subscriber\"')\n",
    "subscriberLoc = subscriberData[['start_station_latitude', 'start_station_longitude']].values.tolist()\n",
    "\n",
    "smap = gmaps.Map()\n",
    "subscriber_layer = gmaps.Heatmap(data=subscriberLoc)#, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "smap.add_layer(subscriber_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project3/Images/SMAP_StartLocations_Satellite.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trip Duration and Linear Distance vs Weather by Customer/Subscriber\n",
    "\n",
    "Because we were able to bring together historical weather data for the dates we had in our records, we wanted to explore the relationship these variables had with our usertype status. If subscribers were regularly using the bikes for commuting as we've begun to see, then weather wouldn't impact their rental stastics as much as customers who appear to be primarily opportunistic in their usage.\n",
    "\n",
    "A quick cursory glance reveals a noticeable difference in bike rentals in regards to low temperatures, precipitation, and snowfall. While true, there are fewer customers than subscribers, we're concerned primarily with the spread or distribution of the plot points rather than the quantity. And we can see that on the customer pair plots that there are fewer points distributed across the lower temperature ranges and higher precipitation/snowfall ranges. The distributions pick back up at higher temperatures and lower precipitation points between the two usertypes.\n",
    "\n",
    "If stations consistently see use during \"bad\" weather, then those stations could be identified as subscriber stations. Further, if certain customers are found making the same trips consistently in all weather types, then they could be pushed for subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Customer'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subscriber Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.pairplot(CitiBikeDataSampled_5050.query(\"usertype == 'Subscriber'\"), x_vars=[\"PRCP\",\"SNOW\",\"TAVE\",\"TMAX\",\"TMIN\"], y_vars=[\"tripduration\",\"tripdurationLog\",\"LinearDistance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Trip Duration by Day of the Week Split Violin Plot\n",
    "\n",
    "Almost universally, across every day of the week, customers appear to have a higher trip duration than subscribers. While additional analysis will be required to confirm this, it's possible that one explanation is that subscribers can freely take and return their bikes which means that they're more willing to make shorter trips versus customers that pay each time they want to rent a bike in the first place. An alternate explanation, based on what we know in regards to the relationship between trip duration and linear distance traveled, is that subscribers are using the bikes for commuting to and from specific locations. This would result in a lower trip duration than customers that might use their bikes for general travel around the city. This possibility is corroborated by the decrease in activity on the weekends by subscribers.\n",
    "\n",
    "Identifying the point at which a customer might become a subscriber using this data would probably include monitoring weekday activity and trip duration. If a station has a lot of customers with trip durations similar to those of subscribers, then that station would be a good location to do a focused advertisement of the benefits of subscribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"tripdurationLog\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer vs. Subscriber Linear Trip Distance by Day of the Week Split Violin Plot\n",
    "\n",
    "Unlike trip duration, the linear distance between start and end stations for both customers and subscribers appear to be similar in regards to means and are close in their quartiles. But what's noticeable here, is that customers are more widely distributed in how far or near they ride, with a significant increase in the number of customers that return their bikes to the station they started from.\n",
    "\n",
    "Further analysis will be necessary to explore the statistical significance of these differences, but it would be possible to identify those stations that are frequented by subscribers and assume that most stations within the first standard deviation of the linear distance found below to be considered \"subscriber stations\" and then seen which stations are outside of those zones to further build up the messaging encouraging subscription. Furthermore, by identifying those \"hot zones\" it's possible to rotate out bikes to increase their longevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"DayOfWeek\", y=\"LinearDistance\", hue=\"usertype\", data=CitiBikeDataSampled_5050, split=True,\n",
    "               inner=\"quart\", palette={\"Subscriber\": \"g\", \"Customer\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2 - Prepping for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "\n",
    "Initially including all start (and end) locations resulted in excessive system resource loading, later during randomized principal component computations, that froze our personal workstations and eventually ended with Python 'MemoryError' messaging. Therefore, due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size manually. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations) and including all of them resulted in system crashes during principal component analysis later in our lab. We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual integer columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these encodings complete, our final dataset to cross-validate on test/train datasets would appear to be complete. However, given the large number of attributes now present in our dataset, it would be wise to investigate a means of dimensionality reduction to not only speed up model generation, but to also improve accuracy by removing variable redundancy and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Set Summary\n",
    "\n",
    "At this stage, we've converted our original 30 variables into 107 attributes after creating dummy variables for categorical data such as day of the week, time of day, station names, gender, etc. These 107 attributes and their data types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(CitiBikeDataSampled_5050.columns):\n",
    "    data_type.append(CitiBikeDataSampled_5050.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(CitiBikeDataSampled_5050.columns, index = range(len(CitiBikeDataSampled_5050.columns))), 'Data Type' : pd.Series(data_type, index = range(len(CitiBikeDataSampled_5050.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put brief text in place of this referencing PCA in last lab and the columns that move forward\n",
    "Even with our data cleaned and prepped using OneHotEncoding, there is the innate need to reduce this number of attributes to a more manageable size before classification and regression predictions are made. For this reason, we've performed randomized PCA to compute linear combinations of the data and have chosen to use the first 14 principal components for the *n_components* parameter in our classification PCA and the first 15 principal components for regression PCA, based on explained variance and cumulative variance measures.\n",
    "\n",
    "Eigenvectors, proportions of explained variance, and cumulative proportions of explained variance are provided for the classification principal components and then again for regression principal components below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE START\n",
    "\n",
    "Because our stratified, processed data is comprised of 105 various attributes ranging from weather and distance data to location and user type data across all 500,000 sample observations, and some variables such as weather attributes and even some start and end stations correlate to one another, we felt it would be wise during our previous Lab 2 analysis to reduce our number of attributes considered during Customer/Subscriber prediction. We proceeded to use Principal Component Analysis (PCA) to reduce the dimensionality of our dataset.\n",
    "\n",
    "Furthermore, during analysis of our principal components' loadings, we identified only 22 of the originally processed 105 attributes as being contextually important in identifying Customers and Subscribers. As the intent of our Lab 3 analysis is to further identify Customer users that should be Subscribers based on their behaviour, we deem it wise to only use these 22 attributes while clustering as well.\n",
    "\n",
    "These attributes are as follows:\n",
    "\n",
    "* start_station_latitude\n",
    "* start_station_longitude\n",
    "* end_station_latitude\n",
    "* end_station_longitude\n",
    "* PRCP\n",
    "* SNOW\n",
    "* TAVE\n",
    "* TMAX\n",
    "* TMIN\n",
    "* DayOfWeek_Friday\n",
    "* DayOfWeek_Monday\n",
    "* DayOfWeek_Saturday\n",
    "* DayOfWeek_Sunday\n",
    "* DayOfWeek_Thursday\n",
    "* DayOfWeek_Tuesday\n",
    "* DayOfWeek_Wednesday\n",
    "* TimeOfDay_Afternoon\n",
    "* TimeOfDay_Evening\n",
    "* TimeOfDay_Midday\n",
    "* TimeOfDay_Morning\n",
    "* TimeOfDay_Night\n",
    "* tripdurationLog\n",
    "\n",
    "In addition to using only these attributes while clustering, we chose to split our stratified sample data set of 500,000 transactions into separate Customer and Subscriber data sets while clustering. This will provide us the advantage of identifying clusters based on Customer data separately from Subscriber data - the advantage being that further granularity will be offered into the user sub-groups (based on transaction details) that comprise each user class. When later comparing these clusterings between each user class, we will be able to further classify one user type's data against the opposite user type's cluster IDs. This, and its implementation, will be described in much greater detail in the Deployment section. Currently, it suffices to say that clustering against each known user type is a necessary means of identifying would-be Subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset data set to only variables identified to have the greatest PCA loadings\n",
    "CitiBike_clus = CitiBikeDataSampled_5050[['start_station_latitude',\n",
    "                                          'start_station_longitude',\n",
    "                                          'end_station_latitude',\n",
    "                                          'end_station_longitude',\n",
    "                                          'PRCP',\n",
    "                                          'SNOW',\n",
    "                                          'TAVE',\n",
    "                                          'TMAX',\n",
    "                                          'TMIN',\n",
    "                                          'DayOfWeek_Friday',\n",
    "                                          'DayOfWeek_Monday',\n",
    "                                          'DayOfWeek_Saturday',\n",
    "                                          'DayOfWeek_Sunday',\n",
    "                                          'DayOfWeek_Thursday',\n",
    "                                          'DayOfWeek_Tuesday',\n",
    "                                          'DayOfWeek_Wednesday',\n",
    "                                          'TimeOfDay_Afternoon',\n",
    "                                          'TimeOfDay_Evening',\n",
    "                                          'TimeOfDay_Midday',\n",
    "                                          'TimeOfDay_Morning',\n",
    "                                          'TimeOfDay_Night',\n",
    "                                          'tripdurationLog',\n",
    "                                          'usertype']]\n",
    "\n",
    "CitiBike_C = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Customer']\n",
    "CitiBike_S = CitiBike_clus.loc[CitiBike_clus['usertype'] == 'Subscriber']\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)])\n",
    "cols = CitiBike_C.ix[:,0:(len(CitiBike_C.columns)-1)].columns\n",
    "CitiBike_C = pd.DataFrame(scaled, columns=cols)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)])\n",
    "cols = CitiBike_S.ix[:,0:(len(CitiBike_S.columns)-1)].columns\n",
    "CitiBike_S = pd.DataFrame(scaled, columns=cols)\n",
    "\n",
    "print('Customer data dimensions =', CitiBike_C.shape)\n",
    "print('Subscriber data dimensions =',CitiBike_S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to separting the data based on user type, we also scaled the data values to remove bias while clustering due to different attribute scales. Without scaling the data, attributes such as station coordinates and trip duration would carry heavier weights when compared against the OneHotEncoded attributes and precipitation data. This would cause unbalanced and improperly clustered groups. The first 5 standardized ride transactions are shown below for Customers and Subscribers as an example of what the data looks like after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(CitiBike_C.head())\n",
    "display(CitiBike_S.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 1 - Train and adjust parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN\n",
    "\n",
    "Our third approach to clustering the CitiBike user data is to implement DBSCAN on the 22 attributes selected for clustering. However, initial clustering attempts while including longitude and latitude data resulted in processing failure due to memory errors. For this reason, we have chosen to remove start and end station coordinate data from the attributes identified for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "#CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']]\n",
    "C_dbscan = CitiBike_C.ix[:,5:22]\n",
    "S_dbscan = CitiBike_S.ix[:,5:22]\n",
    "CitiBike_loc = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "print('Customer dbscan data dimensions =', C_dbscan.shape)\n",
    "print('Subscriber dbscan data dimensions =', S_dbscan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate removal aside, we do have some reservations about implementing DBSCAN clustering due to the density method being based on Euclidean distance measures. Even with 18 attributes instead of 22, the high dimensionality within our Customer and Subscriber data sets is expected to minimize the effectiveness of using distance as the primary measure. Nevertheless, such an attempt is still worth while in order to compare the results against our other clustering methods.\n",
    "\n",
    "By plotting cluster data against the geographical coordinates for each CitiBike station, we expect to be able to gain better insight into which cluster ID's appear at each station (Again, coordinate attributes are removed from the DBSCAN cluster data... plotting against coordinates is for marketing application purposes only). The initial coordinates, without clustering, are depicted in the following scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "#plt.plot(CitiBike_dbscan.start_station_longitude, CitiBike_dbscan.start_station_latitude, 'bo', markersize=3) #plot the data\n",
    "plt.plot(CitiBike_loc.start_station_longitude, CitiBike_loc.start_station_latitude, 'bo', markersize=3) #plot the data\n",
    "plt.title('Latitude/Longitude Data'.format(2))\n",
    "plt.xlabel('Longitude Coordinate'.format(2))\n",
    "plt.ylabel('Latitude Coordinate'.format(2))\n",
    "plt.xlim(-74.15,-73.8)\n",
    "plt.ylim(40.67,40.78)\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.grid()\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously while K-Means and Hierarchical clustering, customer and subscriber data will be clustered separately in order to help identify customers that should be subscribers but aren't, as will be discussed in greater detail in the Deployment section.\n",
    "\n",
    "Below are functions that will be utilized throughout the DBSCAN clustering process. The *getGraph* function simply generates a k-neighbors graph of the data in order to plot potential *eps* values based on a prescribed minimum number of samples to be used in the DBSCAN algorithm. Next, the *epsPlot* function plots these potential *eps* values. Finally, the third function, *dbs*, clusters the data based on prescribed *eps* and minimum number of samples parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def getGraph(N, data):\n",
    "    graph = kneighbors_graph(data, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    #CitiBike_knn_graph = kneighbors_graph(CitiBike_clus, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def epsPlot(graph):\n",
    "    N1 = graph.shape[0]\n",
    "    CitiBike_distances = np.zeros((N1,1))\n",
    "    for i in range(N1):\n",
    "        CitiBike_distances[i] = graph[i,:].max()\n",
    "\n",
    "    CitiBike_distances = np.sort(CitiBike_distances, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #plt.subplot(1,2,1)\n",
    "    plt.plot(range(N1), CitiBike_distances, 'r.', markersize=4) #plot the data\n",
    "    plt.title('Dataset name: CitiBike_clus, sorted by neighbor distance')\n",
    "    plt.xlabel('CitiBike_clus, Instance Number')\n",
    "    plt.ylabel('CitiBike_clus, Distance to {0}th nearest neighbor'.format(N))\n",
    "    #plt.xlim([400000,500000])\n",
    "    plt.annotate('Expected Eps value = approx. 0.0054', xy=(287, 0.0054), xytext=(200, 0.007),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "    plt.plot([0, 350], [0.0054, 0.0054], 'k--', lw=0.5)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def dbs(eps, minpts, data):\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_dbscan)\n",
    "    db = DBSCAN(eps=eps, min_samples=minpts).fit(data)\n",
    "    #db = DBSCAN(eps=eps, min_samples=minpts, metric='cosine', algorithm='brute').fit(CitiBike_clus)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    # mark the samples that are considered \"core\"\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    plt.figure(figsize=(15,4))\n",
    "    unique_labels = set(labels) # the unique labels\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = CitiBike_C[class_member_mask & core_samples_mask]\n",
    "        # plot the core points in this class\n",
    "        plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "                 markeredgecolor='w', markersize=6)\n",
    "\n",
    "        # plot the remaining points that are edge points\n",
    "        xy = CitiBike_C[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "                 markeredgecolor='w', markersize=3)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    #plt.xlim(-74.15,-73.8)\n",
    "    #plt.ylim(40.67,40.78)\n",
    "    plt.grid()\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.show()\n",
    "    \n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customer DBSCAN\n",
    "\n",
    "Before running the DBSCAN algorithm on our customer data set, we would first like to obtain a basic understanding of what types of eps and minimum number of samples to use. Due to the multi-dimensionality of our data, and the fact that we have 250,000 observations in our customer data set, it is difficult to define an appropriate mininum number of samples through visual inspection of the data. Therefore, we have chosen a preliminary sample count of 100 from which we will identify an eps value to use as our initial starting point.\n",
    "\n",
    "The plot below....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "CitiBike_knn_graph = getGraph(100, C_dbscan)\n",
    "epsPlot(CitiBike_knn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#eps = 0.0054\n",
    "#eps = 0.2 # for CitiBike_clus\n",
    "#eps = 1 # for CitiBike_clus without locations\n",
    "#eps = 0.1 # for CitiBike_clus without locations\n",
    "#minpts = 200\n",
    "db = dbs(0.5, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db1 = dbs(0.7, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db2 = dbs(0.9, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db3 = dbs(0.7, 250, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db4 = dbs(0.7, 150, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#for k, col in zip(unique_labels, colors):\n",
    "#    if k == -1:\n",
    "#        # Black used for noise.\n",
    "#        col = 'k'\n",
    "#\n",
    "#    class_member_mask = (labels == k)\n",
    "#\n",
    "#    xy = CitiBike_C[class_member_mask & core_samples_mask]\n",
    "#    # plot the core points in this class\n",
    "#    plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "#             markeredgecolor='w', markersize=6)\n",
    "#\n",
    "#    # plot the remaining points that are edge points\n",
    "#    xy = CitiBike_C[class_member_mask & ~core_samples_mask]\n",
    "#    plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "#             markeredgecolor='w', markersize=3)\n",
    "#\n",
    "#plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "#plt.xlim(-74.15,-73.8)\n",
    "#plt.ylim(40.67,40.78)\n",
    "#plt.grid()\n",
    "#plt.ticklabel_format(useOffset=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_customer = pd.DataFrame ({\"Cluster_ID_Customer\": db.labels_, \"Cluster_ID_Subscriber\": np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_C = pd.concat([CitiBike_C.reset_index(), cluster_id_customer], axis=1)\n",
    "#CitiBike_C = pd.concat([CitiBike_C, cluster_id_customer], axis=1)\n",
    "CitiBike_C.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subscriber DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#N = 100\n",
    "#CitiBike_knn_graph = kneighbors_graph(S_dbscan, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "#CitiBike_knn_graph = kneighbors_graph(CitiBike_clus, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors\n",
    "CitiBike_knn_graph = getGraph(100, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#N1 = CitiBike_knn_graph.shape[0]\n",
    "#CitiBike_distances = np.zeros((N1,1))\n",
    "#for i in range(N1):\n",
    "#    CitiBike_distances[i] = CitiBike_knn_graph[i,:].max()\n",
    "#\n",
    "#CitiBike_distances = np.sort(CitiBike_distances, axis=0)\n",
    "#\n",
    "#plt.figure(figsize=(15,5))\n",
    "##plt.subplot(1,2,1)\n",
    "#plt.plot(range(N1), CitiBike_distances, 'r.', markersize=4) #plot the data\n",
    "#plt.title('Dataset name: CitiBike_clus, sorted by neighbor distance')\n",
    "#plt.xlabel('CitiBike_clus, Instance Number')\n",
    "#plt.ylabel('CitiBike_clus, Distance to {0}th nearest neighbor'.format(N))\n",
    "##plt.xlim([400000,500000])\n",
    "#plt.annotate('Expected Eps value = approx. 0.0054', xy=(287, 0.0054), xytext=(200, 0.007),\n",
    "#            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "#plt.plot([0, 350], [0.0054, 0.0054], 'k--', lw=0.5)\n",
    "#plt.grid()\n",
    "#plt.show()\n",
    "epsPlot(CitiBike_knn_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#eps = 0.0054\n",
    "#eps = 0.2 # for CitiBike_clus\n",
    "#eps = 1 # for CitiBike_clus without locations\n",
    "#eps = 0.2 # for CitiBike_clus without locations **** with minpts = 200 provides 2 clusters with many outliers\n",
    "#eps = 0.25 # for CitiBike_clus without locations\n",
    "#minpts = 200\n",
    "#\n",
    "##db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_dbscan)\n",
    "#db = DBSCAN(eps=eps, min_samples=minpts).fit(S_dbscan)\n",
    "##db = DBSCAN(eps=eps, min_samples=minpts, metric='cosine', algorithm='brute').fit(CitiBike_clus)\n",
    "#labels = db.labels_\n",
    "#\n",
    "## Number of clusters in labels, ignoring noise if present.\n",
    "#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#\n",
    "## mark the samples that are considered \"core\"\n",
    "#core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#core_samples_mask[db.core_sample_indices_] = True\n",
    "#\n",
    "#plt.figure(figsize=(15,4))\n",
    "#unique_labels = set(labels) # the unique labels\n",
    "#colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "db_s = dbs(0.125, 200, S_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run kmeans algorithm (this is the most traditional use of k-means)\n",
    "kmeans = KMeans(init='k-means++', # initialization\n",
    "        n_clusters=2,  # number of clusters\n",
    "        n_init=20,       # number of different times to run k-means\n",
    "        n_jobs=-1)\n",
    "\n",
    "kmeans.fit(CitiBike_clus)\n",
    "\n",
    "# visualize the data\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(CitiBike_clus.tripdurationLog, CitiBike_clus.TAVE, 'r.', markersize=3)\n",
    "plt.scatter(centroids[:, 21], centroids[:, 6],\n",
    "            marker='+', s=200, linewidths=3, color='k')  # plot the centroids\n",
    "plt.title('K-means clustering for X1')\n",
    "plt.xlabel('X1, Feature 1')\n",
    "plt.ylabel('X1, Feature 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db_s1 = dbs(0.25, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_s2 = dbs(0.09, 200, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db3 = dbs(0.7, 250, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db4 = dbs(0.7, 150, C_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#for k, col in zip(unique_labels, colors):\n",
    "#    if k == -1:\n",
    "#        # Black used for noise.\n",
    "#        col = 'k'\n",
    "#\n",
    "#    class_member_mask = (labels == k)\n",
    "#\n",
    "#    xy = CitiBike_S[class_member_mask & core_samples_mask]\n",
    "#    # plot the core points in this class\n",
    "#    plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "#             markeredgecolor='w', markersize=6)\n",
    "#\n",
    "#    # plot the remaining points that are edge points\n",
    "#    xy = CitiBike_S[class_member_mask & ~core_samples_mask]\n",
    "#    plt.plot(xy.start_station_longitude, xy.start_station_latitude, '.', markerfacecolor=col,\n",
    "#             markeredgecolor='w', markersize=3)\n",
    "#\n",
    "#plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "#plt.xlim(-74.15,-73.8)\n",
    "#plt.ylim(40.67,40.78)\n",
    "#plt.grid()\n",
    "#plt.ticklabel_format(useOffset=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_subscriber = pd.DataFrame ({\"Cluster_ID_Customer\": np.NaN, \"Cluster_ID_Subscriber\": db_s.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_S = pd.concat([CitiBike_S.reset_index(), cluster_id_subscriber], axis=1)\n",
    "#CitiBike_S = pd.concat([CitiBike_S, cluster_id_subscriber], axis=1)\n",
    "CitiBike_S.tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "##This is for backup purposes only\n",
    "#\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "#\n",
    "#clf = GaussianMixture(n_components=22, covariance_type='full')\n",
    "#\n",
    "#clf.fit(CitiBike_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "#%R install.packages(\"kernlab\")\n",
    "%R library(kernlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%R -i CitiBike_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R print(head(CitiBike_clus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = %R capture.output(str(CitiBike_clus))\n",
    "\n",
    "for line in out:\n",
    "         print(line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "CitiBike_clus$DayOfWeek_Friday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Friday)))\n",
    "CitiBike_clus$DayOfWeek_Monday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Monday)))\n",
    "CitiBike_clus$DayOfWeek_Saturday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Saturday)))\n",
    "CitiBike_clus$DayOfWeek_Sunday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Sunday)))\n",
    "CitiBike_clus$DayOfWeek_Thursday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Thursday)))\n",
    "CitiBike_clus$DayOfWeek_Tuesday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Tuesday)))\n",
    "CitiBike_clus$DayOfWeek_Wednesday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Wednesday)))\n",
    "CitiBike_clus$TimeOfDay_Afternoon <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Afternoon)))\n",
    "CitiBike_clus$TimeOfDay_Evening <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Evening)))\n",
    "CitiBike_clus$TimeOfDay_Midday <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Midday)))\n",
    "CitiBike_clus$TimeOfDay_Morning <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Morning)))\n",
    "CitiBike_clus$TimeOfDay_Night <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Night)))\n",
    "\n",
    "CitiBike_clus$usertype <- NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = %R capture.output(str(CitiBike_clus))\n",
    "\n",
    "for line in test:\n",
    "         print(line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "CitiBike_mini <- CitiBike_clus[sample(1:nrow(CitiBike_clus), 500, replace=FALSE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "s <- function(x1, x2, alpha=1) {\n",
    "  exp(- alpha * norm(as.matrix(x1-x2), type=\"F\"))\n",
    "}\n",
    "\n",
    "make.similarity <- function(my.data, similarity) {\n",
    "  N <- nrow(my.data)\n",
    "  S <- matrix(rep(NA,N^2), ncol=N)\n",
    "  for(i in 1:N) {\n",
    "    for(j in 1:N) {\n",
    "      S[i,j] <- similarity(my.data[i,], my.data[j,])\n",
    "    }\n",
    "  }\n",
    "  S\n",
    "}\n",
    "\n",
    "S <- make.similarity(CitiBike_mini, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "make.affinity <- function(S, n.neighboors=2) {\n",
    "  N <- length(S[,1])\n",
    "\n",
    "  if (n.neighboors >= N) {  # fully connected\n",
    "    A <- S\n",
    "  } else {\n",
    "    A <- matrix(rep(0,N^2), ncol=N)\n",
    "    for(i in 1:N) { # for each line\n",
    "      # only connect to those points with larger similarity \n",
    "      best.similarities <- sort(S[i,], decreasing=TRUE)[1:n.neighboors]\n",
    "      for (s in best.similarities) {\n",
    "        j <- which(S[i,] == s)\n",
    "        A[i,j] <- S[i,j]\n",
    "        A[j,i] <- S[i,j] # to make an undirected graph, ie, the matrix becomes symmetric\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  A  \n",
    "}\n",
    "\n",
    "A <- make.affinity(S, 3)  # use 3 neighboors (includes self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R D <- diag(apply(A, 1, sum)) # sum rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%R\n",
    "#\"%^%\" <- function(M, power)\n",
    "#  with(eigen(M), vectors %*% (values^power * solve(vectors)))\n",
    "#    \n",
    "#L <- (D %^% (-1/2)) %*% A %*% (D %^% (-1/2))  # normalized Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R U <- D - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "k   <- 2\n",
    "evL <- eigen(U, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R signif(evL$values,2) # eigenvalues are in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evL$values)[1:10], xlab = \"Clusters\", ylab = \"Laplacian Eigenvalues\")\n",
    "abline(v=2.5, col=\"red\", lty=2) # there are just 2 clusters as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss <- 0\n",
    "wss[1] <- ((length(mCiti)/length(colnames(mCiti)))-1)*sum(apply(mCiti,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCiti, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Clusters\", ylab=\"Within groups sum of squares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%R set.seed(100)\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "%R mCiti <- data.matrix(CitiBike_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%R colnames(mCiti) <- colnames(CitiBike_clus[5:22])\n",
    "%R colnames(mCiti) <- colnames(CitiBike_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "sc <- specc(mCiti, centers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R plot(mCiti, col=sc, pch=19)            # estimated classes (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R length(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = %R capture.output(sc)\n",
    "\n",
    "for line in test:\n",
    "         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 2 - Evaluate and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 3 - Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 4 - Summarize the Ramifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE END\n",
    "# ALEX'S CLASSIFICATION MODEL CODE START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Clusters\n",
    "\n",
    "With our clusters fit and appended to the original data, we still do not know what customer cluster our subscriber observations are fit to and vice versa. In order to fit clusters for the inverse user type, we need to build a classification model that can predict the appropriate cluster with strong confidence. For these models, when focusing on accuracy well primarily use confusion matrices to explore our results alongside plotted ROC curves. \n",
    "\n",
    "We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 10 folds. This means, that from our original sample size of 250,000, each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst clusters. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Create CV Object for StratifiedKFold with 10 Folds, seeded at the length of our sample size\n",
    "seed = len(CitiBike_S)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. We tuned to provide output that completed all 10 iterations in under 10 minutes.\n",
    "\n",
    "After 13 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 70.2668 % from default inputs of the random forest classification model to a value of 72.5192 % in the best tuned model fit. Although the run-time of this model parameter choice is the largest performed, we decided to remain with these inputs due to the amount increase in accuracy. As mentioned previously, we tuned the n_estimators parameter to ensure we stayed under 10 minutes execution. Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "**Subscriber Parameters**\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>1000.0</th>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Customer Parameters**\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>1000.0</th>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def rfc_explor(ScaledData,\n",
    "               n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf,\n",
    "               y,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    accuracy = cross_val_score(rfc_clf, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,  \n",
    "                                         ScaledData,\n",
    "                                         y,\n",
    "                                         classes,\n",
    "                                         cv       = cv):\n",
    "    \n",
    "\n",
    "    X = ScaledData.as_matrix() \n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation\n",
    "\n",
    "    accuracy = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "       \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "   \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5)))\n",
    "\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(y[test], y_hat, rownames = ['True'], colnames = ['Predicted'], margins = True)))   \n",
    "    \n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   = classes, \n",
    "                          normalize = True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Subscribers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor(ScaledData        = CitiBike_S.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                              n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i],\n",
    "                              y = CitiBike_S[\"Cluster_ID_Subscriber\"].values\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Cluster Parameters for Customers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "for i in range(0,len(n_estimators)):\n",
    "    acclist.append(rfc_explor(ScaledData        = CitiBike_C.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                              n_estimators      = n_estimators[i],\n",
    "                              max_features      = max_features[i],\n",
    "                              max_depth         = max_depth[i],\n",
    "                              min_samples_split = min_samples_split[i],\n",
    "                              min_samples_leaf  = min_samples_leaf[i],\n",
    "                              y = CitiBike_C[\"Cluster_ID_Customer\"].values\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"n_estimators\": n_estimators,          \n",
    "                                                \"max_features\": max_features,         \n",
    "                                                \"max_depth\": max_depth,        \n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "rfcdf.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "display(rfcdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Analyze the Results\n",
    "We have created a function to be used for our cross-validation Accuracy Scores. Model CLF object, original sample y values, a distinct list of classes, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results. \n",
    "\n",
    "With our tuned parameters identified we may now assess futher insights. The Random Forest Model computes a more successful average accuracy rating of 72.496 %, and an Average Log Loss Value of 9.49958 across all 10 iterations. Once again, plotting our True Positive rate vs. False Positive rate in an ROC curve provides insight depicting consistency across iterations. Our Mean ROC area under the curve for all 10 iterations is 0.80, an improvement from logistic regression. We notice the ROC curve line is slightly further to the top left of the plot - as previously discussed, this is the type of improvement we were looking for! In a confusion matrix of predicted results, we find that we have a true positive rating of 72.272 % for subscriber users, whereas our customer true positive rating is 72.952 %. This provides us with 6762 Customer riders(out of 25,000), which may be flagged as potential targets for conversion marketing to subscribing members. With such an improvement in our true positive subscriber ratings, we have much more confidence in the false positive customer predictions here in comparison to logistic regression results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Subscriber, rfc_acc_Subscriber = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_S.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_S[\"Cluster_ID_Subscriber\"].values,\n",
    "                                                       classes     = CitiBike_S[\"Cluster_ID_Subscriber\"].drop_duplicates().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf_Customer, rfc_acc_Customer = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                                       ScaledData  = CitiBike_C.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1),\n",
    "                                                       y           = CitiBike_C[\"Cluster_ID_Customer\"].values,\n",
    "                                                       classes     = CitiBike_C[\"Cluster_ID_Customer\"].drop_duplicates().values\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Predict Cluster Values for Inverse UserType Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = CitiBike_C.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1)\n",
    "\n",
    "CitiBike_C[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = CitiBike_S.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1)\n",
    "\n",
    "CitiBike_S[\"Cluster_ID_Customer\"] = rfc_clf_Customer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_WithClusters = pd.concat([CitiBike_C, CitiBike_S])\n",
    "display(CitiBike_WithClusters.head())\n",
    "display(CitiBike_WithClusters.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALEX'S CLASSIFICATION MODEL CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "### How useful is the model for interested parties?\n",
    "\n",
    "##### Classification\n",
    "**How is this model useful?** Citibike aims to increase rider subscriptions, but may fear adverts to the wrong people could keep customers from enjoying their services. Our classification model predicting usertype as customer or subscriber allows Citibike to identify those customers who meet the criteria for common subscribing members. The Random Forest Classifier model produced an overall accuracy rating of 72.496 %, and although this may not be extremely optimal for True Positive matches, it does provide us with enough information to make educated decisions on which customers may already be considering converting to a subscribing member. These False Positive Customers, may be targeted leaving all others alone, reducing risk of losing customer traffic in general. \n",
    "\n",
    "**How would this model be deployed?** This model has the ability to be deployed as real-time predictions, or as a periodic corporate marketing alert if customer email addresses are readily available. Our preference would be a real-time prediction as a customer returns a bike. Upon return, if the model suggests a customer contains subcribing tendencies email alerts, pop-up promotions,etc. may be deployed in order to gain the customer's attention towards subscriber offerings. Alternatively, the marketing team could receive this information periodically, and implement custom strategies based on industry best practice techniques to this target group. This group of individuals are likely more apt to acknowledge these tactics positively, since their usage tendencies already align more closely to that of a subcribing member.\n",
    "\n",
    "**How often would the model need to be updated?** This model would definitely need to be updated periodically. As the citibike Share service grows, and new locations arrive, the model will need to be updated to account for the new sites. Also, as the population in NYC shifts over time(new businesses, schools, residential, etc.), trends may also fluctuate. These fluctuations will need to be accounted for in the model regularly to keep it up to date with current state NYC. Our recommendation for these updates would be periodic (monthly or quarterly) model fit updates in CitiBike systems to account for these possible changes.\n",
    "\n",
    "\n",
    "##### Regression\n",
    "**How is this model useful?** One of the known issues with CitiBike rentals is a problem of availability. With this model we can attempt to predict how long a bike would be unavailable once it's rented. With the known features and classifiers, it'd be possible to proactively predict bike availability during the day. Even with a low goodness of fit, the low MSE provided by the model results in the ability to allow CitiBike to anticipate the trip duration for each rental. At the end of the predicted time, it could be assumed by the station that the bike had been relocated to another station (updating otherwise should the bike be returned).\n",
    "\n",
    "**How would this model be deployed?** We built our model using only information that would be available to a station at the point at which a rental is made. Our deployment then, would be best used by the station itself to predict, once the bike is removed, how long the bike would be unavailable. If it reaches a point where a defined percentage of its bikes are calculated to be unavailable all at the same time, the station would be able to alert CitiBike and arrangements could be made to either resupply the station or, as we might suggest, send out automated messages to subscribers noting the station as a \"high reward\" station for returning bikes. This would give CitiBike hours of notice rather than minutes in the even of a high-activity day.\n",
    "\n",
    "**How often would the model need to be updated?** As the data set itself is constantly updating, the model could be updated by each station or as an aggregate of all stations in a remote database. Because it's a simple enough model, it could be run on a schedule depending upon the number of observations and the available processing power to stay up to day with current trends. Like the classification models, this would also need to be updated periodically, but likely at a much higher rate, possibly weekly, with less emphasis on historical trends. \n",
    "\n",
    "##### Additional Data to Collect:\n",
    "* **Event/Restaurant/Retail Data:** Given that we have detailed geocoordinate data and have already demonstrated powerful use of the Google Maps API, it would be possible to incorporate location details surrounding Citi Bike start and stop locations. There is potential for such data to be gathered automatically using API's such as Google's. Having this data would provide further insight into some of the reasons some bike share locations are more popular than others. Such data could even help Citi Bike predict changes in station demand based on changing surroundings and help determine where new stations should be installed.\n",
    "* **Special Events:** Similar to the previous idea, merging other public data based on geophysical coordinates and timeline could introduce other external factors such as the effects of parades, public concerts, festivals, and other events on ride activity for a given day or week. This would help identify/predict abnormal activity in this and future data sets. Additionally, it would provide insight to Citi Bike as to how to better plan and prepare for such events to boost rental count and increase trip duration.\n",
    "* **GPS Enabled Bike Computers:** Though not influenced by the data we have at hand, adding bicycle tracking hardware to each Citi Bike rental would provide substantial value to future data sets. Adding GPS tracking would enable Citi Bike to track specific routes followed by clients and could even aid NYC planners with transportation projects. Having route information means that true distance covered would be available, an attribute that would have far more meaning than our LinearDistance attribute. Incorporating GPS tracking with bike speed would provide insights into individual rider activity. For example, just because a rider's trip duration was 6 hours doesn't mean they actively rode for that amount of time. It is far more likely such a rider would have stopped for an extended period of time at least once during this period of time. Adding GPS and speed data would alleviate these existing gaps.\n",
    "\n",
    "### Deploying the Chosen Model on new data\n",
    "We have discussed above, what value our model holds, our preferred method of deployment, and frequency of model updates. Below we will walk through the process for prepping a real-time prediction model for deployment, and actually executing model predictions on new data inputs. \n",
    "\n",
    "##### Prepping the Model for Deployment\n",
    "A key component in deploying our model is the re-use of data transformation and / or Model fit objects created during this process. We need to be able to apply new data into the same constructs listed below, which were utilized in the Testing and Training process:\n",
    "* Compute both cluster attributes, and append to the dataset\n",
    "* Min-Max Scaler?\n",
    "* Model CLF Fit (RF decision trees)\n",
    "\n",
    "To do this, we must take our python objects currently stored in our active Kernel and permanently store them in a \"Pickled\" (.pkl) file. This .pkl file is a serializes version of our python object which can be accessed later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "def pickleObject(objectname, filename, filepath = \"PickleFiles/\"):\n",
    "    fullpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    picklefile = open(fullpicklepath, 'wb')\n",
    "    pickle.dump(objectname, picklefile)\n",
    "    picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SubClusterCols = CitiBike_S.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() \n",
    "CusClusterCols = CitiBike_S.drop([\"index\",\"usertype\",\"Cluster_ID_Customer\", \"Cluster_ID_Subscriber\"], axis=1).columns.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objectlist   = [[1, 2, 3, 4, 5], rfc_clf_Subscriber, rfc_clf_Customer, SubClusterCols, CusClusterCols   ]\n",
    "filenamelist = [\"list\",          \"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(objectlist)):\n",
    "    pickleObject(objectlist[i], filenamelist[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions on New Data\n",
    "Below is a single data entry for a Customer upon the return of a bike rental. We have produces fake values, based on possible value ranges present in our original dataset from Citibike. For the purpose of this \"real-time\" prediction, the current date/time of execution is utilized for a user submitting a bike return. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "NewData = pd.DataFrame({\n",
    "                        \"tripduration\": [579],\n",
    "                        \"starttime\": [datetime.now() + timedelta(seconds = -579)],\n",
    "                        \"stoptime\": [datetime.now()],\n",
    "                        \"start_station_id\": [477],\n",
    "                        \"start_station_name\": [\"W 41 St & 8 Ave\"],\n",
    "                        \"start_station_latitude\": [40.756405],\n",
    "                        \"start_station_longitude\": [-73.990026],\n",
    "                        \"end_station_id\": [441],\n",
    "                        \"end_station_name\": [\"E 52 St & 2 Ave\"],\n",
    "                        \"end_station_latitude\": [40.756014],\n",
    "                        \"end_station_longitude\": [-73.967416],\n",
    "                        \"bikeid\": [16537],\n",
    "                        \"usertype\": [\"Customer\"],\n",
    "                        \"birthyear\": [\"\\\\N\"],\n",
    "                        \"gender\": [0]\n",
    "                      })\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformations and Additional Features**\n",
    "\n",
    "xxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"LinearDistance\"] = NewData.apply(lambda x: vincenty((x[\"start_station_latitude\"], x[\"start_station_longitude\"]), \n",
    "                                                             (x[\"end_station_latitude\"],   x[\"end_station_longitude\"])).miles,\n",
    "                                          axis = 1)\n",
    "\n",
    "    \n",
    "    ## Compute DayOfWeek from Start Time\n",
    "        # starttime needs to be converted to a pandas datetime type before we can find the weekday name\n",
    "NewData['starttime'] = pd.to_datetime(NewData['starttime'])\n",
    "NewData[\"DayOfWeek\"] = NewData['starttime'].dt.weekday_name\n",
    "\n",
    "    ## Compute TimeOfDay from Start Time\n",
    "        ##Morning       5AM-10AM\n",
    "        ##Midday        10AM-2PM\n",
    "        ##Afternoon     2PM-5PM\n",
    "        ##Evening       5PM-10PM\n",
    "        ##Night         10PM-5AM\n",
    "\n",
    "\n",
    "NewData[\"TimeOfDay\"] = np.where((NewData['starttime'].dt.hour >= 5) & (NewData['starttime'].dt.hour < 10), 'Morning',\n",
    "                                np.where((NewData['starttime'].dt.hour >= 10) & (NewData['starttime'].dt.hour < 14), 'Midday',\n",
    "                                         np.where((NewData['starttime'].dt.hour >= 14) & (NewData['starttime'].dt.hour < 17), 'Afternoon',\n",
    "                                                  np.where((NewData['starttime'].dt.hour >= 17) & (NewData['starttime'].dt.hour < 22), 'Evening',\n",
    "                                                           'Night' ### ELSE case represents Night\n",
    "                                                          )\n",
    "                                                 )\n",
    "                                        )\n",
    "                               )\n",
    "                                                  \n",
    "    ## Compute LinearDistance from Start/End Lat,Long Coordinates\n",
    "NewData[\"HolidayFlag\"] = NewData['starttime'].isin(holidays.UnitedStates())\n",
    "NewData[\"HolidayFlag\"] = np.where(NewData[\"HolidayFlag\"] == False, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import pyowm\n",
    "\n",
    "owm = pyowm.OWM('462d2effa0ba127689b824b37efc9d12')  # You MUST provide a valid API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "forecaster = owm.three_hours_forecast_at_coords(lat = float(NewData[\"start_station_latitude\"]), lon = float(NewData[\"start_station_longitude\"]))\n",
    "forecast = forecaster.get_forecast()\n",
    "fweather_list = forecast.get_weathers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tlist = []\n",
    "plist = []\n",
    "slist = []\n",
    "\n",
    "\n",
    "for x in fweather_list:\n",
    "    date = datetime.date(datetime.strptime(x.get_reference_time('iso'),\"%Y-%m-%d %H:%M:%S+00\") + timedelta(hours = -5))\n",
    "    \n",
    "    if (date >= datetime.date(NewData[\"starttime\"].min())) \\\n",
    "    and (date < datetime.date(NewData[\"starttime\"].min())+ timedelta(days = 1)): \n",
    "        temp = x.get_temperature('fahrenheit')['temp']\n",
    "        prcp = x.get_rain()\n",
    "        snow = x.get_snow()\n",
    "        tlist.append(temp)\n",
    "        \n",
    "        if prcp == {}:\n",
    "            plist.append(0)\n",
    "        else:\n",
    "            plist.append(prcp)\n",
    "        \n",
    "        if snow == {}:\n",
    "            slist.append(0)\n",
    "        else:\n",
    "            slist.append(snow)\n",
    "        \n",
    "tempdata = pd.DataFrame(tlist)\n",
    "prcpdata = pd.DataFrame(plist)\n",
    "snowdata = pd.DataFrame(slist)\n",
    "\n",
    "NewData[\"TMIN\"] = tempdata.min()\n",
    "NewData[\"TAVE\"] = tempdata.mean()\n",
    "NewData[\"TMAX\"] = tempdata.max()\n",
    "NewData[\"PRCP\"] = prcpdata.sum()\n",
    "NewData[\"SNOW\"] = snowdata.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "NewData[\"birthyear\"] = NewData[\"birthyear\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = NewData[['tripduration', 'birthyear', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "NewData[['starttime', 'stoptime']] \\\n",
    "    = NewData[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "NewData[\"Age\"] = np.where(NewData[\"birthyear\"] == 0, 0,\n",
    "                                       NewData[\"starttime\"].dt.year - NewData[\"birthyear\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = NewData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)\n",
    "\n",
    "# Log Transform Column Added\n",
    "NewData[\"tripdurationLog\"] = NewData[\"tripduration\"].apply(np.log)\n",
    "    \n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding categorical attributes**\n",
    "\n",
    "xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## DayOfWeek\n",
    "NewData[\"DayOfWeek_Monday\"] = np.where(NewData[\"DayOfWeek\"] == \"Monday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Tuesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Tuesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Wednesday\"] = np.where(NewData[\"DayOfWeek\"] == \"Wednesday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Thursday\"] = np.where(NewData[\"DayOfWeek\"] == \"Thursday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Friday\"] = np.where(NewData[\"DayOfWeek\"] == \"Friday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Saturday\"] = np.where(NewData[\"DayOfWeek\"] == \"Saturday\", 1, 0)\n",
    "NewData[\"DayOfWeek_Sunday\"] = np.where(NewData[\"DayOfWeek\"] == \"Sunday\", 1, 0)\n",
    "\n",
    "## TimeOfDay\n",
    "NewData[\"TimeOfDay_Morning\"] = np.where(NewData[\"TimeOfDay\"] == \"Morning\", 1, 0)\n",
    "NewData[\"TimeOfDay_Midday\"] = np.where(NewData[\"TimeOfDay\"] == \"Midday\", 1, 0)\n",
    "NewData[\"TimeOfDay_Afternoon\"] = np.where(NewData[\"TimeOfDay\"] == \"Afternoon\", 1, 0)\n",
    "NewData[\"TimeOfDay_Evening\"] = np.where(NewData[\"TimeOfDay\"] == \"Evening\", 1, 0)\n",
    "NewData[\"TimeOfDay_Night\"] = np.where(NewData[\"TimeOfDay\"] == \"Night\", 1, 0)\n",
    "\n",
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to deploying predictions on new data is to \"unpickle\" the .pkl file objects needed for our model. We need to unpickle the below constructs in order to re-produce our model, and apply it on new data:\n",
    "* Compute both cluster attributes, and append to the dataset\n",
    "* Min-Max Scaler?\n",
    "* Model CLF Fit (RF decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickleObject(filename, filepath = \"PickleFiles/\"):\n",
    "    fullunpicklepath = \"{0}{1}.pkl\".format(filepath, filename)\n",
    "    # Create an variable to pickle and open it in write mode\n",
    "    unpicklefile = open(fullunpicklepath, 'rb')\n",
    "    unpickleObject = pickle.load(unpicklefile)\n",
    "    unpicklefile.close()\n",
    "    \n",
    "    return unpickleObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unpickleObjectList = []\n",
    "filenamelist = [\"list\",\"rfc_clf_Subscriber\",  \"rfc_clf_Customer\", \"SubClusterCols\", \"CusClusterCols\" ]\n",
    "\n",
    "for i in range(0,len(filenamelist)):\n",
    "    unpickleObjectList.append(unpickleObject(filenamelist[i]))\n",
    "\n",
    "list = unpickleObjectList[0]\n",
    "rfc_clf_Subscriber = unpickleObjectList[1]\n",
    "rfc_clf_Customer = unpickleObjectList[2]\n",
    "SubClusterCols = unpickleObjectList[3]\n",
    "CusClusterCols = unpickleObjectList[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = NewData[SubClusterCols]\n",
    "\n",
    "NewData[\"Cluster_ID_Subscriber\"] = rfc_clf_Subscriber.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = NewData[CusClusterCols]\n",
    "\n",
    "NewData[\"Cluster_ID_Customer\"] = rfc_clf_Customer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(NewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our clusters added to the dataset, the bike return data for this user is now finally ready to be passed through a classication / regression model of choice. As discussed previously, given the purpose of this report and the focus on clustering - this model has not been prepared. \n",
    "\n",
    "Note that the run-time for transforming/adding new features to the original input completes within XX seconds. This, of course, does not include the classification / regression model, but is a realistic timeframe for real-time application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "This lab was the most challenging and time consuming yet, partly because we chose to implement PCA in order to reduce the dimensionality of our data set and partly because we chose to explore new model types such as Neural Networks MLP Regression for trip duration prediction. The additional research and work put toward these records seemed to balloon at times, especially when considering MLP interpretations and ranking attributes through the haze of PCA's linear combinations. We hope, however, that our efforts did not go unnoticed given the added time we spent ensuring we utilized and interpreted these tools correctly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "793f6c3588b241bea02fa95829c013e4": {
     "views": [
      {
       "cell_index": 36
      }
     ]
    },
    "8ad7880c98454b9b90b2b8246bbe6beb": {
     "views": [
      {
       "cell_index": 49
      }
     ]
    },
    "fd8dadcf3d2a44e2bb004d0e94ec2eaa": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
