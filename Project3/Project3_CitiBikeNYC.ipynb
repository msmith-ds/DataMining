{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Classification and Regression -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 4/05/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2_Full/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for Lab 2 consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behavior.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this lab is to: 1) Predict whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, etc., and 2) predict a rider's trip duration as a function of the aforementioned variables.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve these goals (as described further in the sections below). By leveraging Stratified 10-Fold Cross Validation, we expect to be able to derive dependable and accurate user type and ride duration prediction models for which accuracy and performance will be discussed in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250,000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Preparation 1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1 - Define and prepare class variables\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2_Full/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4,881,384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680,909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to each other. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 hour period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however, we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepping Data for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "\n",
    "Initially including all start (and end) locations resulted in excessive system resource loading, later during randomized principal component computations, that froze our personal workstations and eventually ended with Python 'MemoryError' messaging. Therefore, due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size manually. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations) and including all of them resulted in system crashes during principal component analysis later in our lab. We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual integer columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these encodings complete, our final dataset to cross-validate on test/train datasets would appear to be complete. However, given the large number of attributes now present in our dataset, it would be wise to investigate a means of dimensionality reduction to not only speed up model generation, but to also improve accuracy by removing variable redundancy and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2 - Describe the final dataset that is used for classification/regression\n",
    "\n",
    "##### Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "With our data split evenly among customer and subscriber user types and OneHotEncoding complete, we are ready to consider the new dimensions of our dataset. Because our processed data is comprised of 105 various attributes ranging from weather and distance data to location and user type data across all 500,000 sample observations, and some variables such as weather attributes and even some start and end stations correlate to one another as depicted in the correlation matrix below, we feel it would be wise to reduce our number of attributes considered during model generation. After considering a variety of feature selection techniques, we've opted for dimensionality reduction via principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(CitiBikeDataSampled_5050.shape)\n",
    "\n",
    "sns.set(font_scale=0.5)\n",
    "sns.heatmap(CitiBikeDataSampled_5050.corr(), \n",
    "            xticklabels=CitiBikeDataSampled_5050.corr().columns.values,\n",
    "            yticklabels=CitiBikeDataSampled_5050.corr().columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing PCA, we are able to shrink the number of variables required for prediction purposes, replacing our latest variables derived up to this point with linear combinations of themselves. Each of these linear combinations makes up a component and comprises an eigenvector. When these eigenvectors are ordered by largest eigenvalues, representing the extent of variability explained by each vector, the vectors with largest eigenvalues may be identified as the principal components of the analysis. Selecting only those components which describe the most variance will help us reduce processing times and improve model performance and accuracy by avoiding variable inflation and overfitting associated with high dimensionality.\n",
    "\n",
    "Since this lab consists of two primary tasks (the first being to classify user types and the second being to predict trip duration), this section outlines two different PCA's, one for each task, in order to identify the appropriate number of components to include during model generation in subsequent sections. The appropriate number of components must be derived separately since user type classification will include trip duration predictors whereas trip duration prediction will require user type predictors. The expectation is that the number of components should be similar for both tasks, but it is not enough to make this assumption alone; both will be reviewed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA for Classification\n",
    "Our first objective is to identify the number of components to be used for user type classification. In order to do so, we first exclude redundant and non-value variables up front. Non-value variables include gender, birth year, and age since these data were missing for most Customer user types and were replaced with filler values as discussed in previous sections. We will exclude these since they misrepresent correlation with user type. Remaining attribute data will then be standardized so that all variables are on the same scale given that our explanatory variables are comprised of many different measures.\n",
    "\n",
    "*(Note: PCA code steps adapted and modified from https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class_att = CitiBikeDataSampled_5050.drop(['starttime', 'stoptime', 'start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'usertype', 'gender', 'gender_0', 'gender_1', 'gender_2', 'birth year', 'Age', 'tripduration', 'DayOfWeek', 'TimeOfDay'], axis=1)\n",
    "myData = class_att.as_matrix() \n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "myData_scaled_classification = min_max_scaler.fit_transform(myData)\n",
    "\n",
    "del myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excluding duplicate and non-value attributes and scaling our data, we verify that 89 attributes remain. The maximum number of components to be produced will match this number. For this reason, we will identify 89 to be the number of components produced by our PCA and will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. Using full PCA required unacceptable lengths of time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(myData_scaled_classification.shape)\n",
    "pca_class = PCA(n_components=89, svd_solver='randomized')\n",
    "\n",
    "pca_class.fit(myData_scaled_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first 14 principal components  the change is rather steep through the 14th component. After the 1% drop between components 14 and 15, the rate of decreasing explained variance begins to somewhat flatten out, reducing to a 0.2% change or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,90), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, 90, -0.1, 12])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance arguably begins to plateau around the 14th principal component and that the first 14 components together explain 77.92% of variance in the data set. For this reason, 14 principal components may be selected as being the most appropriate for user type classification modeling given the variables among these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,90), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, 90, 10, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pca_classification = PCA(n_components=14, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA for Regression\n",
    "\n",
    "Now that 14 principal components have been identified for Customer/Subscriber classification, we want to determine the correct number of principal components to use when predicting trip duration. For regression, we will need to interchange user type and log trip duration (which will be what we are actually predicting later before back transforming to regular trip duration) inclusion/exclusion. Before including user type, however, OneHotEncoding this variable is required since this has not been done as user type has always been discussed in terms of response up to this point (classification).\n",
    "\n",
    "We will also be removing end station names for regression since, contextually, CitiBike does not know the end location of a rider at the time the rider checks out a bike. Yes, this information becomes available at checkin... but if the company is to counterbalance the shortage effects of having too many bikes checked out during the same period of time, the model needs to exclude trip end location details. This concept will be discussed further later in this writeup. Again, all attributes are standardized before eigenvectors are computed for reasons stated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate usertype dummies for use in regression PCA\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.usertype,prefix='usertype')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "# Include usertype but exclude trip duration\n",
    "reg_att = CitiBikeDataSampled_5050.drop(['starttime', 'stoptime', 'start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'usertype','tripdurationLog', 'gender', 'gender_0', 'gender_1', 'gender_2', 'birth year', 'Age', 'tripduration', 'DayOfWeek', 'TimeOfDay', 'LinearDistance'], axis=1)\n",
    "myData = reg_att.columns.values.tolist() \n",
    "myData = [i for i in myData if \"end_station_name_\" not in i]\n",
    "reg_att = CitiBikeDataSampled_5050[myData]\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "myData_scaled_regression = min_max_scaler.fit_transform(reg_att.as_matrix())\n",
    "\n",
    "del myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After OneHotEncoding the user type variable, including only appropriate variables, and standardizing, we are left with 56 attributes. Therefore we can expect to produce a total of 56 linear combinations in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(myData_scaled_regression.shape)\n",
    "pca_reg = PCA(n_components=56, svd_solver='randomized')\n",
    "\n",
    "pca_reg.fit(myData_scaled_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering the resulting eigenvectors by variance proportion explained and plotting the scree plot below reveals dramatic change in explained variance through component 15. The 0.85% change in explained variance between components 15 and 16 flattens out to changes less than 0.33% between components thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_reg.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1,57), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, 60, -0.1, 20])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative variance values and plot that follow indicate that total variance begins to plateau around the 15th principal component, supporting our previous conclusions. The plot also indicates the first 15 components explain 89.35% of data set variance. Based on these results, we will use 15 principal components for our trip duration prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_reg.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,57), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([-1, 60, 15, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pca_regression = PCA(n_components=15, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Set Summary\n",
    "\n",
    "At this stage, we've converted our original 30 variables into 107 attributes after creating dummy variables for categorical data such as day of the week, time of day, station names, gender, etc. These 107 attributes and their data types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(CitiBikeDataSampled_5050.columns):\n",
    "    data_type.append(CitiBikeDataSampled_5050.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(CitiBikeDataSampled_5050.columns, index = range(len(CitiBikeDataSampled_5050.columns))), 'Data Type' : pd.Series(data_type, index = range(len(CitiBikeDataSampled_5050.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with our data cleaned and prepped using OneHotEncoding, there is the innate need to reduce this number of attributes to a more manageable size before classification and regression predictions are made. For this reason, we've performed randomized PCA to compute linear combinations of the data and have chosen to use the first 14 principal components for the *n_components* parameter in our classification PCA and the first 15 principal components for regression PCA, based on explained variance and cumulative variance measures.\n",
    "\n",
    "Eigenvectors, proportions of explained variance, and cumulative proportions of explained variance are provided for the classification principal components and then again for regression principal components below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Classification Task Principal Component Breakdown\n",
    "pca_classification.fit(myData_scaled_classification)\n",
    "var = pca_classification.explained_variance_\n",
    "var1 = pca_classification.explained_variance_ratio_\n",
    "var2 = np.cumsum(np.round(pca_classification.explained_variance_ratio_, decimals=4))\n",
    "idx = ['PC'+str(i) for i in range(1,15)]\n",
    "\n",
    "PCA_df = {'...Eigenvector...' : pd.Series(var, index = idx),\n",
    "          '..Variance Proportion..' : pd.Series(var1, index = idx),\n",
    "          '.Variance Cumulative Proportion.' : pd.Series(var2, index = idx)}\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_df)\n",
    "print('Classification Task Principal Component Breakdown')\n",
    "display(PCA_df)\n",
    "\n",
    "del var, var1, var2, idx\n",
    "pca_classification = PCA(n_components=14, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Regression Task Principal Component Breakdown\n",
    "pca_regression.fit(myData_scaled_regression)\n",
    "var = pca_regression.explained_variance_\n",
    "var1 = pca_regression.explained_variance_ratio_\n",
    "var2 = np.cumsum(np.round(pca_regression.explained_variance_ratio_, decimals=4))\n",
    "idx = ['PC'+str(i) for i in range(1,16)]\n",
    "\n",
    "PCA_df = {'...Eigenvector...' : pd.Series(var, index = idx),\n",
    "          '..Variance Proportion..' : pd.Series(var1, index = idx),\n",
    "          '.Variance Cumulative Proportion.' : pd.Series(var2, index = idx)}\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_df)\n",
    "print('Regression Task Principal Component Breakdown')\n",
    "display(PCA_df)\n",
    "\n",
    "del var, var1, var2, idx\n",
    "pca_regression = PCA(n_components=15, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our discussions revolving data preparation and principal component derivations address our current interests for variable understanding moving forward into modeling and evaluation, there is still more to be understood regarding principal component loadings. Loadings will be described as we discuss the most important attributes for our tasks later in the Modeling and Evaluation 6 section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset data set to only variables identified to have the greatest PCA loadings\n",
    "CitiBike_clus = CitiBikeDataSampled_5050[['start_station_latitude',\n",
    "                                          'start_station_longitude',\n",
    "                                          'end_station_latitude',\n",
    "                                          'end_station_longitude',\n",
    "                                          'PRCP',\n",
    "                                          'SNOW',\n",
    "                                          'TAVE',\n",
    "                                          'TMAX',\n",
    "                                          'TMIN',\n",
    "                                          'DayOfWeek_Friday',\n",
    "                                          'DayOfWeek_Monday',\n",
    "                                          'DayOfWeek_Saturday',\n",
    "                                          'DayOfWeek_Sunday',\n",
    "                                          'DayOfWeek_Thursday',\n",
    "                                          'DayOfWeek_Tuesday',\n",
    "                                          'DayOfWeek_Wednesday',\n",
    "                                          'TimeOfDay_Afternoon',\n",
    "                                          'TimeOfDay_Evening',\n",
    "                                          'TimeOfDay_Midday',\n",
    "                                          'TimeOfDay_Morning',\n",
    "                                          'TimeOfDay_Night',\n",
    "                                          'tripdurationLog']]\n",
    "display(CitiBike_clus.head().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 1 - Train and adjust parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN\n",
    "\n",
    "Another measure of interest within our CitiBike data set is that of efficient bike station zoning for marketing purposes. By better understanding the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CitiBike_dbscan = CitiBike_clus[['start_station_latitude','start_station_longitude']].drop_duplicates()\n",
    "CitiBike_dbscan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(CitiBike_dbscan.start_station_longitude, CitiBike_dbscan.start_station_latitude, 'bo', markersize=3) #plot the data\n",
    "plt.title('Latitude/Longitude Data'.format(2))\n",
    "plt.xlabel('Longitude Coordinate'.format(2))\n",
    "plt.ylabel('Latitude Coordinate'.format(2))\n",
    "plt.xlim(-74.15,-73.8)\n",
    "plt.ylim(40.67,40.78)\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.grid()\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 5\n",
    "CitiBike_knn_graph = kneighbors_graph(CitiBike_dbscan, n_neighbors = N, mode='distance') # calculate distance to nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N1 = CitiBike_knn_graph.shape[0]\n",
    "CitiBike_distances = np.zeros((N1,1))\n",
    "for i in range(N1):\n",
    "    CitiBike_distances[i] = CitiBike_knn_graph[i,:].max()\n",
    "\n",
    "CitiBike_distances = np.sort(CitiBike_distances, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "#plt.subplot(1,2,1)\n",
    "plt.plot(range(N1), CitiBike_distances, 'r.', markersize=2) #plot the data\n",
    "plt.title('Dataset name: CitiBike_clus, sorted by neighbor distance')\n",
    "plt.xlabel('CitiBike_clus, Instance Number')\n",
    "plt.ylabel('CitiBike_clus, Distance to {0}th nearest neighbor'.format(N))\n",
    "#plt.xlim([400000,500000])\n",
    "plt.annotate('Expected Eps value = approx. 0.0054', xy=(287, 0.0054), xytext=(200, 0.007),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.plot([0, 350], [0.0054, 0.0054], 'k--', lw=0.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#eps = 0.0054\n",
    "eps = 0.2\n",
    "minpts = 5\n",
    "\n",
    "#db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_dbscan)\n",
    "db = DBSCAN(eps=eps, min_samples=minpts).fit(CitiBike_clus)\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# mark the samples that are considered \"core\"\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "unique_labels = set(labels) # the unique labels\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = CitiBike_dbscan[class_member_mask & core_samples_mask]\n",
    "    # plot the core points in this class\n",
    "    plt.plot(xy.start_station_longitude, xy.start_station_latitude, 'o', markerfacecolor=col,\n",
    "             markeredgecolor='w', markersize=6)\n",
    "\n",
    "    # plot the remaining points that are edge points\n",
    "    xy = CitiBike_dbscan[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy.start_station_longitude, xy.start_station_latitude, 'o', markerfacecolor=col,\n",
    "             markeredgecolor='w', markersize=3)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.xlim(-74.15,-73.8)\n",
    "plt.ylim(40.67,40.78)\n",
    "plt.grid()\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#This is for backup purposes only\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=22, covariance_type='full')\n",
    "\n",
    "clf.fit(CitiBike_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%R -i CitiBike_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R print(head(CitiBike_clus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = %R capture.output(str(CitiBike_clus))\n",
    "\n",
    "for line in test:\n",
    "         print(line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Monday)))[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "CitiBike_clus$DayOfWeek_Friday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Friday)))\n",
    "CitiBike_clus$DayOfWeek_Monday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Monday)))\n",
    "CitiBike_clus$DayOfWeek_Saturday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Saturday)))\n",
    "CitiBike_clus$DayOfWeek_Sunday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Sunday)))\n",
    "CitiBike_clus$DayOfWeek_Thursday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Thursday)))\n",
    "CitiBike_clus$DayOfWeek_Tuesday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Tuesday)))\n",
    "CitiBike_clus$DayOfWeek_Wednesday <- as.logical(as.integer(as.character(CitiBike_clus$DayOfWeek_Wednesday)))\n",
    "CitiBike_clus$TimeOfDay_Afternoon <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Afternoon)))\n",
    "CitiBike_clus$TimeOfDay_Evening <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Evening)))\n",
    "CitiBike_clus$TimeOfDay_Midday <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Midday)))\n",
    "CitiBike_clus$TimeOfDay_Morning <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Morning)))\n",
    "CitiBike_clus$TimeOfDay_Night <- as.logical(as.integer(as.character(CitiBike_clus$TimeOfDay_Night)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%R install.packages(\"kernlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R nrow(CitiBike_clus[1:500,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "CitiBike_mini <- CitiBike_clus[sample(1:nrow(CitiBike_clus), 500, replace=FALSE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "s <- function(x1, x2, alpha=1) {\n",
    "  exp(- alpha * norm(as.matrix(x1-x2), type=\"F\"))\n",
    "}\n",
    "\n",
    "make.similarity <- function(my.data, similarity) {\n",
    "  N <- nrow(my.data)\n",
    "  S <- matrix(rep(NA,N^2), ncol=N)\n",
    "  for(i in 1:N) {\n",
    "    for(j in 1:N) {\n",
    "      S[i,j] <- similarity(my.data[i,], my.data[j,])\n",
    "    }\n",
    "  }\n",
    "  S\n",
    "}\n",
    "\n",
    "S <- make.similarity(CitiBike_mini, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "make.affinity <- function(S, n.neighboors=2) {\n",
    "  N <- length(S[,1])\n",
    "\n",
    "  if (n.neighboors >= N) {  # fully connected\n",
    "    A <- S\n",
    "  } else {\n",
    "    A <- matrix(rep(0,N^2), ncol=N)\n",
    "    for(i in 1:N) { # for each line\n",
    "      # only connect to those points with larger similarity \n",
    "      best.similarities <- sort(S[i,], decreasing=TRUE)[1:n.neighboors]\n",
    "      for (s in best.similarities) {\n",
    "        j <- which(S[i,] == s)\n",
    "        A[i,j] <- S[i,j]\n",
    "        A[j,i] <- S[i,j] # to make an undirected graph, ie, the matrix becomes symmetric\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  A  \n",
    "}\n",
    "\n",
    "A <- make.affinity(S, 3)  # use 3 neighboors (includes self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R D <- diag(apply(A, 1, sum)) # sum rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R dim(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "\"%^%\" <- function(M, power)\n",
    "  with(eigen(M), vectors %*% (values^power * solve(vectors)))\n",
    "    \n",
    "L <- (D %^% (-1/2)) %*% A %*% (D %^% (-1/2))  # normalized Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "k   <- 2\n",
    "evL <- eigen(L, symmetric=TRUE)\n",
    "Z   <- evL$vectors[,(ncol(evL$vectors)-k+1):ncol(evL$vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R signif(evL$values,2) # eigenvalues are in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "plot(1:10, rev(evL$values)[1:10])\n",
    "abline(v=2.5, col=\"red\", lty=2) # there are just 2 clusters as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%R set.seed(200)\n",
    "#%R mCiti <- data.matrix(CitiBike_mini[,5:22])\n",
    "%R mCiti <- data.matrix(CitiBike_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%R colnames(mCiti) <- colnames(CitiBike_clus[5:22])\n",
    "%R colnames(mCiti) <- colnames(CitiBike_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "wss[1] <- ((length(mCiti)/length(colnames(mCiti)))-1)*sum(apply(mCiti,2,var))\n",
    "for (i in 2:22) wss[i] <- sum(withinss(specc(mCiti, centers = i)))\n",
    "    \n",
    "plot(1:22, wss, type=\"b\", xlab=\"Number of Clusters\", ylab=\"Within groups sum of squares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "library(kernlab)\n",
    "\n",
    "sc <- specc(mCiti, centers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%R plot(mCiti, col=sc, pch=19)            # estimated classes (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R length(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = %R capture.output(sc)\n",
    "\n",
    "for line in test:\n",
    "         print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 2 - Evaluate and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 3 - Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 4 - Summarize the Ramifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CLUSTERING CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 1 - Choose and explain the evaluation metrics that will be used\n",
    "\n",
    "##### Classification\n",
    "\n",
    "With our goal being to properly classify users between customers and subscribers, we have a unique opportunity to explore false positives. As mentioned previously, false positives in classifying renters as subscribers gives us a subset of possible targets of conversion from customer to subscriber. In this context, when focusing on accuracy well primarily use confusion matrices to explore our results alongside plotted ROC curves.\n",
    "\n",
    "The matrices will illustrate our subset of false positives and give us an idea of the overall accuracy of the model. While normally accuracy is primarily focused on increasing true positives, with our interest in false positives, well judge our models based on the reduction of false negatives. Well make these judgements based on log loss as calculated along the ROC curve plotted out after each regression. While its true that log loss penalizes all false classifications, its easier to evaluate a model based on it rather than a simple quantification of true positives.\n",
    "\n",
    "In interpreting our ROC curves, sensitivity will be our primary element by which we evaluate our model, rather than specificity, as, like previously mentioned, we dont want to lose out on false negatives as our use depends on false positives. That said, specificity will not be ignored as true negatives ultimate help us build up our confidence in exploring those false positives later on.\n",
    "\n",
    "##### Linear Regression\n",
    "\n",
    "In evaluating our model for predicting trip duration, well rely on the usual measures of Mean Squared Error and the adjusted R squared of the model. In comparison to our classification model, we want this model to be as accurate as possible, with little variance between our predicted durations and the real durations based on our test data. While well want the model with the highest adjusted R squared, well place the emphasis in our evaluation on the MSE as we performed a log transform on the linear distance reducing the amount of variance between each observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 2 - Build Test & Train Datasets\n",
    "\n",
    "#### Classification\n",
    "With our final encoded dataset complete, and PCA analysis performed to identify our principal components we begin splitting the data into Test vs Train datasets. We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 10 folds. This means, that from our original sample size of 500,000, each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst customers and subscribers. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Create CV Object for StratifiedKFold with 10 Folds, seeded at the length of our sample size\n",
    "seed = len(CitiBikeDataSampled_5050)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "Alternatively, we have chosen to utilize standard KFold Cross Validation for our regresssion analysis, with 10 folds. This is because our regression target is a continuous attribute, and we have already done stratification of the original dataset in general. We have chosen to shuffle the data before splitting into batches to prevent any potential issues with the order of our dataset. Once again, this means, that from our original sample size of 500,000, data will be randomly shuffled then each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We continue to utilize the previously established random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Create CV Object for KFold with 10 Folds, seeded at the length of our sample size\n",
    "\n",
    "cvReg = KFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "print(cvReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 3 - Build/Tune Three Classification Models (usertype predictions)\n",
    "Now that our classification train/test cross-validation splits are configured, it is time to generate our classifiers. We have chosen to utilize the following three classification methods for our Subscriber/Customer prediction task:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. K Nearest Neighbors (KNN)\n",
    "\n",
    "We chose to create a logistic regression model it performed better than SVM during our previous mini-lab. Random Forest was also selected because we expected it to produce the most accurate model among all three of our classifiers. Finally, KNN was selected as our third classifier type since it functions rather differently from the logistic regression and random forest in that it computes distance between records, identifying k nearest neighbors and using the class labels of nearest neighbors to identify unknown record labels. All three classification methods take very different approaches to the same task and should make for some interesting comparisons.\n",
    "\n",
    "*It is important to note that while we have left our function definitions uncommented for each model type, we have commented out the actual model generation code blocks due to their long run-times. Alternatively, we ran the code before commenting and then embedded the rendered HTML outputs in the cell block following the code for interpretation. This allows us to provide the model code outputs while significantly reducing project development times.*\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Our first classifier model built is the logistic regression model. As we would like to test the effects of parameter adjustment on model accuracies for each classification method, we have primarily one parameter in our logistic regression fit that appears appropriate to manipulate for accuracies. We have chosen to manipulate the cost variable (*C*) within our logistic regression analyzing accuracies at {1.0, .01, .05, 5}. This parameter is essentially an inverted regularization strength equal to 1/lambda per scikit-learn class function code (lambda being the actual regularization item). Therefore, the smaller the cost value the stronger the regularization. Calculations show that our best value for cost = 0.01 (mean accuracy across 10 iterations = 0.681886), and that accuracy values were very close and even shifted order between most to least accurate from iteration to iteration. While it may not intuitively make sense up front that the strongest regularization value is best given that a cost value of 1 was best during our mini-lab, it is important to remember that we are now using principal components in our model; the mini-lab used the original, unscaled, CitiBike attributes. Though the change in accuracy is minimal, and maybe even negligible, it appears our principal components inputs benefit slightly from added regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def lr_explor(cost,  \n",
    "              ScaledData,\n",
    "              PCA         = pca_classification,\n",
    "              Data        = CitiBikeDataSampled_5050,\n",
    "              cv          = cv,\n",
    "              seed        = seed):\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None, random_state=seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',lr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#cost    = [.01, .05, 1.0, 5.0]\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(cost)):\n",
    "#    acclist.append(lr_explor(cost       = cost[i],\n",
    "#                             ScaledData = myData_scaled_classification))\n",
    "#\n",
    "#LRcostdf = pd.DataFrame(pd.concat([pd.DataFrame(cost),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#LRcostdf.columns = ['Cost','Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(LRcostdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Cost</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0.01</td>\n",
    "      <td>0.68128</td>\n",
    "      <td>0.68198</td>\n",
    "      <td>0.68054</td>\n",
    "      <td>0.68190</td>\n",
    "      <td>0.68584</td>\n",
    "      <td>0.67858</td>\n",
    "      <td>0.68522</td>\n",
    "      <td>0.67878</td>\n",
    "      <td>0.68100</td>\n",
    "      <td>0.68374</td>\n",
    "      <td>0.681886</td>\n",
    "      <td>00:00:55.301026</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>0.05</td>\n",
    "      <td>0.68122</td>\n",
    "      <td>0.68212</td>\n",
    "      <td>0.68052</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68588</td>\n",
    "      <td>0.67842</td>\n",
    "      <td>0.68508</td>\n",
    "      <td>0.67870</td>\n",
    "      <td>0.68080</td>\n",
    "      <td>0.68372</td>\n",
    "      <td>0.681854</td>\n",
    "      <td>00:00:54.031372</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1.00</td>\n",
    "      <td>0.68136</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68050</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68586</td>\n",
    "      <td>0.67848</td>\n",
    "      <td>0.68504</td>\n",
    "      <td>0.67882</td>\n",
    "      <td>0.68080</td>\n",
    "      <td>0.68374</td>\n",
    "      <td>0.681876</td>\n",
    "      <td>00:00:53.148202</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>5.00</td>\n",
    "      <td>0.68138</td>\n",
    "      <td>0.68202</td>\n",
    "      <td>0.68048</td>\n",
    "      <td>0.68210</td>\n",
    "      <td>0.68586</td>\n",
    "      <td>0.67846</td>\n",
    "      <td>0.68508</td>\n",
    "      <td>0.67876</td>\n",
    "      <td>0.68082</td>\n",
    "      <td>0.68376</td>\n",
    "      <td>0.681872</td>\n",
    "      <td>00:00:52.870310</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**n_estimators**\n",
    "Number of Trees generated in the forest. Increasing the number of trees, in our models increased accuracy while decreasing performance. We tuned to provide output that completed all 10 iterations in under 10 minutes.\n",
    "\n",
    "After 13 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 70.2668 % from default inputs of the random forest classification model to a value of 72.5192 % in the best tuned model fit. Although the run-time of this model parameter choice is the largest performed, we decided to remain with these inputs due to the amount increase in accuracy. As mentioned previously, we tuned the n_estimators parameter to ensure we stayed under 10 minutes execution. Parameter inputs for the final Random Forest Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>n_estimators</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>1000.0</th>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def rfc_explor(ScaledData,\n",
    "               n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf, \n",
    "               PCA         = pca_classification,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "#max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "#max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "#min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "#min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "#\n",
    "#for i in range(0,len(n_estimators)):\n",
    "#    acclist.append(rfc_explor(ScaledData        = myData_scaled_classification,\n",
    "#                              n_estimators      = n_estimators[i],\n",
    "#                              max_features      = max_features[i],\n",
    "#                              max_depth         = max_depth[i],\n",
    "#                              min_samples_split = min_samples_split[i],\n",
    "#                              min_samples_leaf  = min_samples_leaf[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_estimators\": n_estimators,          \n",
    "#                                                \"max_features\": max_features,         \n",
    "#                                                \"max_depth\": max_depth,        \n",
    "#                                                \"min_samples_split\": min_samples_split,\n",
    "#                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table border=\\1\\ class=\\dataframe\\>\n",
    " <thead>\n",
    "   <tr style=\\text-align: right;\\>\n",
    "     <th></th>\n",
    "     <th>max_depth</th>\n",
    "     <th>max_features</th>\n",
    "     <th>min_samples_leaf</th>\n",
    "     <th>min_samples_split</th>\n",
    "     <th>n_estimators</th>\n",
    "     <th>Iteration 0</th>\n",
    "     <th>Iteration 1</th>\n",
    "     <th>Iteration 2</th>\n",
    "     <th>Iteration 3</th>\n",
    "     <th>Iteration 4</th>\n",
    "     <th>Iteration 5</th>\n",
    "     <th>Iteration 6</th>\n",
    "     <th>Iteration 7</th>\n",
    "     <th>Iteration 8</th>\n",
    "     <th>Iteration 9</th>\n",
    "     <th>MeanAccuracy</th>\n",
    "     <th>RunTime</th>\n",
    "   </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "   <tr>\n",
    "     <th>0</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>1</td>\n",
    "     <td>2</td>\n",
    "     <td>10</td>\n",
    "     <td>0.70206</td>\n",
    "     <td>0.70432</td>\n",
    "     <td>0.70548</td>\n",
    "     <td>0.70046</td>\n",
    "     <td>0.70300</td>\n",
    "     <td>0.69678</td>\n",
    "     <td>0.70606</td>\n",
    "     <td>0.70146</td>\n",
    "     <td>0.70256</td>\n",
    "     <td>0.70450</td>\n",
    "     <td>0.702668</td>\n",
    "     <td>00:02:22.858960</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>1</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>4</td>\n",
    "     <td>8</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71310</td>\n",
    "     <td>0.71480</td>\n",
    "     <td>0.71486</td>\n",
    "     <td>0.71398</td>\n",
    "     <td>0.71414</td>\n",
    "     <td>0.70878</td>\n",
    "     <td>0.71756</td>\n",
    "     <td>0.71150</td>\n",
    "     <td>0.71694</td>\n",
    "     <td>0.71564</td>\n",
    "     <td>0.714130</td>\n",
    "     <td>00:02:19.138254</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>2</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>6</td>\n",
    "     <td>12</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71576</td>\n",
    "     <td>0.71660</td>\n",
    "     <td>0.72088</td>\n",
    "     <td>0.71724</td>\n",
    "     <td>0.71766</td>\n",
    "     <td>0.71410</td>\n",
    "     <td>0.71964</td>\n",
    "     <td>0.71730</td>\n",
    "     <td>0.71990</td>\n",
    "     <td>0.71808</td>\n",
    "     <td>0.717716</td>\n",
    "     <td>00:02:18.481832</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>3</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>8</td>\n",
    "     <td>16</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72052</td>\n",
    "     <td>0.72110</td>\n",
    "     <td>0.72216</td>\n",
    "     <td>0.71840</td>\n",
    "     <td>0.71940</td>\n",
    "     <td>0.71478</td>\n",
    "     <td>0.72130</td>\n",
    "     <td>0.71560</td>\n",
    "     <td>0.71952</td>\n",
    "     <td>0.71852</td>\n",
    "     <td>0.719130</td>\n",
    "     <td>00:02:17.359507</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>4</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>10</td>\n",
    "     <td>20</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71878</td>\n",
    "     <td>0.72088</td>\n",
    "     <td>0.72286</td>\n",
    "     <td>0.71678</td>\n",
    "     <td>0.72022</td>\n",
    "     <td>0.71692</td>\n",
    "     <td>0.72426</td>\n",
    "     <td>0.71810</td>\n",
    "     <td>0.72134</td>\n",
    "     <td>0.72066</td>\n",
    "     <td>0.720080</td>\n",
    "     <td>00:02:15.995898</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>5</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72098</td>\n",
    "     <td>0.72240</td>\n",
    "     <td>0.72164</td>\n",
    "     <td>0.71710</td>\n",
    "     <td>0.72258</td>\n",
    "     <td>0.71856</td>\n",
    "     <td>0.72334</td>\n",
    "     <td>0.71908</td>\n",
    "     <td>0.72254</td>\n",
    "     <td>0.72320</td>\n",
    "     <td>0.721142</td>\n",
    "     <td>00:02:11.538186</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>6</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>40</td>\n",
    "     <td>80</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71874</td>\n",
    "     <td>0.71902</td>\n",
    "     <td>0.72102</td>\n",
    "     <td>0.71716</td>\n",
    "     <td>0.72050</td>\n",
    "     <td>0.71686</td>\n",
    "     <td>0.72410</td>\n",
    "     <td>0.71760</td>\n",
    "     <td>0.72158</td>\n",
    "     <td>0.72104</td>\n",
    "     <td>0.719762</td>\n",
    "     <td>00:02:06.851562</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>7</th>\n",
    "     <td>NaN</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72230</td>\n",
    "     <td>0.72428</td>\n",
    "     <td>0.72382</td>\n",
    "     <td>0.71876</td>\n",
    "     <td>0.72684</td>\n",
    "     <td>0.71958</td>\n",
    "     <td>0.72524</td>\n",
    "     <td>0.72160</td>\n",
    "     <td>0.72436</td>\n",
    "     <td>0.72328</td>\n",
    "     <td>0.723006</td>\n",
    "     <td>00:06:37.091719</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>8</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72260</td>\n",
    "     <td>0.72488</td>\n",
    "     <td>0.72530</td>\n",
    "     <td>0.72070</td>\n",
    "     <td>0.72446</td>\n",
    "     <td>0.71846</td>\n",
    "     <td>0.72462</td>\n",
    "     <td>0.72124</td>\n",
    "     <td>0.72546</td>\n",
    "     <td>0.72346</td>\n",
    "     <td>0.723118</td>\n",
    "     <td>00:06:33.179886</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>9</th>\n",
    "     <td>500.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72228</td>\n",
    "     <td>0.72430</td>\n",
    "     <td>0.72430</td>\n",
    "     <td>0.71970</td>\n",
    "     <td>0.72520</td>\n",
    "     <td>0.71920</td>\n",
    "     <td>0.72702</td>\n",
    "     <td>0.72076</td>\n",
    "     <td>0.72326</td>\n",
    "     <td>0.72404</td>\n",
    "     <td>0.723006</td>\n",
    "     <td>00:06:35.014423</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>10</th>\n",
    "     <td>100.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72234</td>\n",
    "     <td>0.72360</td>\n",
    "     <td>0.72392</td>\n",
    "     <td>0.71976</td>\n",
    "     <td>0.72480</td>\n",
    "     <td>0.71958</td>\n",
    "     <td>0.72520</td>\n",
    "     <td>0.72080</td>\n",
    "     <td>0.72534</td>\n",
    "     <td>0.72464</td>\n",
    "     <td>0.722998</td>\n",
    "     <td>00:06:33.892724</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>11</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>5</td>\n",
    "     <td>0.71642</td>\n",
    "     <td>0.71852</td>\n",
    "     <td>0.71884</td>\n",
    "     <td>0.71458</td>\n",
    "     <td>0.71964</td>\n",
    "     <td>0.71342</td>\n",
    "     <td>0.71864</td>\n",
    "     <td>0.71534</td>\n",
    "     <td>0.71868</td>\n",
    "     <td>0.71980</td>\n",
    "     <td>0.717388</td>\n",
    "     <td>00:04:21.575723</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>12</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>15</td>\n",
    "     <td>0.72432</td>\n",
    "     <td>0.72640</td>\n",
    "     <td>0.72618</td>\n",
    "     <td>0.72126</td>\n",
    "     <td>0.72712</td>\n",
    "     <td>0.72128</td>\n",
    "     <td>0.72728</td>\n",
    "     <td>0.72448</td>\n",
    "     <td>0.72708</td>\n",
    "     <td>0.72652</td>\n",
    "     <td>0.725192</td>\n",
    "     <td>00:08:59.111516</td>\n",
    "   </tr>\n",
    " </tbody>\n",
    "</table>\n",
    "</div>\n",
    "Wall time: 55min 32s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Options include \"Ball Tree\" and \"KD Tree\". \n",
    "* Ball Trees are binary trees formed from nodes of multidimensional hyperspheres, or \"balls\". Node hyperspheres may intersect, but each point is assigned to one according to distance from the hypersphere center. \n",
    "* KD Trees are binary trees formed from nodes of multidimensional hyperplanes. Every node in the tree is associated with one of the dimensions, with the hyperplane perpendicular to that dimension's axis.\n",
    "\n",
    "Our findings, were that the Ball Tree algorithm was considerably less efficient to produce results for all 10 iterations in comparison to the KD Tree Algorithm.\n",
    "\n",
    "**Leaf Size**\n",
    "\n",
    "The size for leaf nodes in the KNN Tree.\n",
    "\n",
    "**Number of Neighbors**\n",
    "\n",
    "After 24 iterations of modifying the above parameters, we land on a final winner based on the highest average Accuracy value across all iterations. Average Accuracy values in our 10 test/train iterations ranged from 66.5216 % from the worst parameter inputs of the Ball_Tree Algorith to a value of 69.5528 % in best tuned KNN Classification model fit. We have chosen to utilize the best input for KD tree, although losing an improvement of .0004 % due to the cost(slower runtime of 07 Minutes 25 Seconds through 10 iterations) of fitting the model as Ball Tree. Parameter inputs for the final K Nearest Neighbor Classification model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>kd_tree</th>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def knn_explor(ScaledData,\n",
    "               n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               PCA         = pca_classification,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',knn_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  ['ball_tree'     , 'ball_tree'     , 'ball_tree'     , 'ball_tree'     , 'ball_tree'       , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      ] \n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knn_explor(ScaledData  = myData_scaled_classification,\n",
    "#                              n_neighbors = n_neighbors[i],\n",
    "#                              algorithm   = algorithm[i],\n",
    "#                              leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  ['kd_tree'     , 'kd_tree'     , 'kd_tree'     , 'kd_tree'     , 'kd_tree'       , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      ] \n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knn_explor(ScaledData  = myData_scaled_classification,\n",
    "#                              n_neighbors = n_neighbors[i],\n",
    "#                              algorithm   = algorithm[i],\n",
    "#                              leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66732</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66590</td>\n",
    "      <td>0.66382</td>\n",
    "      <td>0.66776</td>\n",
    "      <td>0.66150</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66194</td>\n",
    "      <td>0.66566</td>\n",
    "      <td>0.66724</td>\n",
    "      <td>0.665266</td>\n",
    "      <td>00:06:53.743385</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66722</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66406</td>\n",
    "      <td>0.66784</td>\n",
    "      <td>0.66154</td>\n",
    "      <td>0.66562</td>\n",
    "      <td>0.66188</td>\n",
    "      <td>0.66534</td>\n",
    "      <td>0.66700</td>\n",
    "      <td>0.665216</td>\n",
    "      <td>00:05:40.484082</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66716</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66606</td>\n",
    "      <td>0.66422</td>\n",
    "      <td>0.66790</td>\n",
    "      <td>0.66162</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66182</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.665328</td>\n",
    "      <td>00:06:54.453049</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66748</td>\n",
    "      <td>0.66598</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66386</td>\n",
    "      <td>0.66786</td>\n",
    "      <td>0.66178</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66192</td>\n",
    "      <td>0.66540</td>\n",
    "      <td>0.66708</td>\n",
    "      <td>0.665288</td>\n",
    "      <td>00:09:07.276291</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.66592</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66398</td>\n",
    "      <td>0.66806</td>\n",
    "      <td>0.66152</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66190</td>\n",
    "      <td>0.66542</td>\n",
    "      <td>0.66698</td>\n",
    "      <td>0.665258</td>\n",
    "      <td>00:06:54.266485</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.68034</td>\n",
    "      <td>0.67936</td>\n",
    "      <td>0.67816</td>\n",
    "      <td>0.67758</td>\n",
    "      <td>0.68036</td>\n",
    "      <td>0.67388</td>\n",
    "      <td>0.67840</td>\n",
    "      <td>0.67480</td>\n",
    "      <td>0.68068</td>\n",
    "      <td>0.67866</td>\n",
    "      <td>0.678222</td>\n",
    "      <td>00:07:03.431483</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.68648</td>\n",
    "      <td>0.68584</td>\n",
    "      <td>0.68636</td>\n",
    "      <td>0.68364</td>\n",
    "      <td>0.68574</td>\n",
    "      <td>0.68160</td>\n",
    "      <td>0.68682</td>\n",
    "      <td>0.68154</td>\n",
    "      <td>0.68738</td>\n",
    "      <td>0.68630</td>\n",
    "      <td>0.685170</td>\n",
    "      <td>00:07:14.175179</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.68922</td>\n",
    "      <td>0.68816</td>\n",
    "      <td>0.68828</td>\n",
    "      <td>0.68702</td>\n",
    "      <td>0.68972</td>\n",
    "      <td>0.68486</td>\n",
    "      <td>0.69022</td>\n",
    "      <td>0.68466</td>\n",
    "      <td>0.69014</td>\n",
    "      <td>0.68880</td>\n",
    "      <td>0.688108</td>\n",
    "      <td>00:07:45.081930</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.69556</td>\n",
    "      <td>0.69452</td>\n",
    "      <td>0.69446</td>\n",
    "      <td>0.69290</td>\n",
    "      <td>0.69572</td>\n",
    "      <td>0.69068</td>\n",
    "      <td>0.69476</td>\n",
    "      <td>0.69210</td>\n",
    "      <td>0.69590</td>\n",
    "      <td>0.69468</td>\n",
    "      <td>0.694128</td>\n",
    "      <td>00:08:10.106731</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.69504</td>\n",
    "      <td>0.69628</td>\n",
    "      <td>0.69582</td>\n",
    "      <td>0.69370</td>\n",
    "      <td>0.69856</td>\n",
    "      <td>0.69170</td>\n",
    "      <td>0.69632</td>\n",
    "      <td>0.69318</td>\n",
    "      <td>0.69630</td>\n",
    "      <td>0.69670</td>\n",
    "      <td>0.695360</td>\n",
    "      <td>00:08:54.870863</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.69618</td>\n",
    "      <td>0.69692</td>\n",
    "      <td>0.69596</td>\n",
    "      <td>0.69322</td>\n",
    "      <td>0.69690</td>\n",
    "      <td>0.69150</td>\n",
    "      <td>0.69764</td>\n",
    "      <td>0.69344</td>\n",
    "      <td>0.69672</td>\n",
    "      <td>0.69680</td>\n",
    "      <td>0.695528</td>\n",
    "      <td>00:09:27.100008</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.69592</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.69374</td>\n",
    "      <td>0.69716</td>\n",
    "      <td>0.69184</td>\n",
    "      <td>0.69722</td>\n",
    "      <td>0.69380</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.69588</td>\n",
    "      <td>0.695366</td>\n",
    "      <td>00:09:52.138668</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "Wall time: 1h 33min 57s\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66726</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66568</td>\n",
    "      <td>0.66414</td>\n",
    "      <td>0.66804</td>\n",
    "      <td>0.66150</td>\n",
    "      <td>0.66570</td>\n",
    "      <td>0.66168</td>\n",
    "      <td>0.66564</td>\n",
    "      <td>0.66714</td>\n",
    "      <td>0.665266</td>\n",
    "      <td>00:01:13.676624</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66732</td>\n",
    "      <td>0.66598</td>\n",
    "      <td>0.66574</td>\n",
    "      <td>0.66376</td>\n",
    "      <td>0.66764</td>\n",
    "      <td>0.66182</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66194</td>\n",
    "      <td>0.66548</td>\n",
    "      <td>0.66706</td>\n",
    "      <td>0.665246</td>\n",
    "      <td>00:01:10.890032</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66726</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66590</td>\n",
    "      <td>0.66402</td>\n",
    "      <td>0.66802</td>\n",
    "      <td>0.66158</td>\n",
    "      <td>0.66562</td>\n",
    "      <td>0.66180</td>\n",
    "      <td>0.66518</td>\n",
    "      <td>0.66734</td>\n",
    "      <td>0.665248</td>\n",
    "      <td>00:01:12.039968</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66738</td>\n",
    "      <td>0.66600</td>\n",
    "      <td>0.66600</td>\n",
    "      <td>0.66366</td>\n",
    "      <td>0.66794</td>\n",
    "      <td>0.66162</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66160</td>\n",
    "      <td>0.66542</td>\n",
    "      <td>0.66720</td>\n",
    "      <td>0.665268</td>\n",
    "      <td>00:01:17.640469</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66394</td>\n",
    "      <td>0.66802</td>\n",
    "      <td>0.66152</td>\n",
    "      <td>0.66578</td>\n",
    "      <td>0.66184</td>\n",
    "      <td>0.66560</td>\n",
    "      <td>0.66700</td>\n",
    "      <td>0.665256</td>\n",
    "      <td>00:01:17.354629</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.68052</td>\n",
    "      <td>0.67930</td>\n",
    "      <td>0.67830</td>\n",
    "      <td>0.67748</td>\n",
    "      <td>0.68034</td>\n",
    "      <td>0.67394</td>\n",
    "      <td>0.67810</td>\n",
    "      <td>0.67488</td>\n",
    "      <td>0.68094</td>\n",
    "      <td>0.67860</td>\n",
    "      <td>0.678240</td>\n",
    "      <td>00:01:19.506095</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.68658</td>\n",
    "      <td>0.68566</td>\n",
    "      <td>0.68624</td>\n",
    "      <td>0.68360</td>\n",
    "      <td>0.68588</td>\n",
    "      <td>0.68146</td>\n",
    "      <td>0.68650</td>\n",
    "      <td>0.68156</td>\n",
    "      <td>0.68738</td>\n",
    "      <td>0.68594</td>\n",
    "      <td>0.685080</td>\n",
    "      <td>00:01:23.414543</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.68918</td>\n",
    "      <td>0.68810</td>\n",
    "      <td>0.68830</td>\n",
    "      <td>0.68724</td>\n",
    "      <td>0.68992</td>\n",
    "      <td>0.68492</td>\n",
    "      <td>0.69012</td>\n",
    "      <td>0.68480</td>\n",
    "      <td>0.69018</td>\n",
    "      <td>0.68854</td>\n",
    "      <td>0.688130</td>\n",
    "      <td>00:01:22.808268</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.69550</td>\n",
    "      <td>0.69440</td>\n",
    "      <td>0.69404</td>\n",
    "      <td>0.69292</td>\n",
    "      <td>0.69560</td>\n",
    "      <td>0.69072</td>\n",
    "      <td>0.69500</td>\n",
    "      <td>0.69222</td>\n",
    "      <td>0.69570</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.694064</td>\n",
    "      <td>00:01:34.127069</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.69496</td>\n",
    "      <td>0.69624</td>\n",
    "      <td>0.69570</td>\n",
    "      <td>0.69372</td>\n",
    "      <td>0.69846</td>\n",
    "      <td>0.69174</td>\n",
    "      <td>0.69630</td>\n",
    "      <td>0.69314</td>\n",
    "      <td>0.69638</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.695342</td>\n",
    "      <td>00:01:49.578879</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.69610</td>\n",
    "      <td>0.69696</td>\n",
    "      <td>0.69588</td>\n",
    "      <td>0.69322</td>\n",
    "      <td>0.69702</td>\n",
    "      <td>0.69148</td>\n",
    "      <td>0.69770</td>\n",
    "      <td>0.69356</td>\n",
    "      <td>0.69670</td>\n",
    "      <td>0.69662</td>\n",
    "      <td>0.695524</td>\n",
    "      <td>00:02:02.480648</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.69592</td>\n",
    "      <td>0.69694</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.69374</td>\n",
    "      <td>0.69726</td>\n",
    "      <td>0.69158</td>\n",
    "      <td>0.69724</td>\n",
    "      <td>0.69384</td>\n",
    "      <td>0.69672</td>\n",
    "      <td>0.69586</td>\n",
    "      <td>0.695364</td>\n",
    "      <td>00:02:11.007620</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "Wall time: 17min 54s\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build/Tune Three Regression Models (TripDurationLog predictions)\n",
    "#### KNN Regression\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Options include \"Ball Tree\" and \"KD Tree\". \n",
    "* Ball Trees are binary trees formed from nodes of multidimensional hyperspheres, or \"balls\". Node hyperspheres may intersect, but each point is assigned to one according to distance from the hypersphere center. \n",
    "* KD Trees are binary trees formed from nodes of multidimensional hyperplanes. Every node in the tree is associated with one of the dimensions, with the hyperplane perpendicular to that dimension's axis.\n",
    "\n",
    "Our findings, were that the Ball Tree algorithm was considerably less efficient to produce results for all 10 iterations in comparison to the KD Tree Algorithm.\n",
    "\n",
    "**Leaf Size**\n",
    "\n",
    "The size for leaf nodes in the KNN Tree.\n",
    "\n",
    "**Number of Neighbors**\n",
    "\n",
    "After 24 iterations of modifying the above parameters, we land on a final winner based on the lowest average MSE value across all iterations. Average MSE values in our 10 test/train iterations ranged from 0.557855 from the worst parameter inputs of the Ball_Tree Algorith to a value of 0.485734 in best tuned KNN Regression model fit. We have chosen to utilize the best input for KD tree, although losing an improvement of 0.000001 due to the cost(slower runtime of 7 Minutes 38 Seconds through 10 iterations) of fitting the model as Ball Tree. Parameter inputs for the final K Nearest Neighbor Regression model with the KD Tree Algorithm are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>kd_tree</th>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def knnr_explor(ScaledData,\n",
    "               n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               PCA         = pca_regression,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cvReg,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    knnr_clf = KNeighborsRegressor(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',knnr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, scoring = 'neg_mean_squared_error', cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    accuracy = accuracy * -1\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  'ball_tree'\n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knnr_explor(ScaledData  = myData_scaled_regression,\n",
    "#                               n_neighbors = n_neighbors[i],\n",
    "#                               algorithm   = algorithm,\n",
    "#                               leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#knnrdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "#display(knnrdf)\n",
    "#\n",
    "#\n",
    "#acclist = []\n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  'kd_tree'\n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knnr_explor(ScaledData  = myData_scaled_regression,\n",
    "#                               n_neighbors = n_neighbors[i],\n",
    "#                               algorithm   = algorithm,\n",
    "#                               leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#knnrdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "#display(knnrdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanSquaredError</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557237</td>\n",
    "      <td>0.562826</td>\n",
    "      <td>0.558308</td>\n",
    "      <td>0.554276</td>\n",
    "      <td>0.562881</td>\n",
    "      <td>0.558429</td>\n",
    "      <td>0.557172</td>\n",
    "      <td>0.553140</td>\n",
    "      <td>0.558521</td>\n",
    "      <td>0.555134</td>\n",
    "      <td>0.557792</td>\n",
    "      <td>00:08:11.337445</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557292</td>\n",
    "      <td>0.562653</td>\n",
    "      <td>0.558342</td>\n",
    "      <td>0.554373</td>\n",
    "      <td>0.562961</td>\n",
    "      <td>0.558587</td>\n",
    "      <td>0.557402</td>\n",
    "      <td>0.553222</td>\n",
    "      <td>0.558668</td>\n",
    "      <td>0.555053</td>\n",
    "      <td>0.557855</td>\n",
    "      <td>00:05:46.830013</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557146</td>\n",
    "      <td>0.562758</td>\n",
    "      <td>0.558269</td>\n",
    "      <td>0.554191</td>\n",
    "      <td>0.562888</td>\n",
    "      <td>0.558403</td>\n",
    "      <td>0.557345</td>\n",
    "      <td>0.553155</td>\n",
    "      <td>0.558692</td>\n",
    "      <td>0.555266</td>\n",
    "      <td>0.557811</td>\n",
    "      <td>00:07:25.224921</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557191</td>\n",
    "      <td>0.562601</td>\n",
    "      <td>0.558335</td>\n",
    "      <td>0.554403</td>\n",
    "      <td>0.562796</td>\n",
    "      <td>0.558394</td>\n",
    "      <td>0.557394</td>\n",
    "      <td>0.553283</td>\n",
    "      <td>0.558623</td>\n",
    "      <td>0.555201</td>\n",
    "      <td>0.557822</td>\n",
    "      <td>00:10:51.581113</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.556980</td>\n",
    "      <td>0.562671</td>\n",
    "      <td>0.558217</td>\n",
    "      <td>0.554300</td>\n",
    "      <td>0.562829</td>\n",
    "      <td>0.558462</td>\n",
    "      <td>0.557364</td>\n",
    "      <td>0.553113</td>\n",
    "      <td>0.558435</td>\n",
    "      <td>0.555253</td>\n",
    "      <td>0.557762</td>\n",
    "      <td>00:07:28.555236</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.516544</td>\n",
    "      <td>0.521360</td>\n",
    "      <td>0.514769</td>\n",
    "      <td>0.511683</td>\n",
    "      <td>0.518606</td>\n",
    "      <td>0.518443</td>\n",
    "      <td>0.514767</td>\n",
    "      <td>0.512905</td>\n",
    "      <td>0.516082</td>\n",
    "      <td>0.513862</td>\n",
    "      <td>0.515902</td>\n",
    "      <td>00:07:58.473706</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.502598</td>\n",
    "      <td>0.507682</td>\n",
    "      <td>0.500755</td>\n",
    "      <td>0.498222</td>\n",
    "      <td>0.507140</td>\n",
    "      <td>0.503541</td>\n",
    "      <td>0.500696</td>\n",
    "      <td>0.498643</td>\n",
    "      <td>0.503091</td>\n",
    "      <td>0.501013</td>\n",
    "      <td>0.502338</td>\n",
    "      <td>00:07:56.432365</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.495704</td>\n",
    "      <td>0.500741</td>\n",
    "      <td>0.492725</td>\n",
    "      <td>0.492472</td>\n",
    "      <td>0.501235</td>\n",
    "      <td>0.496499</td>\n",
    "      <td>0.494343</td>\n",
    "      <td>0.492267</td>\n",
    "      <td>0.496978</td>\n",
    "      <td>0.495034</td>\n",
    "      <td>0.495800</td>\n",
    "      <td>00:08:00.550927</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.486580</td>\n",
    "      <td>0.491213</td>\n",
    "      <td>0.482974</td>\n",
    "      <td>0.483827</td>\n",
    "      <td>0.491855</td>\n",
    "      <td>0.486606</td>\n",
    "      <td>0.485583</td>\n",
    "      <td>0.481404</td>\n",
    "      <td>0.487405</td>\n",
    "      <td>0.486372</td>\n",
    "      <td>0.486382</td>\n",
    "      <td>00:08:56.286637</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.486407</td>\n",
    "      <td>0.491350</td>\n",
    "      <td>0.481402</td>\n",
    "      <td>0.482939</td>\n",
    "      <td>0.491055</td>\n",
    "      <td>0.484986</td>\n",
    "      <td>0.484800</td>\n",
    "      <td>0.480595</td>\n",
    "      <td>0.487419</td>\n",
    "      <td>0.486387</td>\n",
    "      <td>0.485734</td>\n",
    "      <td>00:09:18.201664</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.487243</td>\n",
    "      <td>0.492394</td>\n",
    "      <td>0.481941</td>\n",
    "      <td>0.483494</td>\n",
    "      <td>0.491507</td>\n",
    "      <td>0.485976</td>\n",
    "      <td>0.485829</td>\n",
    "      <td>0.481373</td>\n",
    "      <td>0.488428</td>\n",
    "      <td>0.487203</td>\n",
    "      <td>0.486539</td>\n",
    "      <td>00:09:43.042908</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.487999</td>\n",
    "      <td>0.493375</td>\n",
    "      <td>0.482984</td>\n",
    "      <td>0.484097</td>\n",
    "      <td>0.492235</td>\n",
    "      <td>0.486474</td>\n",
    "      <td>0.486590</td>\n",
    "      <td>0.482132</td>\n",
    "      <td>0.489590</td>\n",
    "      <td>0.487973</td>\n",
    "      <td>0.487345</td>\n",
    "      <td>00:10:12.208299</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanSquaredError</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557108</td>\n",
    "      <td>0.562508</td>\n",
    "      <td>0.558280</td>\n",
    "      <td>0.554258</td>\n",
    "      <td>0.562980</td>\n",
    "      <td>0.558358</td>\n",
    "      <td>0.557302</td>\n",
    "      <td>0.553167</td>\n",
    "      <td>0.558300</td>\n",
    "      <td>0.555192</td>\n",
    "      <td>0.557745</td>\n",
    "      <td>00:01:07.693415</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557139</td>\n",
    "      <td>0.562738</td>\n",
    "      <td>0.558306</td>\n",
    "      <td>0.554254</td>\n",
    "      <td>0.562944</td>\n",
    "      <td>0.558353</td>\n",
    "      <td>0.557395</td>\n",
    "      <td>0.553266</td>\n",
    "      <td>0.558760</td>\n",
    "      <td>0.555356</td>\n",
    "      <td>0.557851</td>\n",
    "      <td>00:01:05.597960</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557102</td>\n",
    "      <td>0.562685</td>\n",
    "      <td>0.558251</td>\n",
    "      <td>0.554388</td>\n",
    "      <td>0.562916</td>\n",
    "      <td>0.558414</td>\n",
    "      <td>0.557390</td>\n",
    "      <td>0.552967</td>\n",
    "      <td>0.558202</td>\n",
    "      <td>0.555191</td>\n",
    "      <td>0.557750</td>\n",
    "      <td>00:01:06.959792</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557031</td>\n",
    "      <td>0.562640</td>\n",
    "      <td>0.558409</td>\n",
    "      <td>0.554346</td>\n",
    "      <td>0.562962</td>\n",
    "      <td>0.558464</td>\n",
    "      <td>0.557445</td>\n",
    "      <td>0.553233</td>\n",
    "      <td>0.558376</td>\n",
    "      <td>0.555167</td>\n",
    "      <td>0.557807</td>\n",
    "      <td>00:01:11.795853</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.557115</td>\n",
    "      <td>0.562777</td>\n",
    "      <td>0.558264</td>\n",
    "      <td>0.554345</td>\n",
    "      <td>0.563027</td>\n",
    "      <td>0.558331</td>\n",
    "      <td>0.557370</td>\n",
    "      <td>0.553316</td>\n",
    "      <td>0.558392</td>\n",
    "      <td>0.555166</td>\n",
    "      <td>0.557810</td>\n",
    "      <td>00:01:07.012667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.516470</td>\n",
    "      <td>0.521314</td>\n",
    "      <td>0.514840</td>\n",
    "      <td>0.511695</td>\n",
    "      <td>0.518607</td>\n",
    "      <td>0.518541</td>\n",
    "      <td>0.514755</td>\n",
    "      <td>0.512897</td>\n",
    "      <td>0.516087</td>\n",
    "      <td>0.513809</td>\n",
    "      <td>0.515901</td>\n",
    "      <td>00:01:10.840696</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.502617</td>\n",
    "      <td>0.507683</td>\n",
    "      <td>0.500753</td>\n",
    "      <td>0.498227</td>\n",
    "      <td>0.507165</td>\n",
    "      <td>0.503541</td>\n",
    "      <td>0.500668</td>\n",
    "      <td>0.498663</td>\n",
    "      <td>0.503144</td>\n",
    "      <td>0.501001</td>\n",
    "      <td>0.502346</td>\n",
    "      <td>00:01:13.893366</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.495699</td>\n",
    "      <td>0.500809</td>\n",
    "      <td>0.492756</td>\n",
    "      <td>0.492490</td>\n",
    "      <td>0.501192</td>\n",
    "      <td>0.496611</td>\n",
    "      <td>0.494326</td>\n",
    "      <td>0.492330</td>\n",
    "      <td>0.496987</td>\n",
    "      <td>0.494955</td>\n",
    "      <td>0.495816</td>\n",
    "      <td>00:01:16.681958</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.486607</td>\n",
    "      <td>0.491230</td>\n",
    "      <td>0.483011</td>\n",
    "      <td>0.483820</td>\n",
    "      <td>0.491831</td>\n",
    "      <td>0.486592</td>\n",
    "      <td>0.485581</td>\n",
    "      <td>0.481403</td>\n",
    "      <td>0.487401</td>\n",
    "      <td>0.486367</td>\n",
    "      <td>0.486384</td>\n",
    "      <td>00:01:27.023725</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.486400</td>\n",
    "      <td>0.491349</td>\n",
    "      <td>0.481395</td>\n",
    "      <td>0.482946</td>\n",
    "      <td>0.491021</td>\n",
    "      <td>0.484999</td>\n",
    "      <td>0.484834</td>\n",
    "      <td>0.480591</td>\n",
    "      <td>0.487427</td>\n",
    "      <td>0.486391</td>\n",
    "      <td>0.485735</td>\n",
    "      <td>00:01:40.024923</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.487254</td>\n",
    "      <td>0.492384</td>\n",
    "      <td>0.481942</td>\n",
    "      <td>0.483496</td>\n",
    "      <td>0.491511</td>\n",
    "      <td>0.485981</td>\n",
    "      <td>0.485830</td>\n",
    "      <td>0.481377</td>\n",
    "      <td>0.488444</td>\n",
    "      <td>0.487199</td>\n",
    "      <td>0.486542</td>\n",
    "      <td>00:01:52.096167</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.488003</td>\n",
    "      <td>0.493375</td>\n",
    "      <td>0.482983</td>\n",
    "      <td>0.484093</td>\n",
    "      <td>0.492223</td>\n",
    "      <td>0.486466</td>\n",
    "      <td>0.486573</td>\n",
    "      <td>0.482129</td>\n",
    "      <td>0.489579</td>\n",
    "      <td>0.487963</td>\n",
    "      <td>0.487339</td>\n",
    "      <td>00:02:02.946341</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>\n",
    "Wall time: 1h 58min 11s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DecisionTree Regression\n",
    "\n",
    "**Max Depth**\n",
    "The maximum depth (levels) in the tree. When a value is set, the tree may not split further once this level has been met regardless of how many nodes are in the leaf. \n",
    "\n",
    "**Max Features**\n",
    "Number of features to consider when looking for a split. \n",
    "\n",
    "**Minimum Samples in Leaf**\n",
    "Minimum number of samples required to be in a leaf node. Splits may not occur which cause the number of samples in a leaf to be less than this value. Too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Minimum Samples to Split**\n",
    "Minimum number fo samples required to split a node. Care was taken during parameter tests to keep the ratio between Min Samples in Leaf and Min Samples to Split equal to that of the default values (1:2). This was done to allow an even 50/50 split on nodes which match the lowest granularity split criteria. similar to the min samples in leaf, too low a value here leads to overfitting the tree to train data.\n",
    "\n",
    "**Splitter**\n",
    "Options include \"best\" and \"random\". Over all test parameter iterations, \"random\" out-performed \"best likely due to overfitting of specific train data when selecting the \"best\" split every time. \n",
    "\n",
    "After 26 iterations of modifying the above parameters, we land on a final winner based on the lowest average MSE value across all iterations. Average MSE values in our 10 test/train iterations ranged from 0.958022 when utilizing defaults for the random splitter algorithm to a value of 0.491977 in our final tuned random splitter Decision Tree model fit. Parameter inputs for the final Decision Tree Regression model are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>splitter</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>None</th>\n",
    "      <td>auto</td>\n",
    "      <td>40</td>\n",
    "      <td>80</td>\n",
    "      <td>random</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def dtr_explor(ScaledData,\n",
    "                splitter,\n",
    "                max_features ,\n",
    "                max_depth,\n",
    "                min_samples_split,\n",
    "                min_samples_leaf,\n",
    "                PCA         = pca_regression,\n",
    "                Data        = CitiBikeDataSampled_5050,\n",
    "                cv          = cvReg,\n",
    "                seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    dtr_clf = DecisionTreeRegressor(splitter           = splitter, \n",
    "                                    max_features       = max_features, \n",
    "                                    max_depth          = max_depth, \n",
    "                                    min_samples_split  = min_samples_split, \n",
    "                                    min_samples_leaf   = min_samples_leaf, \n",
    "                                    random_state       = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',dtr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, scoring = 'neg_mean_squared_error', cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    accuracy = accuracy * -1\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#splitter           =  'best' \n",
    "#max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "#max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "#min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "#min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(max_features)):\n",
    "#    acclist.append(dtr_explor( ScaledData         = myData_scaled_regression,\n",
    "#                               splitter           = splitter, \n",
    "#                               max_features       = max_features[i], \n",
    "#                               max_depth          = max_depth[i], \n",
    "#                               min_samples_split  = min_samples_split[i],\n",
    "#                               min_samples_leaf   = min_samples_leaf[i],\n",
    "#                              )\n",
    "#                  )\n",
    "#\n",
    "#dtrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"splitter\": splitter,\n",
    "#                                                \"max_features\": max_features,\n",
    "#                                                \"max_depth\": max_depth,\n",
    "#                                                \"min_samples_split\": min_samples_split,\n",
    "#                                                \"min_samples_leaf\": min_samples_leaf \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#dtrdf.columns = ['max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split', 'splitter', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "#display(dtrdf)\n",
    "#\n",
    "#\n",
    "#acclist = []\n",
    "#\n",
    "#splitter           =  'random' \n",
    "#max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "#max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "#min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "#min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "#\n",
    "#for i in range(0,len(max_features)):\n",
    "#    acclist.append(dtr_explor( ScaledData         = myData_scaled_regression,\n",
    "#                               splitter           = splitter, \n",
    "#                               max_features       = max_features[i], \n",
    "#                               max_depth          = max_depth[i], \n",
    "#                               min_samples_split  = min_samples_split[i],\n",
    "#                               min_samples_leaf   = min_samples_leaf[i],\n",
    "#                              )\n",
    "#                  )\n",
    "#\n",
    "#dtrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"splitter\": splitter,\n",
    "#                                                \"max_features\": max_features,\n",
    "#                                                \"max_depth\": max_depth,\n",
    "#                                                \"min_samples_split\": min_samples_split,\n",
    "#                                                \"min_samples_leaf\": min_samples_leaf \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#dtrdf.columns = ['max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split', 'splitter', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "#display(dtrdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>splitter</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanSquaredError</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>1</td>\n",
    "      <td>2</td>\n",
    "      <td>best</td>\n",
    "      <td>0.936970</td>\n",
    "      <td>0.942341</td>\n",
    "      <td>0.929318</td>\n",
    "      <td>0.932539</td>\n",
    "      <td>0.935147</td>\n",
    "      <td>0.938790</td>\n",
    "      <td>0.945778</td>\n",
    "      <td>0.935421</td>\n",
    "      <td>0.948208</td>\n",
    "      <td>0.937663</td>\n",
    "      <td>0.938217</td>\n",
    "      <td>00:04:29.832851</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "      <td>best</td>\n",
    "      <td>0.752071</td>\n",
    "      <td>0.759461</td>\n",
    "      <td>0.752580</td>\n",
    "      <td>0.752722</td>\n",
    "      <td>0.757493</td>\n",
    "      <td>0.759364</td>\n",
    "      <td>0.759938</td>\n",
    "      <td>0.754290</td>\n",
    "      <td>0.767083</td>\n",
    "      <td>0.752146</td>\n",
    "      <td>0.756715</td>\n",
    "      <td>00:04:08.655548</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>6</td>\n",
    "      <td>12</td>\n",
    "      <td>best</td>\n",
    "      <td>0.687408</td>\n",
    "      <td>0.692976</td>\n",
    "      <td>0.684710</td>\n",
    "      <td>0.687400</td>\n",
    "      <td>0.692690</td>\n",
    "      <td>0.690946</td>\n",
    "      <td>0.692571</td>\n",
    "      <td>0.685643</td>\n",
    "      <td>0.691605</td>\n",
    "      <td>0.680705</td>\n",
    "      <td>0.688665</td>\n",
    "      <td>00:03:54.451484</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>8</td>\n",
    "      <td>16</td>\n",
    "      <td>best</td>\n",
    "      <td>0.650561</td>\n",
    "      <td>0.654238</td>\n",
    "      <td>0.643889</td>\n",
    "      <td>0.647622</td>\n",
    "      <td>0.656978</td>\n",
    "      <td>0.651761</td>\n",
    "      <td>0.649921</td>\n",
    "      <td>0.646707</td>\n",
    "      <td>0.649033</td>\n",
    "      <td>0.644511</td>\n",
    "      <td>0.649522</td>\n",
    "      <td>00:03:47.606387</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>10</td>\n",
    "      <td>20</td>\n",
    "      <td>best</td>\n",
    "      <td>0.614989</td>\n",
    "      <td>0.621233</td>\n",
    "      <td>0.615566</td>\n",
    "      <td>0.615328</td>\n",
    "      <td>0.629091</td>\n",
    "      <td>0.625127</td>\n",
    "      <td>0.620880</td>\n",
    "      <td>0.618504</td>\n",
    "      <td>0.620081</td>\n",
    "      <td>0.613094</td>\n",
    "      <td>0.619389</td>\n",
    "      <td>00:03:38.684232</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.535243</td>\n",
    "      <td>0.543630</td>\n",
    "      <td>0.537558</td>\n",
    "      <td>0.535330</td>\n",
    "      <td>0.541051</td>\n",
    "      <td>0.537609</td>\n",
    "      <td>0.536723</td>\n",
    "      <td>0.532167</td>\n",
    "      <td>0.540783</td>\n",
    "      <td>0.536138</td>\n",
    "      <td>0.537623</td>\n",
    "      <td>00:03:21.950141</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>40</td>\n",
    "      <td>80</td>\n",
    "      <td>best</td>\n",
    "      <td>0.515849</td>\n",
    "      <td>0.520111</td>\n",
    "      <td>0.512057</td>\n",
    "      <td>0.513447</td>\n",
    "      <td>0.520051</td>\n",
    "      <td>0.514982</td>\n",
    "      <td>0.511475</td>\n",
    "      <td>0.509958</td>\n",
    "      <td>0.518549</td>\n",
    "      <td>0.511859</td>\n",
    "      <td>0.514834</td>\n",
    "      <td>00:03:19.063416</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>NaN</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.534657</td>\n",
    "      <td>0.542010</td>\n",
    "      <td>0.537141</td>\n",
    "      <td>0.534812</td>\n",
    "      <td>0.541916</td>\n",
    "      <td>0.537149</td>\n",
    "      <td>0.533610</td>\n",
    "      <td>0.529112</td>\n",
    "      <td>0.540442</td>\n",
    "      <td>0.534741</td>\n",
    "      <td>0.536559</td>\n",
    "      <td>00:03:09.878058</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.534714</td>\n",
    "      <td>0.543850</td>\n",
    "      <td>0.537375</td>\n",
    "      <td>0.532762</td>\n",
    "      <td>0.541810</td>\n",
    "      <td>0.538529</td>\n",
    "      <td>0.534843</td>\n",
    "      <td>0.530002</td>\n",
    "      <td>0.541345</td>\n",
    "      <td>0.534672</td>\n",
    "      <td>0.536990</td>\n",
    "      <td>00:03:16.820636</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>500.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.537218</td>\n",
    "      <td>0.543574</td>\n",
    "      <td>0.533963</td>\n",
    "      <td>0.534588</td>\n",
    "      <td>0.540753</td>\n",
    "      <td>0.540022</td>\n",
    "      <td>0.535222</td>\n",
    "      <td>0.530567</td>\n",
    "      <td>0.537818</td>\n",
    "      <td>0.533859</td>\n",
    "      <td>0.536758</td>\n",
    "      <td>00:03:12.738317</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>100.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.536429</td>\n",
    "      <td>0.541849</td>\n",
    "      <td>0.533200</td>\n",
    "      <td>0.533426</td>\n",
    "      <td>0.541033</td>\n",
    "      <td>0.540775</td>\n",
    "      <td>0.537362</td>\n",
    "      <td>0.531359</td>\n",
    "      <td>0.541081</td>\n",
    "      <td>0.536905</td>\n",
    "      <td>0.537342</td>\n",
    "      <td>00:03:14.152400</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.536224</td>\n",
    "      <td>0.543427</td>\n",
    "      <td>0.536396</td>\n",
    "      <td>0.533279</td>\n",
    "      <td>0.542871</td>\n",
    "      <td>0.536698</td>\n",
    "      <td>0.533875</td>\n",
    "      <td>0.530325</td>\n",
    "      <td>0.536687</td>\n",
    "      <td>0.533726</td>\n",
    "      <td>0.536351</td>\n",
    "      <td>00:03:19.537583</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>best</td>\n",
    "      <td>0.537667</td>\n",
    "      <td>0.543525</td>\n",
    "      <td>0.538331</td>\n",
    "      <td>0.534824</td>\n",
    "      <td>0.542371</td>\n",
    "      <td>0.537775</td>\n",
    "      <td>0.535973</td>\n",
    "      <td>0.532457</td>\n",
    "      <td>0.536556</td>\n",
    "      <td>0.533017</td>\n",
    "      <td>0.537250</td>\n",
    "      <td>00:03:15.565956</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>max_depth</th>\n",
    "      <th>max_features</th>\n",
    "      <th>min_samples_leaf</th>\n",
    "      <th>min_samples_split</th>\n",
    "      <th>splitter</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanSquaredError</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>1</td>\n",
    "      <td>2</td>\n",
    "      <td>random</td>\n",
    "      <td>0.951165</td>\n",
    "      <td>0.971850</td>\n",
    "      <td>0.959571</td>\n",
    "      <td>0.949130</td>\n",
    "      <td>0.966839</td>\n",
    "      <td>0.955048</td>\n",
    "      <td>0.958913</td>\n",
    "      <td>0.954587</td>\n",
    "      <td>0.959011</td>\n",
    "      <td>0.954103</td>\n",
    "      <td>0.958022</td>\n",
    "      <td>00:01:11.727801</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "      <td>random</td>\n",
    "      <td>0.635415</td>\n",
    "      <td>0.642623</td>\n",
    "      <td>0.629517</td>\n",
    "      <td>0.634981</td>\n",
    "      <td>0.638684</td>\n",
    "      <td>0.637994</td>\n",
    "      <td>0.631890</td>\n",
    "      <td>0.627198</td>\n",
    "      <td>0.634157</td>\n",
    "      <td>0.631426</td>\n",
    "      <td>0.634388</td>\n",
    "      <td>00:01:04.419089</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>6</td>\n",
    "      <td>12</td>\n",
    "      <td>random</td>\n",
    "      <td>0.576626</td>\n",
    "      <td>0.583834</td>\n",
    "      <td>0.576669</td>\n",
    "      <td>0.577794</td>\n",
    "      <td>0.587732</td>\n",
    "      <td>0.578675</td>\n",
    "      <td>0.579569</td>\n",
    "      <td>0.572405</td>\n",
    "      <td>0.582647</td>\n",
    "      <td>0.580671</td>\n",
    "      <td>0.579662</td>\n",
    "      <td>00:01:02.529776</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>8</td>\n",
    "      <td>16</td>\n",
    "      <td>random</td>\n",
    "      <td>0.551533</td>\n",
    "      <td>0.556458</td>\n",
    "      <td>0.554025</td>\n",
    "      <td>0.550375</td>\n",
    "      <td>0.556129</td>\n",
    "      <td>0.551801</td>\n",
    "      <td>0.550237</td>\n",
    "      <td>0.545924</td>\n",
    "      <td>0.552682</td>\n",
    "      <td>0.549855</td>\n",
    "      <td>0.551902</td>\n",
    "      <td>00:01:00.247705</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>10</td>\n",
    "      <td>20</td>\n",
    "      <td>random</td>\n",
    "      <td>0.537302</td>\n",
    "      <td>0.540479</td>\n",
    "      <td>0.533560</td>\n",
    "      <td>0.528145</td>\n",
    "      <td>0.543937</td>\n",
    "      <td>0.537868</td>\n",
    "      <td>0.537912</td>\n",
    "      <td>0.528990</td>\n",
    "      <td>0.536854</td>\n",
    "      <td>0.536039</td>\n",
    "      <td>0.536109</td>\n",
    "      <td>00:01:01.246999</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.500324</td>\n",
    "      <td>0.505974</td>\n",
    "      <td>0.496632</td>\n",
    "      <td>0.499545</td>\n",
    "      <td>0.507485</td>\n",
    "      <td>0.501330</td>\n",
    "      <td>0.500178</td>\n",
    "      <td>0.495743</td>\n",
    "      <td>0.504212</td>\n",
    "      <td>0.500504</td>\n",
    "      <td>0.501193</td>\n",
    "      <td>00:00:57.026882</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>NaN</td>\n",
    "      <td>auto</td>\n",
    "      <td>40</td>\n",
    "      <td>80</td>\n",
    "      <td>random</td>\n",
    "      <td>0.491252</td>\n",
    "      <td>0.497105</td>\n",
    "      <td>0.490139</td>\n",
    "      <td>0.488494</td>\n",
    "      <td>0.499163</td>\n",
    "      <td>0.490388</td>\n",
    "      <td>0.491458</td>\n",
    "      <td>0.487272</td>\n",
    "      <td>0.495891</td>\n",
    "      <td>0.488610</td>\n",
    "      <td>0.491977</td>\n",
    "      <td>00:00:58.783894</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>NaN</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.499172</td>\n",
    "      <td>0.505983</td>\n",
    "      <td>0.497109</td>\n",
    "      <td>0.496881</td>\n",
    "      <td>0.506959</td>\n",
    "      <td>0.499494</td>\n",
    "      <td>0.500449</td>\n",
    "      <td>0.495286</td>\n",
    "      <td>0.500920</td>\n",
    "      <td>0.500916</td>\n",
    "      <td>0.500317</td>\n",
    "      <td>00:00:57.685931</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.502207</td>\n",
    "      <td>0.506740</td>\n",
    "      <td>0.498746</td>\n",
    "      <td>0.498204</td>\n",
    "      <td>0.507032</td>\n",
    "      <td>0.500179</td>\n",
    "      <td>0.498984</td>\n",
    "      <td>0.493321</td>\n",
    "      <td>0.502544</td>\n",
    "      <td>0.500674</td>\n",
    "      <td>0.500863</td>\n",
    "      <td>00:00:56.639504</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>500.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.499133</td>\n",
    "      <td>0.503886</td>\n",
    "      <td>0.495351</td>\n",
    "      <td>0.499098</td>\n",
    "      <td>0.504625</td>\n",
    "      <td>0.502198</td>\n",
    "      <td>0.499949</td>\n",
    "      <td>0.494933</td>\n",
    "      <td>0.501500</td>\n",
    "      <td>0.500346</td>\n",
    "      <td>0.500102</td>\n",
    "      <td>00:00:57.002340</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>100.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.500832</td>\n",
    "      <td>0.508415</td>\n",
    "      <td>0.498722</td>\n",
    "      <td>0.499630</td>\n",
    "      <td>0.506633</td>\n",
    "      <td>0.499886</td>\n",
    "      <td>0.499911</td>\n",
    "      <td>0.495575</td>\n",
    "      <td>0.499764</td>\n",
    "      <td>0.499878</td>\n",
    "      <td>0.500925</td>\n",
    "      <td>00:00:56.437142</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.499827</td>\n",
    "      <td>0.505587</td>\n",
    "      <td>0.498121</td>\n",
    "      <td>0.497188</td>\n",
    "      <td>0.503682</td>\n",
    "      <td>0.501258</td>\n",
    "      <td>0.497854</td>\n",
    "      <td>0.493423</td>\n",
    "      <td>0.501655</td>\n",
    "      <td>0.500480</td>\n",
    "      <td>0.499908</td>\n",
    "      <td>00:00:55.017518</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>1000.0</td>\n",
    "      <td>14</td>\n",
    "      <td>25</td>\n",
    "      <td>50</td>\n",
    "      <td>random</td>\n",
    "      <td>0.500172</td>\n",
    "      <td>0.504005</td>\n",
    "      <td>0.498092</td>\n",
    "      <td>0.496990</td>\n",
    "      <td>0.505760</td>\n",
    "      <td>0.497199</td>\n",
    "      <td>0.499508</td>\n",
    "      <td>0.495422</td>\n",
    "      <td>0.500009</td>\n",
    "      <td>0.500779</td>\n",
    "      <td>0.499794</td>\n",
    "      <td>00:00:55.939379</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>\n",
    "Wall time: 59min 3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi-Layer Perceptron (MLP) Regression\n",
    "\n",
    "**Hidden Layers**\n",
    "\n",
    "One core component of Neural Networks is then notion of layers. There are three layer types:\n",
    "* Input Layer: One input layer is comprised of a number of neurons equal to the number of features in your data\n",
    "* Output Layer: One output layer is produced in regression as a single regressor node.\n",
    "* Hidden Layer(s): There may be 1:N hidden layers, each comprised of X neurons. This N,X is defined as we define our CLF model object. A general rule of thumb is provided in a <a href=\"http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\">stackexchange article</a> for the number of hidden layers to use and how many neurons are needed in each layer. As described in the article provided information from the \"Introduction to Neural Networks for Java, Second Edition\" textbook, most situations do not need more than one hidden layer - however there is a general rule of thumb to be computed for the number of neurons in that layer. This rule of thumb for computing the number of neurons in a Hidden Layer is as follows:\n",
    "\n",
    "    * The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "    * The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "    * The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "We have chosen to utilized this rule of thumb as a starting place (e.g. 15*(2/3)+1 = 11), when testing inputs for the hidden_layer_sizes parameter.\n",
    "\n",
    "**Alpha**\n",
    "\n",
    "Alpha is utilized as a smoothing parameter.\n",
    "\n",
    "**Batch Size** \n",
    "\n",
    "MLP Regression utilizes stochastic gradient descent with minibatches in its function. As defined in a <a href=\"http://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent\">stackexchange article</a>: \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *~\"The Batch size determines how many examples you look at before making a weight update. The lower it is, the noisier the training signal is going to be, the higher it is, the longer it will take to compute the gradient for each step.\"* \n",
    "\n",
    "We will be cognizant of this factor as to obtain reasonable run-times while avoiding high noise in our model.\n",
    "\n",
    "**Max Iterations**\n",
    "\n",
    "Max Iterations defines the maximum number of iterations to process regardless of the optimization tolerance identified. If optimization tolerance threshold is not met sooner than the defined number of max iterations, the training will stop early.\n",
    "\n",
    "**Initial Learning Rate**\n",
    "\n",
    "The Initial Learning rate identifies step-size for weight updates.\n",
    "\n",
    "After 20 iterations of modifying the above parameters, we land on a final winner based on the lowest average MSE value across all iterations. Average MSE values in our 10 test/train iterations ranged from 0.502850 from the worst parameter inputs to a value of 0.492009 in our final tuned MLP model fit. Parameter inputs for the final MLP model are as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Batch_Size</th>\n",
    "      <th>alpha</th>\n",
    "      <th>hidden_layer_sizes</th>\n",
    "      <th>learning_rate_init</th>\n",
    "      <th>max_iter</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>500</td>\n",
    "      <td>0.05</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>20</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def mlp_explor(ScaledData,\n",
    "               hidden_layer_sizes,\n",
    "               alpha,\n",
    "               batch_size,\n",
    "               max_iter,\n",
    "               learning_rate_init,\n",
    "               PCA         = pca_regression,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cvReg,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "\n",
    "    mlp_clf = MLPRegressor(hidden_layer_sizes = hidden_layer_sizes, \n",
    "                           alpha              = alpha,\n",
    "                           batch_size         = batch_size,\n",
    "                           max_iter           = max_iter,\n",
    "                           learning_rate_init = learning_rate_init,\n",
    "                           random_state       = seed,\n",
    "                           verbose            = True\n",
    "                          ) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',mlp_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, scoring = 'neg_mean_squared_error', cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    accuracy = accuracy * -1\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "##hidden_layer_sizes  =  [11,    10,    12    , 13,    14,    14    , 14    , 14    , 14     , 14    , 14   ,  14   ,  14    ,  14    ,  14  ,  14   ,  14   ,  14   ,  14   ,  14  ] \n",
    "##alpha               =  [.0001, .0001, .0001 , .0001, .0001, .001 , .01 ,    .05 ,   .1 ,   , .05   , .05  ,  .05  ,  .05   ,  .05   ,  .05 ,  .05  ,  .05  ,  .05  ,  .05  ,  .05 ]\n",
    "##batch_size          =  ['auto','auto','auto', 'auto','auto','auto', 'auto', 'auto', 'auto' ,  200  , 250  ,  500  ,  500   ,  500   ,  500 ,  500  ,  500  ,  500  ,  500  ,  500 ]  ##auto should resemble 200 by default for our dataset\n",
    "##max_iter            =  [200,   200,   200   , 200,   200,   200   , 200   , 200   , 200    , 200   , 200  , 200   , 200    , 200    , 200  , 10    , 15    , 20    , 25    , 30   ]\n",
    "##learning_rate_init  =  [.001,  .001,  .001  , .001,  .001,  .001  , .001  , .001  , .001   , .001  , .001 , .001  , .01    , .05   , .1    , .001  , .001  , .001  , .001  , .001 ]\n",
    "#\n",
    "#\n",
    "#hidden_layer_sizes  =  [11,    10,    12    , 13,    14,    14    , 14    , 14    , 14     , 14    , 14   ,  14   ,  14    ,  14    ,  14  ,  14   ,  14   ,  14   ,  14   ,  14  ] \n",
    "#alpha               =  [.0001, .0001, .0001 , .0001, .0001, .001 , .01 ,    .05 ,   .1   , .05   , .05  ,  .05  ,  .05   ,  .05   ,  .05 ,  .05  ,  .05  ,  .05  ,  .05  ,  .05 ]\n",
    "#batch_size          =  ['auto','auto','auto', 'auto','auto','auto', 'auto', 'auto', 'auto' ,  200  , 250  ,  500  ,  500   ,  500   ,  500 ,  500  ,  500  ,  500  ,  500  ,  500 ]  ##auto should resemble 200 by default for our dataset\n",
    "#max_iter            =  [200,   200,   200   , 200,   200,   200   , 200   , 200   , 200    , 200   , 200  , 200   , 200    , 200    , 200  , 10    , 15    , 20    , 25    , 30   ]\n",
    "#learning_rate_init  =  [.001,  .001,  .001  , .001,  .001,  .001  , .001  , .001  , .001   , .001  , .001 , .001  , .01    , .05   , .1    , .001  , .001  , .001  , .001  , .001 ]\n",
    "#\n",
    "#for i in range(0,len(hidden_layer_sizes)):\n",
    "#    acclist.append(mlp_explor(ScaledData         = myData_scaled_regression,\n",
    "#                              hidden_layer_sizes = hidden_layer_sizes[i],\n",
    "#                              alpha              = alpha[i],\n",
    "#                              batch_size         = batch_size[i],\n",
    "#                              max_iter           = max_iter[i],\n",
    "#                              learning_rate_init = learning_rate_init[i]\n",
    "#                              )\n",
    "#                  )\n",
    "#\n",
    "#mlpdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "#                                                \"alpha\": alpha,\n",
    "#                                                \"Batch_Size\": batch_size,\n",
    "#                                                \"max_iter\": max_iter,\n",
    "#                                                \"learning_rate_init\": learning_rate_init\n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#mlpdf.columns = ['Batch_Size', 'alpha', 'hidden_layer_sizes', 'learning_rate_init', 'max_iter', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "#display(mlpdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Batch_Size</th>\n",
    "      <th>alpha</th>\n",
    "      <th>hidden_layer_sizes</th>\n",
    "      <th>learning_rate_init</th>\n",
    "      <th>max_iter</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanSquaredError</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0001</td>\n",
    "      <td>11</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.498157</td>\n",
    "      <td>0.495658</td>\n",
    "      <td>0.493068</td>\n",
    "      <td>0.491410</td>\n",
    "      <td>0.500698</td>\n",
    "      <td>0.495552</td>\n",
    "      <td>0.492865</td>\n",
    "      <td>0.490339</td>\n",
    "      <td>0.498439</td>\n",
    "      <td>0.498745</td>\n",
    "      <td>0.495493</td>\n",
    "      <td>00:03:12.262031</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0001</td>\n",
    "      <td>10</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.496425</td>\n",
    "      <td>0.499934</td>\n",
    "      <td>0.491257</td>\n",
    "      <td>0.489330</td>\n",
    "      <td>0.498033</td>\n",
    "      <td>0.491275</td>\n",
    "      <td>0.491529</td>\n",
    "      <td>0.491205</td>\n",
    "      <td>0.494582</td>\n",
    "      <td>0.496787</td>\n",
    "      <td>0.494036</td>\n",
    "      <td>00:02:17.766522</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0001</td>\n",
    "      <td>12</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.493275</td>\n",
    "      <td>0.497698</td>\n",
    "      <td>0.489731</td>\n",
    "      <td>0.488954</td>\n",
    "      <td>0.497222</td>\n",
    "      <td>0.490741</td>\n",
    "      <td>0.492195</td>\n",
    "      <td>0.489988</td>\n",
    "      <td>0.494885</td>\n",
    "      <td>0.492794</td>\n",
    "      <td>0.492748</td>\n",
    "      <td>00:02:30.538386</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0001</td>\n",
    "      <td>13</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.497134</td>\n",
    "      <td>0.502090</td>\n",
    "      <td>0.492664</td>\n",
    "      <td>0.491720</td>\n",
    "      <td>0.501164</td>\n",
    "      <td>0.494029</td>\n",
    "      <td>0.495838</td>\n",
    "      <td>0.491297</td>\n",
    "      <td>0.499210</td>\n",
    "      <td>0.496031</td>\n",
    "      <td>0.496118</td>\n",
    "      <td>00:02:09.501304</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0001</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.494070</td>\n",
    "      <td>0.498247</td>\n",
    "      <td>0.490013</td>\n",
    "      <td>0.490383</td>\n",
    "      <td>0.497200</td>\n",
    "      <td>0.490934</td>\n",
    "      <td>0.492023</td>\n",
    "      <td>0.488059</td>\n",
    "      <td>0.496023</td>\n",
    "      <td>0.493481</td>\n",
    "      <td>0.493043</td>\n",
    "      <td>00:02:28.060405</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0010</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.493969</td>\n",
    "      <td>0.499164</td>\n",
    "      <td>0.489704</td>\n",
    "      <td>0.490340</td>\n",
    "      <td>0.496798</td>\n",
    "      <td>0.491046</td>\n",
    "      <td>0.491931</td>\n",
    "      <td>0.487964</td>\n",
    "      <td>0.494619</td>\n",
    "      <td>0.493528</td>\n",
    "      <td>0.492906</td>\n",
    "      <td>00:02:25.840677</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0100</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.494056</td>\n",
    "      <td>0.497495</td>\n",
    "      <td>0.490564</td>\n",
    "      <td>0.490419</td>\n",
    "      <td>0.496764</td>\n",
    "      <td>0.491136</td>\n",
    "      <td>0.491988</td>\n",
    "      <td>0.488095</td>\n",
    "      <td>0.494396</td>\n",
    "      <td>0.491870</td>\n",
    "      <td>0.492678</td>\n",
    "      <td>00:02:32.937363</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.493827</td>\n",
    "      <td>0.496908</td>\n",
    "      <td>0.488863</td>\n",
    "      <td>0.489456</td>\n",
    "      <td>0.496598</td>\n",
    "      <td>0.490511</td>\n",
    "      <td>0.490308</td>\n",
    "      <td>0.487158</td>\n",
    "      <td>0.494863</td>\n",
    "      <td>0.491783</td>\n",
    "      <td>0.492027</td>\n",
    "      <td>00:04:18.436030</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>auto</td>\n",
    "      <td>0.1000</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.496157</td>\n",
    "      <td>0.499838</td>\n",
    "      <td>0.488890</td>\n",
    "      <td>0.490125</td>\n",
    "      <td>0.499632</td>\n",
    "      <td>0.493240</td>\n",
    "      <td>0.493574</td>\n",
    "      <td>0.487844</td>\n",
    "      <td>0.497027</td>\n",
    "      <td>0.491309</td>\n",
    "      <td>0.493764</td>\n",
    "      <td>00:04:25.932690</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>200</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.493826</td>\n",
    "      <td>0.496695</td>\n",
    "      <td>0.489852</td>\n",
    "      <td>0.489445</td>\n",
    "      <td>0.496591</td>\n",
    "      <td>0.492467</td>\n",
    "      <td>0.492557</td>\n",
    "      <td>0.487138</td>\n",
    "      <td>0.494843</td>\n",
    "      <td>0.491369</td>\n",
    "      <td>0.492479</td>\n",
    "      <td>00:03:50.909407</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>250</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.494364</td>\n",
    "      <td>0.496471</td>\n",
    "      <td>0.488480</td>\n",
    "      <td>0.489218</td>\n",
    "      <td>0.496942</td>\n",
    "      <td>0.490106</td>\n",
    "      <td>0.490384</td>\n",
    "      <td>0.486016</td>\n",
    "      <td>0.494512</td>\n",
    "      <td>0.491658</td>\n",
    "      <td>0.491815</td>\n",
    "      <td>00:03:21.241179</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>200</td>\n",
    "      <td>0.492578</td>\n",
    "      <td>0.497246</td>\n",
    "      <td>0.489482</td>\n",
    "      <td>0.488808</td>\n",
    "      <td>0.496983</td>\n",
    "      <td>0.490966</td>\n",
    "      <td>0.490436</td>\n",
    "      <td>0.487958</td>\n",
    "      <td>0.494417</td>\n",
    "      <td>0.492028</td>\n",
    "      <td>0.492090</td>\n",
    "      <td>00:02:01.222607</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.010</td>\n",
    "      <td>200</td>\n",
    "      <td>0.498737</td>\n",
    "      <td>0.503474</td>\n",
    "      <td>0.489079</td>\n",
    "      <td>0.489032</td>\n",
    "      <td>0.504417</td>\n",
    "      <td>0.493491</td>\n",
    "      <td>0.505238</td>\n",
    "      <td>0.485493</td>\n",
    "      <td>0.494142</td>\n",
    "      <td>0.492875</td>\n",
    "      <td>0.495598</td>\n",
    "      <td>00:02:07.060096</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.050</td>\n",
    "      <td>200</td>\n",
    "      <td>0.492834</td>\n",
    "      <td>0.498755</td>\n",
    "      <td>0.489331</td>\n",
    "      <td>0.487773</td>\n",
    "      <td>0.508064</td>\n",
    "      <td>0.493853</td>\n",
    "      <td>0.494311</td>\n",
    "      <td>0.488441</td>\n",
    "      <td>0.495504</td>\n",
    "      <td>0.492787</td>\n",
    "      <td>0.494165</td>\n",
    "      <td>00:01:55.334441</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.100</td>\n",
    "      <td>200</td>\n",
    "      <td>0.506872</td>\n",
    "      <td>0.506885</td>\n",
    "      <td>0.498746</td>\n",
    "      <td>0.504435</td>\n",
    "      <td>0.502350</td>\n",
    "      <td>0.506350</td>\n",
    "      <td>0.494072</td>\n",
    "      <td>0.503542</td>\n",
    "      <td>0.500440</td>\n",
    "      <td>0.504809</td>\n",
    "      <td>0.502850</td>\n",
    "      <td>00:01:24.985412</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>10</td>\n",
    "      <td>0.495239</td>\n",
    "      <td>0.497960</td>\n",
    "      <td>0.490874</td>\n",
    "      <td>0.489932</td>\n",
    "      <td>0.497914</td>\n",
    "      <td>0.491773</td>\n",
    "      <td>0.492751</td>\n",
    "      <td>0.489714</td>\n",
    "      <td>0.495338</td>\n",
    "      <td>0.493274</td>\n",
    "      <td>0.493477</td>\n",
    "      <td>00:01:36.920482</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>15</td>\n",
    "      <td>0.493328</td>\n",
    "      <td>0.497228</td>\n",
    "      <td>0.490103</td>\n",
    "      <td>0.489093</td>\n",
    "      <td>0.496217</td>\n",
    "      <td>0.490839</td>\n",
    "      <td>0.491264</td>\n",
    "      <td>0.487964</td>\n",
    "      <td>0.494121</td>\n",
    "      <td>0.492163</td>\n",
    "      <td>0.492232</td>\n",
    "      <td>00:01:55.192959</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>17</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>20</td>\n",
    "      <td>0.492459</td>\n",
    "      <td>0.497270</td>\n",
    "      <td>0.489506</td>\n",
    "      <td>0.488790</td>\n",
    "      <td>0.497002</td>\n",
    "      <td>0.490845</td>\n",
    "      <td>0.490447</td>\n",
    "      <td>0.487949</td>\n",
    "      <td>0.493788</td>\n",
    "      <td>0.492035</td>\n",
    "      <td>0.492009</td>\n",
    "      <td>00:02:16.153927</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>18</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>25</td>\n",
    "      <td>0.492583</td>\n",
    "      <td>0.497230</td>\n",
    "      <td>0.489484</td>\n",
    "      <td>0.488816</td>\n",
    "      <td>0.497019</td>\n",
    "      <td>0.490852</td>\n",
    "      <td>0.490459</td>\n",
    "      <td>0.487947</td>\n",
    "      <td>0.494401</td>\n",
    "      <td>0.492023</td>\n",
    "      <td>0.492082</td>\n",
    "      <td>00:02:11.970522</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>19</th>\n",
    "      <td>500</td>\n",
    "      <td>0.0500</td>\n",
    "      <td>14</td>\n",
    "      <td>0.001</td>\n",
    "      <td>30</td>\n",
    "      <td>0.492611</td>\n",
    "      <td>0.497277</td>\n",
    "      <td>0.489516</td>\n",
    "      <td>0.488817</td>\n",
    "      <td>0.496992</td>\n",
    "      <td>0.490838</td>\n",
    "      <td>0.490455</td>\n",
    "      <td>0.487961</td>\n",
    "      <td>0.493788</td>\n",
    "      <td>0.492028</td>\n",
    "      <td>0.492028</td>\n",
    "      <td>00:02:13.830300</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 4 - Analyze the results using a chosen method of evaluation\n",
    "#### Classification\n",
    "\n",
    "We have created a function to be re-used for our cross-validation Accuracy Scores. Inputs of PCA components, Model CLF object, original sample data, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores, and log_loss values for the different permutations of models tested. A ROC Curve plot is also displayed depicting a stacked view of the ROC_AUC Values for each iteration. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ROC_curve(X, y, mean_tpr, mean_fpr, cv = cv, ):\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "             label='Luck')\n",
    "\n",
    "    mean_tpr /= cv.get_n_splits(X, y)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,  \n",
    "                                         ScaledData,\n",
    "                                         PCA      = pca_classification,\n",
    "                                         Data     = CitiBikeDataSampled_5050,\n",
    "                                         cv       = cv):\n",
    "    \n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    X = ScaledData\n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',clf)]\n",
    "    )\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "    lw = 2\n",
    "    i = 0\n",
    "    accuracy = []\n",
    "    logloss = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "        l = float(mt.log_loss(y[test], y_hat))\n",
    "        \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "        logloss.append(round(l,5)) \n",
    "\n",
    "\n",
    "        probas_ = clf_pipe.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        \n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = (12, 6)\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "                 label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\\n\\\n",
    "Log Loss Values across all iterations: {2}\\n\\n\\\n",
    "Average Log Loss: {3}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5), logloss,round(sum(logloss)/len(logloss),5)))\n",
    "    \n",
    "    plot_ROC_curve(X, y, mean_tpr, mean_fpr)\n",
    "    \n",
    "    ytestnames = np.where(y[test] == 1, 'Subscriber', 'Customer')\n",
    "    yhatnames  = np.where(y_hat == 1, 'Subscriber', 'Customer')\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =[\"Customer\", \"Subscriber\"], \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf_pipe.named_steps['CLF'], accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression - Analyze the Results\n",
    "\n",
    "Based on our previously performed parameter input tests above, we would like to dig deeper into additional accuracy metrics as discussed previously. With our final logistic regression model including a cost value of .01, we compute an average accuracy rating of 68.189 %, and an Average Log Loss Value of 10.9871 across all 10 iterations. Plotting our True Positive rate vs. False Positive rate in an ROC curve provides additional insight depicting consistency across iterations. This is a good sign that our test vs train methodology was well established, due to consistent results across varying datasets. Our Mean ROC area under the curve for all 10 iterations is 0.74, leaving much to be desired in terms of true positives as we would like to see the curve reach further to the top left of the plot. However, as discussed previously, our interest lies in the false positives. In a confusion matrix of predicted results, we find that we have a true positive rating of 64.588 % for subscriber users, whereas our customer true positive rating is 72.16 %. This provides us with 6960 Customer riders(out of 25,000), which may be flagged as potential targets for conversion marketing to subscribing members. Ultimately, we would like to find a model with a higher subscriber true positive rating - as this also strengthens the likelihood of strong conversion targets on the false positive customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=.01, class_weight=None, random_state=seed) # get object\n",
    "\n",
    "lr_clf, lr_acc = compute_kfold_scores_Classification(clf         = lr_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Analyze the Results\n",
    "\n",
    "The next model performed in the previous section, is the Random Forest Classification model. With our tuned parameters identified we may now assess futher insights. The Random Forest Model computes a more successful average accuracy rating of 72.496 %, and an Average Log Loss Value of 9.49958 across all 10 iterations. Once again, plotting our True Positive rate vs. False Positive rate in an ROC curve provides insight depicting consistency across iterations. Our Mean ROC area under the curve for all 10 iterations is 0.80, an improvement from logistic regression. We notice the ROC curve line is slightly further to the top left of the plot - as previously discussed, this is the type of improvement we were looking for! In a confusion matrix of predicted results, we find that we have a true positive rating of 72.272 % for subscriber users, whereas our customer true positive rating is 72.952 %. This provides us with 6762 Customer riders(out of 25,000), which may be flagged as potential targets for conversion marketing to subscribing members. With such an improvement in our true positive subscriber ratings, we have much more confidence in the false positive customer predictions here in comparison to logistic regression results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "rfc_clf, rfc_acc = compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K Nearest Neighbors (KNN) - Analyze the Results\n",
    "\n",
    "Finally, the last classification model performed is the K Nearest Neighbors Classification model. With our tuned parameters identified we may now assess futher insights. The KNN Model computes an average accuracy rating of 69.554 %, and an Average Log Loss Value of 10.51565 across all 10 iterations. Although these values are slightly more favorable than logistic regression, they are lacking in comparison to the Random Forest Classifier. Once again, plotting our True Positive rate vs. False Positive rate in an ROC curve provides insight depicting consistency across iterations. We are confident that our test/train methodology is sound while using stratified 10 kfold cross validation. Our Mean ROC area under the curve for all 10 iterations is 0.77, an improvement from logistic regression, but once again less favorable than Random Forest. In a confusion matrix of predicted results, we find that we have a true positive rating of 67.464 % for subscriber users, whereas our customer true positive rating is 71.884 %. This provides us with 7029 Customer riders(out of 25,000), which may be flagged as potential targets for conversion marketing to subscribing members. Given knowledge of the previous two models these results are worrysome, as this model produces the largest false positive rating for customers, while still remaining fairly low true positive subscriber rating. Although true positive subscriber rating is higher than that of logistic regression, we will be hesitant with moving forward with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 150, algorithm = 'kd_tree', leaf_size = 50, n_jobs=-1) # get object\n",
    "\n",
    "knn_clf, knn_acc = compute_kfold_scores_Classification(clf         = knn_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "Like with our classification, we created a function to pass the regression method and would automatically build the model based around the principle component analysis of our data set through a . The results of the PCA is then pipelined into the regression method to be analyzed for the Mean Squared Error, R squared and Adjusted R squared values for each iteration as well as the mean values for all iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Regression(     clf,  \n",
    "                                         ScaledData = myData_scaled_regression,\n",
    "                                         PCA        = pca_regression,\n",
    "                                         Data       = CitiBikeDataSampled_5050,\n",
    "                                         cv         = cvReg):\n",
    "    \n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',clf)]\n",
    "    )\n",
    "\n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "\n",
    "    MSE = []\n",
    "    r2  = []\n",
    "    ar2 = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        m = float(mt.mean_squared_error(y[test],y_hat))\n",
    "        r = float(mt.r2_score(y[test], y_hat))\n",
    "        \n",
    "        MSE.append(round(m,5)) \n",
    "        r2.append(round(r,5))\n",
    "        \n",
    "        N = len(y_hat)\n",
    "        p = 15 # number of components\n",
    "        ar = 1-(((1-r)*(N-1))/(N-p-1))\n",
    "        ar2.append(round(ar,5)) \n",
    "                \n",
    "    \n",
    "    print(\"Mean Squared Error across all iterations: {0}\\n\\n\\\n",
    "Average Mean Squared Error: {1}\\n\\n\\\n",
    "R^2 Values across all iterations: {2}\\n\\n\\\n",
    "Average R^2: {3}\\n\\n\\\n",
    "Adjusted R^2 Values across all iterations: {4}\\n\\n\\\n",
    "Average Adjusted R^2: {5}\\n\".format(MSE, round(sum(MSE)/len(MSE),5), r2, round(sum(r2)/len(ar2),5), ar2,round(sum(ar2)/len(ar2),5)))\n",
    "    \n",
    "       \n",
    "    #ytestnames = np.where(y[test] == 1, 'Subscriber', 'Customer')\n",
    "    #yhatnames  = np.where(y_hat == 1, 'Subscriber', 'Customer')\n",
    "    #print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "    #                      classes   =[\"Customer\", \"Subscriber\"], \n",
    "    #                      normalize =True,\n",
    "    #                      title     ='Confusion matrix, with normalization')\n",
    "    \n",
    "    return clf_pipe.named_steps['CLF'], MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Regression - Analyze the Results\n",
    "\n",
    "With our KNN Regression we found our average Mean Squared Error to be fairly low, with very little variance between the MSE among iterations. The adjusted R squared is also extremely low, giving, at first glance, low confidence in the fit of our model. With with such a small mean squared error, it's likely that the fit is off because of the lack of variance between not only our predicted values, but due in part of the variance of the data set itself. We have to be mindful of the fact that we are dealing with a log transform which produces a narrow band of values based on the original trip duration. With such a low MSE, it could be interpreted that we have a somewhat strong model, even if our goodness of fit score isn't necessarily very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knnr_clf = KNeighborsRegressor(n_neighbors = 100, algorithm = 'kd_tree', leaf_size = 50, n_jobs=-1) # get object\n",
    "knnr_clf, knnr_mse = compute_kfold_scores_Regression(clf = knnr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regression - Analyze the Results\n",
    "\n",
    "Surprisingly, we found similar results with our Decision Tree process. However, with a Decision Tree, we had a higher MSE and a lower adjusted R squared. The two tests aren't different by much, 0.01 in the case of MSE and 1% in the case of the adjusted R squared, with the variance in values that we have in the data set, these normally minute differences could be dramatically different in the original non-log trip durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dtr_clf = DecisionTreeRegressor(splitter           = 'random', \n",
    "                                max_features       = 'auto', \n",
    "                                max_depth          = None, \n",
    "                                min_samples_split  = 80, \n",
    "                                min_samples_leaf   = 40, \n",
    "                                random_state       = seed) # get object\n",
    "dtr_clf, dtr_mse = compute_kfold_scores_Regression(clf = dtr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Layer Perceptron Regression - Analyze the Results\n",
    "\n",
    "The Multi-Layer Perceptron Regression almost mirrors the results found with the decision tree nearly to the thousandsths. But, once more, it's in a higher MSE and a lower adjusted R squared to the KNN Regression. We're still maintaining consistency across our iterations which is good, but ultimately appears to not be quite as precise, despite it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mlp_clf = MLPRegressor(hidden_layer_sizes = 14, \n",
    "                       alpha              = .05,\n",
    "                       batch_size         = 500,\n",
    "                       max_iter           = 20,\n",
    "                       learning_rate_init = .001,\n",
    "                       random_state       = seed\n",
    "                      ) # get object\n",
    "mlp_clf, mlp_mse = compute_kfold_scores_Regression(clf = mlp_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 5 - Discuss the advantages of each model\n",
    "\n",
    "#### Classification\n",
    "\n",
    "**Winner:** Random Forest Classification\n",
    "\n",
    "Despite the long run-time of this model, the accuracy metrics in this classification model are difficult to dispute. An average accuracy rating of 72.496 %, puts the random forest accuracy rating 2.942 % ahead of the next closest model(KNN). However, as discussed in the analysis section, we are hesitant to utilize KNN due to the fact that it contained the lowest True Positive Customer rating of all models. Given this factor - when comparing to Logistic regression, we find that Random forest correctly predicted 1,945 more subscribing members providing more confidence in the false positive customer predictions as potential targets for conversion. If run-time of this model deems as innapropriate in a live environment, measures could be taken to reduce the n_estimators parameter, as it is a large runtime performance factor. This would of course have implications reducing accuracy of the model fit, but as was discovered during parameter tuning, it is very possible that it could continue to be the top model even with an adjustment on n_estimators.\n",
    "\n",
    "To further prove with 95% confidence that the random forest classification model is different from the remaining two models, we have computed a confidence interval for the difference in mean accuracy values between the random forest model in comparison to the other two. Since the value of zero does not appear within either confidence interval, there is sufficient evidence to suggest the mean accuracy score of the random forest model is not equal to any other classification model produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_acc = np.array(lr_acc)\n",
    "rfc_acc = np.array(rfc_acc)\n",
    "knn_acc = np.array(knn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = sms.CompareMeans(sms.DescrStatsW(lr_acc), sms.DescrStatsW(rfc_acc))\n",
    "print (cm.tconfint_diff(usevar='unequal'))\n",
    "\n",
    "cm = sms.CompareMeans(sms.DescrStatsW(knn_acc), sms.DescrStatsW(rfc_acc))\n",
    "print (cm.tconfint_diff(usevar='unequal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "**Winner:** K Nearest Neighbor Regression\n",
    "\n",
    "The differences between our regression methods is very slim going purely by the numbers.  Our KNN regression came out ahead with a slightly better MSE and adjusted R squared. With our current dataset, were leaning heavily on the better numbers for our decision to go with KNN as our final method for modeling our prediction. What was very interesting in our comparisons was the similarity in results for the Decision Tree model and the MLP model. Its fascinating that such two vastly different approaches to machine learning reach very nearly the same level of prediction in regards to MSE and adjusted R squared.\n",
    "It is also important to note one major feature of the KNN model, and thats its efficiency in dealing with massive datasets. In our sampled set, we only dealt with 500,000 observations which we queried from over 5,000,000. Building out a more accurate model using the entirety of the dataset would require keeping an eye on resource needs. Despite this, it still appears to be the best model for predicting trip duration.\n",
    "\n",
    "To further prove with 95% confidence that the KNN Regression model is different from the remaining two models, we have computed a confidence interval for the difference in mean MSE values between the KNN model in comparison to the other two. Since the value of zero does not appear within either confidence interval, there is sufficient evidence to suggest the mean MSE score of the KNN model is not equal to any other Regression model produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knnr_mse = np.array(knnr_mse)\n",
    "dtr_mse = np.array(dtr_mse)\n",
    "mlp_mse = np.array(mlp_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = sms.CompareMeans(sms.DescrStatsW(dtr_mse), sms.DescrStatsW(knnr_mse))\n",
    "print (cm.tconfint_diff(usevar='unequal'))\n",
    "\n",
    "cm = sms.CompareMeans(sms.DescrStatsW(mlp_mse), sms.DescrStatsW(knnr_mse))\n",
    "print (cm.tconfint_diff(usevar='unequal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 6 - Which attributes from the analysis are most important?\n",
    "\n",
    "With our models compared and contrasted, it is time to discuss attribute importance in helping us make our classification and regression predictions. It is important to remember that our variables used were not the original data set features themselves but rather linear combinations of our data constituting our principal components. We will be reviewing the importance of these principal components in our top performing models in the form of model coefficients. The models whose attributes will be reviewed are our Random Forest Classificatino model and KNN KDTree Regression model.\n",
    "\n",
    "Another facet of attribute importance we will discuss is that of PCA loadings. As mentioned when performing our in depth review of PCA for both prediction tasks, loadings are a means by which we may understand the underlying factors each principal component represents.\n",
    "\n",
    "These things being said, this section consists of four subsections:\n",
    "\n",
    "1. Classification PCA Importance\n",
    "2. Classification PCA Loadings\n",
    "3. Regression PCA Importance\n",
    "4. Regression PCA Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Importance\n",
    "\n",
    "When performing our detailed PCA for our Customer/Subscriber classifications earlier, we identified the first 14 components as being all that were necessary to describe 80% of the variance in the classification data set. When generating the random forest model using these 14 principal components as inputs, we generated feature importance values by which our components may be deemed weighted. The larger the value, the greater the component's influence on model response.\n",
    "\n",
    "The feature importance values for each of our principal components are plotted in the horizontal bar chart below. It is immediately apparent that principal components 1 and 3 are weighted heaviest at about 0.165 and 0.29 respectively. Next largest values are those of principal components 10 (~0.125) and 11 (~0.08). So, among the top four most important features in order are PC3, PC1, PC10, and PC11. The ordering if most important to least important continues with largest to smallest feature importance values. While reviewing our random forest input variables' (principal components in this case) coefficient values is helpful, it does not help us contextually understand the significance of our original attributes. For this reason, it is important to follow up with a review and interpretation of our most important principal components' loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=10) \n",
    "matplotlib.rc('ytick', labelsize=10)\n",
    "\n",
    "idx = ['PC'+str(i) for i in range(1,15)]\n",
    "\n",
    "plt.barh(range(len(rfc_clf.feature_importances_)), rfc_clf.feature_importances_, color = 'DarkTurquoise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Loadings\n",
    "\n",
    "Now at the individual principal component level, each classification principal component has 89 loadings which may be esteemed as weights or coefficients representing each original attribute. These loadings represent to what extent each attribute fabricates a given principal component, and the relationship between these attributes in context of the principal component under review. Another perspective is that each principal component is describing an underlying factor which is comprised of the heaviest loadings.\n",
    "\n",
    "Rather than discuss all 14 principal components, we will instead focus on the four principal components with the largest positive coefficients (PC3, PC1, PC10, and PC11) as described in the feature importance plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 3 - Early Birds, Saturdays, and General Travel Days/Times *\n",
    "\n",
    "The 3rd principal component, and component with the largest positive coefficient in the logistic regression model, seems to mostly describe the relationship between day of the week and time of the day and how it relates to rider type. Given PC3's large positive coefficient value in the model, it is interesting to see the negative correlation between Saturday rides (-0.58) and Morning rides (0.58). We could hypothesize that if riders are spending their time riding during the morning hours (5am-10am), they are commuting more for work than they are weekend joy riding. Supporting this arguement are the modest positive loadings for the weekdays and the negative loading for Sunday as well. Interestingly enough however, evening, midday, and afternoon rides are also negatively loaded. While more midday and afternoon riding among Customer vs. Subscribers might be expected, the negative loading for evening riding is a bit of a surprise. It would be good to keep an eye on this attribute's loading in the other principal components as well.\n",
    "\n",
    "Note that while the general signs are arbitrary from one principal component to the next, the sign relationship between attribute loadings within the same principal component are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "weightsplot = pd.Series(pca_class.components_[2], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 1 - Peak vs. Non Peak Commute Times*\n",
    "\n",
    "The first principal component appears to describe the relationships between peak vs. non-peak ride times when people would likely be riding for commutes between work and home vs. leisure rides midday, afternoon, and on the weekends. Specifically, evening rides have a very high loading value around 0.83 while midday and afternoon have values around -0.36 and -0.35 respectively. While the signs in general are not important from one principal component to the next, the sign relationship between attribute loadings within a component are important. In the case of PC1 it would seem that midday and afternoon ride times are negatively correlated with evening ride times. This is an interesting but expected relationship that would suggest midday, afternoon, and evening ride behaviors (the original TimeOfDay attribute in general really) are important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[0], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 10 - Environmental Conditions and the Days of the Week / Time of Day*\n",
    "\n",
    "We admit we were excited to see the influence of weather on Customer/Subscriber classifications via PC10. Min, max, and average temperatures appear to play a significant role in determining who is a Subscriber and who is only a Customer (temperature loadings approximately -0.50 to -0.51). Not only are the strong negative temperature loadings interesting, but they are negatively correlated with Saturday and Sunday activity (~0.15 and 0.205 respectively). This might suggest that, for example, as the temperature outside increases, subscriber activity decreases on the weekends but increases weekdays (except Fridays). Not only that but night time Subscriber activity (loading of approximately -0.16) increases as well. Remember, the overall signs are arbitrary while signed relationships between attributes are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[9], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DeepSkyBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 11 - Location*\n",
    "\n",
    "Our team was also happy to see that our observations from Lab 1 visualizations were correct regarding start and end station locations given the loadings of the 11th principal component. This component clearly portrays very large loadings for start and end station latitudes in excess of -0.6 with the next largest loadings being for start/end station longitudes (loadings around -0.25). This means that riders who are subscribers, for example, are more likely to use stations with larger latitudes (further north in the region) and more positive longitudes (further east in the region) than Customers. Based on what we know about the mid and lower Manhattan areas of NYC and what we observed during Lab 1, we can hypothesize that location is indeed important because of the reasons driving riders to use the CitiBike system. In Lab 1, our heatmaps indicated that customer activity wasn't really centralized to any particular region besides a small concentration at the Central Park Zoo station, whereas subscribers exhibited heavy concentrations throughout the mid and Northeast areas of the region. Notable also is that individual stations also exhibit positive and negative loadings corresponding well with the latitude and longitude loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[10], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'SlateBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize, we believe our most important attributes, in order, for classification are those related to day of the week, time of day, weather, and location for the reasons described previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression PCA Loadings\n",
    "\n",
    "Again, 15 principal components were selected for use in our regression models and KDTree KNN Regression proved to be the best model using these components. Unlike the feature importance capabilities we had with random forest classification, however, KDTree KNN Regression's features were treated equally. This is because we instructed the model to deploy *uniform* weighting, which is also the default for the KNeighborsClassifier scikit-learn function. For this reason, we have opted to describe weightings for the first four principal components that explain the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 1 - Subscribers vs. Customers*\n",
    "\n",
    "The very first principal component for regression modeling highlights the contrasts between Subscribers and Customers. Subscribers have depict a loading of ~0.69 while Customers have show a loading of approximately -0.68. In addition to these weightings, we also see some of the characteristics associated with the two classes as was highlighted during classification important attribute review (i.e. Subscribers riding more weekdays during rush hour and Customers riding more weekends during non-peak hours and when the weather is warmer. We could hypothesize, therefore, that the riding behaviors of each user type are different enough to result in differing trip durations, which also supports our Lab 1 violin plots depicting the different trip durations by user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[0], index=reg_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 2 - Time of Day*\n",
    "\n",
    "The second principal component appears to encapsulate the effects of ride time of day on trip duration. For example, the evening ride attribute boasts a loading of ~0.83 while midday and afternoon rides exhibit loadings of approximately -0.35 and -0.31 respectively. Given what we already know about rider behaviors when classifying Subscribers/Customers, we may argue this supports PC1 since it represents a difference in trip duration based on rider class trip times. Ultimately, we may hypothesize that if a rider is traveling during the evening, likely when returning home from work, he/she is spending less time on the bike while taking the most direct route possible in contrast to a rider riding during non-peak hours who might be vacationing or otherwise sightseeing, therefore taking a more leisurely ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[1], index=reg_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 3 - Lunch Time vs. Leisure Time*\n",
    "\n",
    "Regression PCA component 3 describes a very interesting relationship. Contrasting one another nearly perfectly are loadings for afternoon rides (~0.7) and midday rides (approximately -0.7). The fact that these times of day are so well contrasted since other attributes exhibit little to no loadings is rather perplexing. One might hypothesize that riders riding during lunch hours are more likely to be taking short trips to nearby restaurants or food stands before returning to work whereas riders riding during the afternoon are more likely to be taking longer, leisurely trips while sightseeing. This isn't to say that individuals who embark on trips during lunch don't take longer rides, but the majority of riders during that time are riding with a work schedule in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[2], index=reg_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 4 - Leisure vs. Purpose-Centered Weekend Rides*\n",
    "\n",
    "PC4 is unique in that it requires a respectable amount of domain knowledge to interpret correctly. Though we are not New Yorkers, we will do our best. The most notable loadings are Saturday's ~0.815 and Sunday's approximately -0.43 loadings. These are in direct contrast to one another but both occur on the weekend. Noteworthy is also the fact that morning riding (approximately -0.20) is positively correlated with Sunday riding. This really is a difficult underlying factor to dissect. Could it potentially be that certain attractions are open on Saturday but not Sunday, or vice-versa, or that there are other confounding variables for which we are unaware of that influence such contrasts? This component is rather a mystery to us without additional domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[3], index=reg_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize, we believe our most important attributes, in order, for regression are those related to user type, time of day, and day of the week for the reasons described previously. Of course, reviewing the remaining 11 principal components would provide further insight but due to work scale, we will stick to these first four PCs for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "#### How useful is the model for interested parties?\n",
    "\n",
    "##### Classification\n",
    "**How is this model useful?** Citibike aims to increase rider subscriptions, but may fear adverts to the wrong people could keep customers from enjoying their services. Our classification model predicting usertype as customer or subscriber allows Citibike to identify those customers who meet the criteria for common subscribing members. The Random Forest Classifier model produced an overall accuracy rating of 72.496 %, and although this may not be extremely optimal for True Positive matches, it does provide us with enough information to make educated decisions on which customers may already be considering converting to a subscribing member. These False Positive Customers, may be targeted leaving all others alone, reducing risk of losing customer traffic in general. \n",
    "\n",
    "**How would this model be deployed?** This model has the ability to be deployed as real-time predictions, or as a periodic corporate marketing alert if customer email addresses are readily available. Our preference would be a real-time prediction as a customer returns a bike. Upon return, if the model suggests a customer contains subcribing tendencies email alerts, pop-up promotions,etc. may be deployed in order to gain the customer's attention towards subscriber offerings. Alternatively, the marketing team could receive this information periodically, and implement custom strategies based on industry best practice techniques to this target group. This group of individuals are likely more apt to acknowledge these tactics positively, since their usage tendencies already align more closely to that of a subcribing member.\n",
    "\n",
    "**How often would the model need to be updated?** This model would definitely need to be updated periodically. As the citibike Share service grows, and new locations arrive, the model will need to be updated to account for the new sites. Also, as the population in NYC shifts over time(new businesses, schools, residential, etc.), trends may also fluctuate. These fluctuations will need to be accounted for in the model regularly to keep it up to date with current state NYC. Our recommendation for these updates would be periodic (monthly or quarterly) model fit updates in CitiBike systems to account for these possible changes.\n",
    "\n",
    "\n",
    "##### Regression\n",
    "**How is this model useful?** One of the known issues with CitiBike rentals is a problem of availability. With this model we can attempt to predict how long a bike would be unavailable once it's rented. With the known features and classifiers, it'd be possible to proactively predict bike availability during the day. Even with a low goodness of fit, the low MSE provided by the model results in the ability to allow CitiBike to anticipate the trip duration for each rental. At the end of the predicted time, it could be assumed by the station that the bike had been relocated to another station (updating otherwise should the bike be returned).\n",
    "\n",
    "**How would this model be deployed?** We built our model using only information that would be available to a station at the point at which a rental is made. Our deployment then, would be best used by the station itself to predict, once the bike is removed, how long the bike would be unavailable. If it reaches a point where a defined percentage of its bikes are calculated to be unavailable all at the same time, the station would be able to alert CitiBike and arrangements could be made to either resupply the station or, as we might suggest, send out automated messages to subscribers noting the station as a \"high reward\" station for returning bikes. This would give CitiBike hours of notice rather than minutes in the even of a high-activity day.\n",
    "\n",
    "**How often would the model need to be updated?** As the data set itself is constantly updating, the model could be updated by each station or as an aggregate of all stations in a remote database. Because it's a simple enough model, it could be run on a schedule depending upon the number of observations and the available processing power to stay up to day with current trends. Like the classification models, this would also need to be updated periodically, but likely at a much higher rate, possibly weekly, with less emphasis on historical trends. \n",
    "\n",
    "##### Additional Data to Collect:\n",
    "* **Event/Restaurant/Retail Data:** Given that we have detailed geocoordinate data and have already demonstrated powerful use of the Google Maps API, it would be possible to incorporate location details surrounding Citi Bike start and stop locations. There is potential for such data to be gathered automatically using API's such as Google's. Having this data would provide further insight into some of the reasons some bike share locations are more popular than others. Such data could even help Citi Bike predict changes in station demand based on changing surroundings and help determine where new stations should be installed.\n",
    "* **Special Events:** Similar to the previous idea, merging other public data based on geophysical coordinates and timeline could introduce other external factors such as the effects of parades, public concerts, festivals, and other events on ride activity for a given day or week. This would help identify/predict abnormal activity in this and future data sets. Additionally, it would provide insight to Citi Bike as to how to better plan and prepare for such events to boost rental count and increase trip duration.\n",
    "* **GPS Enabled Bike Computers:** Though not influenced by the data we have at hand, adding bicycle tracking hardware to each Citi Bike rental would provide substantial value to future data sets. Adding GPS tracking would enable Citi Bike to track specific routes followed by clients and could even aid NYC planners with transportation projects. Having route information means that true distance covered would be available, an attribute that would have far more meaning than our LinearDistance attribute. Incorporating GPS tracking with bike speed would provide insights into individual rider activity. For example, just because a rider's trip duration was 6 hours doesn't mean they actively rode for that amount of time. It is far more likely such a rider would have stopped for an extended period of time at least once during this period of time. Adding GPS and speed data would alleviate these existing gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "This lab was the most challenging and time consuming yet, partly because we chose to implement PCA in order to reduce the dimensionality of our data set and partly because we chose to explore new model types such as Neural Networks MLP Regression for trip duration prediction. The additional research and work put toward these records seemed to balloon at times, especially when considering MLP interpretations and ranking attributes through the haze of PCA's linear combinations. We hope, however, that our efforts did not go unnoticed given the added time we spent ensuring we utilized and interpreted these tools correctly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Py3]",
   "language": "python",
   "name": "conda-env-Py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
