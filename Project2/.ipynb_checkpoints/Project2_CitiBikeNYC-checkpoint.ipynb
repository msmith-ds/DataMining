{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1 - SVM & LR Classification -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 2/08/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for the mini-lab consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behaviour.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this mini-lab is to predict whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, etc.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve this goal (as described further in the sections below). By leveraging 80% of the samples as training data and 20% as test data via randomized selection, we expect to be able to derive a dependable and accurate user type prediction model for which accuracy and performance will be discussed in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Quality section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation still may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4881384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to eachother. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Logistic Regression(LR) Model\n",
    "\n",
    "xxxxxxxxx create model here xxxxxxxxx <br>\n",
    "xxxxxxxxx prove that variables included are best for the model xxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepping Data for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "Due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations). We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual binary columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}\n",
    "\n",
    "With these encodings complete, our final dataset to cross-validate on test/train datasets is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Test & Train Datasets\n",
    "*(Note: Below code adapted from MSDS 7331 Data Mining Git Repository Notebook 4)*\n",
    "\n",
    "With our final encoded dataset complete, we begin splitting the data into Test vs Train datasets. The test dataset size for this analysis is 20% of the original data (e.g. 100,000 records). We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results. By splitting the data into three separate Test vs Train datasets, we are able to cross validate results on different random samples.\n",
    "\n",
    "In our first approach to fitting our model, we decided to include all attributes - only excluding those attributes otherwise accommodated for by other binary encodings, time aggregates, or transformed values. \n",
    "\n",
    "This initial fit / prediction produced what at first was unexpected results, a {99.991, 99.995, 99.996} % Accuracy rating across the three prediction iterations!! We knew that there must be something abnormal occurring, so we dug into the attributes included in the model. Referencing back to our findings during exploratory data analysis, we know that customers did not provide all of the same demographic information as subscribing members. Due to this, the LR model is able to too easily discover whether the user is a customer or not based on these demographic attributes {Age, Birth Year, Gender}. Because of these missing demographic values, we are not able to include them in the model. We will continue to explore our LR model with modified input variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create function to create sample test/train datasets with inputs of:\n",
    "    ## original dataset, \n",
    "    ## explanatory dataset (removing appropriate columns not included in model), \n",
    "    ## random seed value\n",
    "def create_sample(Data, dropCols, seed):\n",
    "    if 'usertype' in Data:\n",
    "        y = Data['usertype'].values # get the labels we want\n",
    "        #del CitiBikeDataSampled_5050['usertype'] # get rid of the class label\n",
    "        X = Data.drop(dropCols, axis = 1).values\n",
    "        y = np.where(y == 'Subscriber', 1, 0)\n",
    "\n",
    "    # to use the cross validation object in scikit learn, we need to grab an instance\n",
    "    #    of the object and set it up. This object will be able to split our data into \n",
    "    #    training and testing splits\n",
    "    num_cv_iterations = 3\n",
    "    num_instances = len(y)\n",
    "    cv_object = ShuffleSplit(n=num_instances,\n",
    "                             n_iter=num_cv_iterations,\n",
    "                             test_size  = 0.2,\n",
    "                             random_state = seed)\n",
    "\n",
    "    return cv_object, X, y\n",
    "\n",
    "## Create function to Execute fit and prediction, cross validating 3 test/train combos\n",
    "    # print output for accuracy metrics\n",
    "    # return key attributes for last iteration for future use\n",
    "def test_LRModel(cv_object, X, y):\n",
    "\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "    iter_num=0\n",
    "\n",
    "    for iter_num, (train_indices, test_indices) in enumerate(cv_object):\n",
    "        lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "        y_hat = lr_clf.predict(X[test_indices]) # get test set preditions\n",
    "        \n",
    "        ytestnames = np.where(y[test_indices] == 1, 'Subscriber', 'Customer')\n",
    "        yhatnames  = np.where(y_hat == 1, 'Subscriber', 'Customer')\n",
    "        \n",
    "        # print the accuracy and confusion matrix \n",
    "        print(\"====Iteration\",iter_num,\" ====\")\n",
    "        print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "        print(\"confusion matrix\\n\", pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "        print(\"log_loss = \", log_loss(y[test_indices], y_hat))\n",
    "        print(\"roc_auc_score = \", roc_auc_score(y[test_indices], y_hat))\n",
    "        print(\"F-score = \", mt.f1_score(y[test_indices], y_hat))\n",
    "    \n",
    "    del cv_object\n",
    "    return lr_clf, X[train_indices],y[train_indices], X[test_indices], y[test_indices], y_hat\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define random seed value to re-use\n",
    "seed = len(CitiBikeDataSampled_5050)\n",
    "\n",
    "# build sample test/train with all variables minus time series, redundant station ids, \n",
    "    # trip duration, and original columns for encoded binary columns\n",
    "cv_object_All, X_All, y_All = create_sample(CitiBikeDataSampled_5050, \n",
    "                                            ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration'], \n",
    "                                            seed)\n",
    "print(cv_object_All)\n",
    "\n",
    "# Execute fit and prediction, cross validating 3 test/train combos\n",
    "    # print output for accuracy metrics\n",
    "    # return key attributes for last iteration for future use\n",
    "lr_clf_All, X_Train_All, y_Train_All, x_Test_All, y_Test_All, y_hat_All = test_LRModel(cv_object_All, X_All, y_All)\n",
    "\n",
    "del X_All, y_All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous model rendering such suspicous results and our recollection of missing demographic data, we have re-fitted our model after removing the below additional attributes:\n",
    "- Age\n",
    "- gender_0\n",
    "- gender_1\n",
    "- gender_2\n",
    "- birth year\n",
    "\n",
    "With these additional attributes removed, our accuracy ratings decreased to {74.526, 74.675, 74.725} across the three test / train iterations. Although the accuracy decreased, we feel the *value* of these results are much more substantial. Walking through Iteration 0 results we can see from the Confusion matrix that  of the 49,834 Customers in the test dataset, we correctly predicted 37,327. On the otherhand, we incorrectly predicted 12,507 Customers as Subscribers. These 12,507 Customers, which we think meet the criteria for typical Subscribers are the perfect targets for CitiBike promotions, discounts, adverts, and other means of trying to get them to start subscribing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# build sample test/train with all variables minus time series, redundant station ids, \n",
    "    # trip duration, original columns for encoded binary columns,\n",
    "    # AND Non-Customer provided data {Age, Birth Year, Gender}\n",
    "cv_object_RemABYG, X_RemABYG, y_RemABYG = create_sample(CitiBikeDataSampled_5050, \n",
    "                                                        ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], \n",
    "                                                        seed)\n",
    "print(cv_object_RemABYG)\n",
    "\n",
    "# Execute fit and prediction, cross validating 3 test/train combos\n",
    "    # print output for accuracy metrics\n",
    "    # return key attributes for last iteration for future use\n",
    "lr_clf_RemABYG, X_Train_RemABYG, y_Train_RemABYG, x_Test_RemABYG, y_Test_RemABYG, y_hat_RemABYG = test_LRModel(cv_object_RemABYG, X_RemABYG, y_RemABYG)\n",
    "\n",
    "del X_RemABYG, y_RemABYG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are fairly happy with the accuracy of the model once simply removing the missing demographic attributes, we want to ensure we are maximizing the efficiency of our model fit. We want to find a model which maximizes the Positive Subscriber matches in order to also maximize the number of correctly assigned false positive Customer predictions. To do this we have built the model fit various times by removing the below attributes in different combinations. \n",
    "- start_station_latitude\n",
    "- start_station_longitude\n",
    "- end_station_latitude\n",
    "- end_station_longitude\n",
    "- bikeid\n",
    "- LinearDistance\n",
    "\n",
    "For the purpose of this report, we have chosen to discuss the additional removal of LinearDistance and bikeid to interpret, however all combinations tried produced similar results. With an accuracy ratings of {0.73796, 0.74078, 0.73987}, this model fit produces less accurate overall results. Furthermore, when looking at values for logg_loss, roc_auc, and f-score statistics the previous model rendered a more effective model fit. Interested to maximize our positive Subscriber predictions, the previous model's iteration 0 Subscriber Positive predictions are at 74% compared to this model's 73% accuracy for Subscribers. These several facts lead to our conclusion to include all attributes *except* the missing value demographic attributes moving forward in our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# build sample test/train with all variables minus time series, redundant station ids, \n",
    "    # trip duration, original columns for encoded binary columns,\n",
    "    # AND Non-Customer provided data {Age, Birth Year, Gender}\n",
    "    # AND LinearDistance\n",
    "    # AND BikeID\n",
    "cv_object_RemABYG_LDBID, X_RemABYG_LDBID, y_RemABYG_LDBID = create_sample(CitiBikeDataSampled_5050, \n",
    "                                                                          ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year','LinearDistance', 'bikeid'], \n",
    "                                                                          seed)\n",
    "print(cv_object_RemABYG_LDBID)\n",
    "\n",
    "# Execute fit and prediction, cross validating 3 test/train combos\n",
    "    # print output for accuracy metrics\n",
    "    # return key attributes for last iteration for future use\n",
    "lr_clf_RemABYG_LDBID, X_Train_RemABYG_LDBID, y_Train_RemABYG_LDBID, x_Test_RemABYG_LDBID, y_Test_RemABYG_LDBID, y_hat_RemABYG_LDBID = test_LRModel(cv_object_RemABYG_LDBID, X_RemABYG_LDBID, y_RemABYG_LDBID)\n",
    "\n",
    "del X_RemABYG_LDBID, y_RemABYG_LDBID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our final model variables selected, we have one parameter in our LR fit to manipulate for accuracies. We have chosen to manipulate the cost variable within our logistic regression analyzing accuracies at {1.0, .01, .05, 5}. Calculations show that our original default of Cost = 1 produced the highest accuracy ratings in 2 out of 3 iterations. Due to this, we have chosen to leave our model alone, as was originally computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# build sample test/train with all variables minus time series, redundant station ids, \n",
    "    # trip duration, original columns for encoded binary columns,\n",
    "    # AND Non-Customer provided data {Age, Birth Year, Gender}\n",
    "cv_object_RemABYG, X_RemABYG, y_RemABYG = create_sample(CitiBikeDataSampled_5050, \n",
    "                                                        ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], \n",
    "                                                        seed)\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf, X_RemABYG, y=y_RemABYG, cv=cv_object_RemABYG) # this also can help with parallelism\n",
    "    return accuracies\n",
    "\n",
    "list = [] \n",
    "\n",
    "list.append(lr_explor(cost = 1.0))\n",
    "list.append(lr_explor(cost = .01))\n",
    "list.append(lr_explor(cost = .05))\n",
    "list.append(lr_explor(cost = 5))\n",
    "\n",
    "test = pd.DataFrame(pd.concat([pd.DataFrame(['Cost1.0', 'Cost0.01', 'Cost0.05', 'Cost5.0']),\n",
    "                               pd.DataFrame(list)], axis = 1).reindex())\n",
    "test.columns = ['Cost','Iteration 0', 'Iteration 1', 'Iteration 2']\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_RemABYG, y_RemABYG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Support Vector Machine(SVM) Model\n",
    "\n",
    "In addition to generating a logistic regression model to classify CitiBike rider types based on riding behaviors, we would also like to generate a support vector machine (SVM) model for classification comparison. By utilizing the same 80/20 training/test split samples used during logistic regression model generation, we expect to be able to produce model results very similar to those of logistic regression.\n",
    "\n",
    "To best compare SVM performance against logistic regression, we choose to generate SVM models for all three data set versions modeled in the logistic regression section above. As a reminder, these data sets are as follows: The first data set consists of all attributes; the second data set has Age, Birth Year, and Gender removed; and the last data set has Age, Birth Year, Gender, Linear Trip Distance, and Bike ID removed. Generating SVM models for all three data sets will allow us to compare the SVM and Logistic Regression approaches under various data set conditions.\n",
    "\n",
    "Initially, our team attempted to generate standard SVM models on the data sets. However, with 500,000 sample trips consisting of 105 attributes each (after removing unnecessary/duplicate parameters as was done for logistic regression), SVM model generation proved to be too time consuming. For this reason, stochastic gradient descent classification is pursued instead. An example cell block of our SVM model attempt for the first data set is shown below.\n",
    "\n",
    "*(Note that code within this section was adapted and modified from the MSDS 7331 Notebook 4 SVM tutorial)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create function to fit SVM model and output model accuracy, confusion matrix, and log-loss value.\n",
    "## Inputs consist of X and Y training data and X and Y test data.\n",
    "def SVMfit(X_Train, y_Train, x_Test, y_Test):\n",
    "    scl_obj = StandardScaler()\n",
    "    scl_obj.fit(X_Train)\n",
    "    \n",
    "    X_train_scaled = scl_obj.transform(X_Train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(x_Test)\n",
    "    \n",
    "    # Train the SVM model\n",
    "    svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "    svm_clf.fit(X_train_scaled, y_Train)  # train object\n",
    "    \n",
    "    y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "    \n",
    "    acc = mt.accuracy_score(y_Test,y_hat)\n",
    "    \n",
    "    ytestnames = np.where(y_Test == 1, 'Subscriber', 'Customer')\n",
    "    yhatnames  = np.where(y_hat == 1, 'Subscriber', 'Customer')\n",
    "    \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print('accuracy:', acc )\n",
    "    print(\"confusion matrix\\n\", pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "\n",
    "    return svm_clf\n",
    "\n",
    "## Commented out since regular SVM takes too long to run (14+ hours)\n",
    "#SVMfit(X_Train = X_Train_All, y_Train = y_Train_All, x_Test = x_Test_All, y_Test = y_Test_All)\n",
    "\n",
    "#SVMfit(X_Train = X_Train_RemABYG, y_Train = y_Train_RemABYG, x_Test = x_Test_RemABYG, y_Test = y_Test_RemABYG)\n",
    "\n",
    "#SVMfit(X_Train = X_Train_RemABYG_LDBID, y_Train = y_Train_RemABYG_LDBID, x_Test = x_Test_RemABYG_LDBID,\n",
    "#       y_Test = y_Test_RemABYG_LDBID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since stochastic gradient descent classification is to be used for each data set variation in lieu of SVM modeling in our case, the following function is created to simplify model generation. The function receives independent and dependent variable training and test data for a given data set, normalizes the data, fits it to a stochastic gradient classifier model, and then prints model accuracy, confusion matrix details, and the log-loss value. Notice a seed value is provided for repeatability purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Create function to fit SGDClassifier object and output model accuracy, confusion matrix, and log-loss value.\n",
    "## Inputs consist of X and Y training data and X and Y test data.\n",
    "def SGDfit(X_Train, y_Train, x_Test, y_Test):\n",
    "    regularize_const = 0.1\n",
    "    seed = len(CitiBikeDataSampled_5050)\n",
    "    svm_sgd = SGDClassifier(alpha=regularize_const, l1_ratio=0.0, n_jobs=-1, random_state = seed)\n",
    "\n",
    "    scl = StandardScaler()\n",
    "\n",
    "    svm_sgd.fit(scl.fit_transform(X_Train), y_Train)\n",
    "    yhat = svm_sgd.predict(scl.transform(x_Test))\n",
    "    \n",
    "    acc = mt.accuracy_score(y_Test,yhat)\n",
    "    \n",
    "    ytestnames = np.where(y_Test == 1, 'Subscriber', 'Customer')\n",
    "    yhatnames  = np.where(yhat == 1, 'Subscriber', 'Customer')\n",
    "    \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print('SVM:', acc)\n",
    "    print(\"confusion matrix\\n\", pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "    print(\"log_loss = \", log_loss(y_Test, yhat))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first of the SVM models trained using stochastic gradient descent is that of the first data set in which \"all\" attributes are present. As was observed when generating the data set's logistic regression model above, the SVM accuracy is nearly perfect with accuracy ratings of {99.998, 99.996, 99.995} % across the three test / train iterations due to the included Age, Birth Year, and Gender attributes. Again, even though this appears to be a fantastic model and the log-loss value is extremely low {0.0007, 0.0014, 0.0017}, we know this model is invalid based on our domain knowledge, or data set understanding. These three demographic attributes are effectively masking all other attributes only because birth year and gender data are typically missing for customers but included for subscribers. In other words, the SVM model learns to identify riders with an age of 0 years or unknown gender as customers (Note Age/BirthYear/Gender data is present for 5 Customers only, which accounts for the {2, 4, 5} mis-classified riders). Removing these \"give-away\" attributes will produce a realistic model that is not skewed by missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cv_object_All, X_All, y_All = create_sample(CitiBikeDataSampled_5050, \n",
    "                                            ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration'], \n",
    "                                            seed)\n",
    "print(cv_object_All)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object_All):\n",
    "    SGDfit(X_Train = X_All[train_indices], y_Train = y_All[train_indices],\n",
    "           x_Test = X_All[test_indices], y_Test = y_All[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second SVM model generated using stochastic gradient descent is for the data set with Age, Birth Year, and Gender attributes removed. As was observed during logistic regression generation, model accuracy is still strong but far more realistic values of {74.121, 74.339, 74.44} % across the three test / train iterations are given now that we've accounted for misleading attributes. Walking through Iteration 0 results, 49,834 Customers in the data set, 37,548 were correctly identified but 12,286 were incorrectly classified as Subscribers. Similarly, of the 50,166 Subscribers, 36,573 were correctly identified as such but 13,593 were incorrectly classified as Customers. The log-loss produced is modestly low at {8.94, 8.86, 8.83}. This is of course much larger than the model's above, but, again, the previous model was invalid since Age, Birth Year, and Gender attributes were included. This model is much better representative of rider attributes while ignoring attributes containing missing data unique to the Customer user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cv_object_RemABYG, X_RemABYG, y_RemABYG = create_sample(CitiBikeDataSampled_5050, \n",
    "                                                        ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], \n",
    "                                                        seed)\n",
    "print(cv_object_RemABYG)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object_RemABYG):\n",
    "    SGDfit(X_Train = X_RemABYG[train_indices], y_Train = y_RemABYG[train_indices],\n",
    "           x_Test = X_RemABYG[test_indices], y_Test = y_RemABYG[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final SVM model generated using stochastic gradient descent is for the data set with Age, Birth Year, Gender, Trip Linear Distance, and Bike ID removed. These latter two attributes were removed because they were deemed potentially insignficant when classifying user types. Surprisingly, model accuracy decreases slightly ({73.728, 73.974, 74.002} % accurate) when removing these last two attributes. The confusion matrix in iteration 0 indicates that while 37,594 riders were correctly identified as Customers, 12,240 Customers were incorrectly identified as Subscribers; similarly 36,134 Subscribers were correctly identified as such but 14,032 Subscribers were mis-classified as Customers. Log-loss is slightly higher {9.07, 8.99, 8.98} than the model in which only Age, Birth Year, and Gender were removed. This suggests that Trip Linear Distance and Bike ID may have a stronger correlation with user type than expected otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cv_object_RemABYG_LDBID, X_RemABYG_LDBID, y_RemABYG_LDBID = create_sample(CitiBikeDataSampled_5050, \n",
    "                                                                          ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year','LinearDistance', 'bikeid'], \n",
    "                                                                          seed)\n",
    "print(cv_object_RemABYG_LDBID)\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object_RemABYG_LDBID):\n",
    "    SGDfit(X_Train = X_RemABYG_LDBID[train_indices], y_Train = y_RemABYG_LDBID[train_indices],\n",
    "           x_Test = X_RemABYG_LDBID[test_indices], y_Test = y_RemABYG_LDBID[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the three SVM models, the data set with only Age, Birth Year, and Gender attributes removed appears to be the best fit due to its higher *valid* accuracy and lowest *valid* log-loss value. This aligns with our findings when classifying user type using logistic regression in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pros and Cons of LR vs. SVM\n",
    "Both Logisitic Regression and Support Vector Machines present advantages as well as disadvantages. As might be expected, which approach is best depends on one's data set and problem statement. Our use of both model types with the same data sample set allows us to break down these pros and cons in detail and determine which produces a better fit given our situation. With this in mind, each model type's general advantages and disadvantages are outlined and discussed below.\n",
    "\n",
    "**General Advantages**\n",
    "\n",
    "*Logisitic Regression*\n",
    "- *Simplicity* - The logistic regression model is built in such a way that independent variables are plotted along an x-axis and single-class classification is represented by 0 or 1 on the y-axis. The logit and cost functions are tuned to accurately fit a logistic curve to the data on this x-y plane. By evaluating P(X) via the logit function, we are able to easily determine whether the class is 0 (Customer), in cases when P < 0.5, or 1 (Subscriber), in cases in which P > 0.5. In other words, logistic regression fits all data points as though they were positioned along a continuous function.\n",
    "\n",
    "- *Small Dimensionality Applications* - Logistic regression works well when there are a smaller number of dimensions and when these attributes do not necessarily determine the response without considering probability of response.\n",
    "\n",
    "*SVM*\n",
    "- *Flexible* - Since SVM is capable of fitting a hyperplane among an infinite number of dimensions to create a boundary between classes of various data points, it allows for seperation of data from varying classes even when data sets are very large or contain many attributes.\n",
    "\n",
    "- *Robust* - In general, SVM is more robust against outliers since it only considers points near the margin as support vectors.\n",
    "\n",
    "- *Speed (SVM training with Stochastic Gradient Descent)* - Because stochastic gradient descent processes the data in batches and is less precise in it's batch-by-batch approach to deepest descent but still arrives there nonetheless in the end, it is significantly faster than logistic regression and SVM on its own (less resource instensive).\n",
    "\n",
    "**General Disadvantages**\n",
    "\n",
    "*Logistic Regression*\n",
    "- *Speed* - Logistic regression, though faster than straightforward SVM for our data set, is slower in comparison to training a classfier model using stochastic gradient descent.\n",
    "\n",
    "- *Sensitivity* - In cases when P = ~0.5, logistic regression has difficulty determining classification.\n",
    "\n",
    "*SVM*\n",
    "- *Speed (SVM by itself)* - Fitting a SVM model on its own was very slow for our data set. Even after 14 hours of run-time on a quad-core i7 workstation, not even a single model was completely generated.\n",
    "\n",
    "**The Winner**\n",
    "\n",
    "Given the general advantages and disadvantages above, we'd now like to discuss both model approaches with respect to our data. These discussions are centered around the data set version in which only Age, Birth Year, and Gender are removed from the originally modeled set. As mentioned above, linear SVM training with stochastic gradient descent for 3x iterations was much faster for our selected data set (~XXX seconds) vs. native logistic regression training (~2 min 30s). This time difference is expected to increase exponentially as the number of rides analyzed increases, giving SVM with stochastic gradient descent the upper edge. <span style=\"color:green\">*+1 for SVM with stochastic gradient descent*</span>\n",
    "\n",
    "On the other hand, however, logistic regression produced accuracy results of {74.52, 74.49, 74.73} % across the three test/train iterations whereas SVM with stochastic gradient descent exhibited accuracy results of {74.121, 74.339, 74.44} %. This slight increase in accuracy among logistic regression models is an interesting observation, especially given logistic regression's added simplicity vs. SVM modelling. A second measure of accuracy employed throughout this analysis has been log-loss. When comparing log-loss among both model types, we expect log-loss values to correspond with model accuracy measures also. As such, logistic regression log-loss values were {8.80, 8.75, 8.73} for each of the three iterations whereas SVM with stochastic gradient values were {8.94, 8.86, 8.83}. As expected, these results correlate with the accuracy scores discussed. <span style=\"color:blue\">*+1 for logistic regression*</span>\n",
    "\n",
    "The final comparison worth further discussion is that of efficiency. As mentioned in the general advantages and disadvantages above, logistic regression consists of a simpler model than SVM, especially when factoring in stochastic gradient descent to SVM model training. Logistic regression classifies our CitiBike user types by optimizing the log likelihood function; SVM tries to maximize the margin between the closest data points of two classification regions. The fact that stochastic gradient descent is required given our data set dimensions and large number of observations means the model generation process may be deemed less efficient. In the end, the principles of Occam's Razor apply. Therefore, logistic regression without stochastic gradient descent takes the win for our data set.<span style=\"color:blue\">*+1 for logistic regression*</span>\n",
    "\n",
    "Given its increased accuracy and efficiency/simplicity, we rate logistic regression as being the better model for user type classification using our sample data set of 500,000 rides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Feature Weightings Explained\n",
    "\n",
    "With our model settled upon for our Logistic Regression, we need to explore the weights associated with each feature to get an idea of how each component impacts the ultimate prediction. Looking at our logistic model directly, we found, interestingly enough, that the highest weight was placed on the latitude of the station where renters began their trips. Having a high positive weight means that subscribers were more likely located at lower latitudes, but based on our heat maps, this seemed to contradict our initial data exploration. Moreover, the next highest weight in terms of magnitude was a specific station, and while it appeared in the top ten percent of our most visited stations, it's odd that it would be so important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "lr_weights_raw = lr_clf_RemABYG.coef_.T # take transpose to make a column vector\n",
    "lr_variable_names = CitiBikeDataSampled_5050.drop(['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], axis = 1).columns\n",
    "lr_weights = zip(lr_weights_raw,lr_variable_names)\n",
    "\n",
    "lr_weights_top_15_sorted = sorted(lr_weights, key=lambda x: abs(x[0]), reverse=True)[:15]\n",
    "\n",
    "lr_weights_top_15_sorted_coef = []\n",
    "lr_weights_top_15_sorted_names = []\n",
    "for coef, name in lr_weights_top_15_sorted:\n",
    "    lr_weights_top_15_sorted_coef.append(coef[0])\n",
    "    lr_weights_top_15_sorted_names.append(name)\n",
    "\n",
    "weightsplot = pd.Series(lr_clf_RemABYG.coef_[0], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "\n",
    "top_15_weights_plot = pd.Series(lr_weights_top_15_sorted_coef, index=lr_weights_top_15_sorted_names)\n",
    "top_15_weights_plot.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#for coef, name in zip(lr_weights_raw,lr_variable_names):\n",
    "#    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing our dataset once more, we realized that it was likely our weights were compromised due to the fact that we were dealing with a mix of values and quantities that weren't directly comparable. To get a better understanding of how these features were interacting, we decided to attempt to normalize the features, improving our ability to compare these features across their vastly different scales.\n",
    "\n",
    "This required a refit using the sklearn Standard Scaler training object, but because we're only concerned with the weights and their interactions, we can easily reuse the sets defined in our ShuffleSplit earlier in our regression. However, because we're dealing with scaled and normalized values, we'll use a lower cost value, in this case 0.05 to reflect the expected decrease in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_Train_RemABYG)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_Train_RemABYG)\n",
    "X_test_scaled = scl_obj.transform(x_Test_RemABYG)\n",
    "\n",
    "scalar_lr_clf = LogisticRegression(penalty='l2', C=0.05) # Earlier this cost was 1.0, because we've normalized the dataset, we now can use a lower cost\n",
    "scalar_lr_clf.fit(X_train_scaled,y_Train_RemABYG)\n",
    "\n",
    "scalar_y_hat = scalar_lr_clf.predict(X_test_scaled)\n",
    "\n",
    "scalar_acc = mt.accuracy_score(y_Test_RemABYG, y_hat_RemABYG)\n",
    "scalar_conf = mt.confusion_matrix(y_Test_RemABYG, y_hat_RemABYG)\n",
    "print('accuracy:', scalar_acc)\n",
    "print(scalar_conf)\n",
    "\n",
    "\n",
    "scalar_weights = zip(scalar_lr_clf.coef_[0],lr_variable_names)\n",
    "\n",
    "scalar_weights_top_15_sorted = sorted(scalar_weights, key=lambda x: abs(x[0]), reverse=True)[:15]\n",
    "\n",
    "scalar_weights_top_15_sorted_coef = []\n",
    "scalar_weights_top_15_sorted_names = []\n",
    "for coef, name in scalar_weights_top_15_sorted:\n",
    "    scalar_weights_top_15_sorted_coef.append(coef)\n",
    "    scalar_weights_top_15_sorted_names.append(name)\n",
    "\n",
    "scalar_weights = pd.Series(scalar_lr_clf.coef_[0],index=lr_variable_names)\n",
    "scalar_weights.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The accuracy and prediction matrix closely resembles our earlier regression and SVM models, meaning that the act of normalization did not greatly impact our model, however it did make understanding the weights much easier and, surprisingly, closely resembled some of our previous hypotheses we posited during our inital data gathering.\n",
    "\n",
    "Perhaps one unique component of these new weights is that the log transform of the trip duration has a negative weight, nearly 1, with the subscriber class. This implies that subscribers spend very little time on their trip, despite the linear distance between stations having a high positive weight. These two features seem to correlate with our earlier observations that subscribers were typically seen commuting rather than touring using the bikes.\n",
    "\n",
    "Another interesting aspect is the temperature component created after merging Citi Bike rental data with historical weather data. Both the maximum temperature and the minimum temperature have a negative weight with the subscriber class, however the average temperature has a positive weight. If subscribers are commuting, then it might make sense that as the average temperature for the day rises, they're much more likely to make use of their subscription. As it's possible they commute regardless of weather, extreme weathers are not as likely to impact their riding habits and behaviors.\n",
    "\n",
    "Finally, a few more quick observations based entirely on weight - saturday and sunday rentals both have negative weights in relationship to subscribers, further building upon the idea that subscribers are also generally commuters, and mornings also have a positive weight in regards to subscriber use. This might mean that subscribers are more likely to ride earlier in the morning on weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "\n",
    "scalar_top_15_weights_plot = pd.Series(scalar_weights_top_15_sorted_coef, index=scalar_weights_top_15_sorted_names)\n",
    "scalar_top_15_weights_plot.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "#zip_vars = zip(scalar_lr_clf.coef_.T,lr_variable_names) # combine attributes\n",
    "#zip_vars = sorted(zip_vars)\n",
    "\n",
    "#for coef, name in zip_vars:\n",
    "#    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors Explained\n",
    "\n",
    "Due to the performance issues of the SVM model without SGD, we were unable to capture support vector coefficients previously. In order to do so, we have sub-sampled our 50/50 dataset down to 10,000 observations. We were once again cognizant of our Customer vs. Subscriber distribution in our sample, and ensured to keep our 50/50 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 10000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050_sub = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050_sub))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050_sub.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%', figsize = (8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our sub-sampled dataset, we are able to Train and Test our SVM model on our dataset. We have chosen to run SVM model on the dataset version with Age, Birth Year, and Gender removed, which brought out the strongest model in both LR and SGD previously. This model fit, although running on a much smaller dataset, resulted in very similar results to our previous executions. With accuracy ratings of {73.1, 7.47, 74} %, we would once again be able to gain value from our false positive customer matches - introducing promotions, discounts and other ways of gaining subscriber traction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# build sample test/train with all variables minus time series, redundant station ids, \n",
    "    # trip duration, original columns for encoded binary columns,\n",
    "    # AND Non-Customer provided data {Age, Birth Year, Gender}\n",
    "cv_object_RemABYG, X_RemABYG, y_RemABYG = create_sample(CitiBikeDataSampled_5050_sub, \n",
    "                                                        ['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], \n",
    "                                                        seed)\n",
    "print(cv_object_RemABYG)\n",
    "\n",
    "iter_num = 0\n",
    "svm_clf = 0\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object_RemABYG):\n",
    "    svm_clf = SVMfit(X_Train = X_RemABYG[train_indices], y_Train = y_RemABYG[train_indices],\n",
    "                               x_Test = X_RemABYG[test_indices], y_Test = y_RemABYG[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of our support vectors in the last iteration (2) of the SVM test, we see that the original train dataset of 8,000 observations has been reduced to a total of 4,679 support vectors, utilizing 89 features in the model. This constitutes a 41.5% reduction in observations to support vectors supporting our model. Of those 4,679 support vectors, 2341 of them represent Customer Vector Indices wheras 2,338 of them represent Subscriber Vector Indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our SVC model fit in place and support vectors identified, we are able to analyze the weightings of support vector coefficients. Plotting these values, we can see the full spectrum of positive/negative weights on each of the 89 attributes included in the model. To take a closer look at the coefficients with the largest magnitude, we zoom in this plot to the top 15 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "# print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=CitiBikeDataSampled_5050_sub.drop(['starttime','stoptime',\n",
    "                                                                              'start_station_id','start_station_name',\n",
    "                                                                              'end_station_id','end_station_name',\n",
    "                                                                              'usertype','gender','DayOfWeek',\n",
    "                                                                              'TimeOfDay','tripduration','Age',\n",
    "                                                                              'gender_0','gender_1','gender_2',\n",
    "                                                                              'birth year'], axis = 1).columns)\n",
    "weights.plot(kind = 'bar', figsize = (12, 6), color = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was expected, we see quite a bit of overlap between our top SVM coefficients and what was found previously in our LR scaled coefficient weightings. TripDurationLog has the strongest impact on determining usertype with a negative weighting, while Linear Distance once again remains positive. This continues to support our theories that Subscribers utilize CitiBike services for commute trips creating shorter rides that travel further. It is likely that Customers utilize the bike service to visit attractions, and then return back near the same station where their car is parked. Similarly, our time segments show consistencies across the two model types, depicting morning start times as common for subscribing members whereas midday/afternoon rides aid to represent customers. Temperature attributes for Minimum and Maximum temperatures also agree with previous LR weightings, depicting higher min/max temperatures as indicators for customer users. Interestingly, average temperature does not show up in our top 15 weights for SVM. Finally, only 2 of the 4 stations match between the two models types.\n",
    "\n",
    "Perhaps, some of these differences could be due to the significant difference in sample size. In order to analyze SVM support vectors, we had to reduce our original sample of 500,000 by 98%, a 10,000 sample size, due to computational load of the model fit. This reduction in sample size results in analyzing a mere 0.18% of the original dataset collected. There is a good chance that important trends in the data are unrecognizeable since we have eliminated so much. It would be interesting to see, given the time and computational resourses, how this SVM model would compare if executed successfully on the full 500,000 sample performed on LR and SGD analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "\n",
    "weights_top15 = abs(weights).sort_values(ascending = False).head(15)\n",
    "\n",
    "weights_top15 = weights[weights_top15.index]\n",
    "\n",
    "weights_top15.plot(kind = 'bar', figsize = (12, 6), color = 'y')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Py3]",
   "language": "python",
   "name": "conda-env-Py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
