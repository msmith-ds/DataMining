{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1 - SVM & LR Classification -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 2/08/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "xxxxxxxxx intro text here xxxxxxxxx \n",
    "xxxxxxxxx Discuss importance/value of usertype classification xxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Data to be loaded from source files by the same conventions used in Data Exploration and Analysis Lab 1 of this course. CSV Files of this sample data output are created and utilized for an optimized load of this notebook. The one difference between compilation is the distribution of Customer vs. Subscriber observations. Given the nature of this analysis, we would like to see an even number of each usertype in the dataset rather than the 12.4% Customer vs. 87.6% Subscriber distribution. In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "If further insight is needed into how and/or why we compiled our sample dataset, please see the below two sources:\n",
    "\n",
    "- xxxxxxxxx link1 - GITHUB ipynb lab1 xxxxxxxxx \n",
    "- xxxxxxxxx link2 - GITHUB Project2 SampleDataLoad.py xxxxxxxxx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load & Merge Source Data / Build & Parse Sample Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ./FinalSampleDataCompile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load Final Sample Data from CSV***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Identify All CSV FileNames needing to be loaded\n",
    "path = r'Final Sampled Data'                     \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "    #Load from glob\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "CitiBikeData      = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "    # Convert Columns to Numerical Values\n",
    "CitiBikeData[['tripduration', 'birth year', 'LinearDistance','PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']]\\\n",
    "    = CitiBikeData[['tripduration', 'birth year','LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "    # Convert Columns to Date Values\n",
    "CitiBikeData[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeData[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "    # Convert Columns to Str Values\n",
    "CitiBikeData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeData[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag','gender']].astype(str)\n",
    "    \n",
    "display(CitiBikeData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, we have chosen to oversample the \"Customer\" usertype in our final sample dataset. To show this distribution, below are example counts & a pie chart representing the distribution of observations between Customer and Subscriber segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeData.groupby([\"usertype\"]).size()}).reset_index()\n",
    "\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n",
    "del UserTypeDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Logistic Regression(LR) Model\n",
    "\n",
    "xxxxxxxxx create model here xxxxxxxxx <br>\n",
    "xxxxxxxxx prove that variables included are best for the model xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Support Vector Machine(SVM) Model\n",
    "\n",
    "xxxxxxxxx create model here xxxxxxxxx <br>\n",
    "xxxxxxxxx prove that variables included are best for the model xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of LR vs. SVM\n",
    "\n",
    "xxxxxxxxx Adv. of each model xxxxxxxxx <br>\n",
    "xxxxxxxxx Disadv. of each model xxxxxxxxx <br>\n",
    "xxxxxxxxx superior model in terms of accuracy, training time/efficiency? xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Feature Weightings Explained\n",
    "\n",
    "xxxxxxxxx Discuss/interpret coefficients of each feature. xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors Explained\n",
    "\n",
    "xxxxxxxxx Do the support vectors provide any insight into the data? xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work\n",
    "\n",
    "xxxxxxxxx Explain Exc. Work Details / Challenges encountered here. xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
