{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1 - SVM & LR Classification -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 2/08/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "xxxxxxxxx intro text here xxxxxxxxx \n",
    "xxxxxxxxx Discuss importance/value of usertype classification xxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Quality section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation still may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4881384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to eachother. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove our theories on a 50/50 classification distribution, we have also computed a stratified sample to match the original data proportions. We will use this dataset for comparisons on accuracy measures in our analysis to make sure this was the correct approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 12.4 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 87.6 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_orig = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_orig))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_orig.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n",
    "    ## Delete Compiled full 5 million record set variable\n",
    "del CitiBikeDataCompiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Logistic Regression(LR) Model\n",
    "\n",
    "xxxxxxxxx create model here xxxxxxxxx <br>\n",
    "xxxxxxxxx prove that variables included are best for the model xxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepping Data for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "Due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations). We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Test & Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##scikit learn test/train code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Support Vector Machine(SVM) Model\n",
    "\n",
    "xxxxxxxxx create model here xxxxxxxxx <br>\n",
    "xxxxxxxxx prove that variables included are best for the model xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of LR vs. SVM\n",
    "\n",
    "xxxxxxxxx Adv. of each model xxxxxxxxx <br>\n",
    "xxxxxxxxx Disadv. of each model xxxxxxxxx <br>\n",
    "xxxxxxxxx superior model in terms of accuracy, training time/efficiency? xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Feature Weightings Explained\n",
    "\n",
    "xxxxxxxxx Discuss/interpret coefficients of each feature. xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors Explained\n",
    "\n",
    "xxxxxxxxx Do the support vectors provide any insight into the data? xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work\n",
    "\n",
    "xxxxxxxxx Explain Exc. Work Details / Challenges encountered here. xxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
