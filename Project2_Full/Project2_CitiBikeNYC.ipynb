{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Classification and Regression -- 2013/2014 CitiBike-NYC Data\n",
    "**Michael Smith, Alex Frye, Chris Boomhower ----- 3/08/2017**\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2_Full/Images/Citi-Bike.jpg?raw=true\" width=\"400\">\n",
    "\n",
    "<center>Image courtesy of http://newyorkeronthetown.com/, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "*** Describe the purpose of the model you are about to build ***\n",
    "\n",
    "The data set again selected by our group for Lab 2 consists of [Citi Bike trip history](https://www.citibikenyc.com/system-data) data collected and released by NYC Bike Share, LLC and Jersey Bike Share, LLC under Citi Bike's [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy). Citi Bike is America's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens, and Jersey City... 55 neighborhoods in all. As such, our data set's trip history includes all rental transactions conducted within the NYC Citi Bike system from July 1st, 2013 to February 28th, 2014. These transactions amount to 5,562,293 trips within this time frame. The original data set includes 15 attributes. In addition to these 15, our team was able to derive 15 more attributes for use in our classification efforts, some attributes of which are NYC weather data which come from [Carbon Dioxide Information Analysis Center (CDIAC)](http://cdiac.ornl.gov/cgi-bin/broker?_PROGRAM=prog.climsite_daily.sas&_SERVICE=default&id=305801&_DEBUG=0). These data are merged with the Citi Bike data to provide environmental insights into rider behavior.\n",
    "\n",
    "The trip data was collected via Citi Bike's check-in/check-out system among 330 of its stations in the NYC system as part of its transaction history log. While the non-publicized data likely includes further particulars such as rider payment details, the publicized data is anonymized to protect rider identity while simultaneously offering bike share transportation insights to urban developers, engineers, academics, statisticians, and other interested parties. The CDIAC data, however, was collected by the Department of Energy's Oak Ridge National Laboratory for research into global climate change. While basic weather conditions are recorded by CDIAC, as included in our fully merged data set, the organization also measures atmospheric carbon dioxide and other radiatively active gas levels to conduct their research efforts.\n",
    "\n",
    "Our team has taken particular interest in this data set as some of our team members enjoy both recreational and commute cycling. By combining basic weather data with Citi Bike's trip data, **our intent in this lab is to: 1) Predict whether riders are more likely to be (or become) Citi Bike subscribers based on ride environmental conditions, the day of the week for his/her trip, trip start and end locations, the general time of day (i.e. morning, midday, afternoon, evening, night) of his/her trip, his/her age and gender, etc., and 2) predict a rider's trip duration as a function of the aforementioned variables.** Due to the exhaustive number of observations in the original data set (5,562,293), a sample of 500,000 is selected to achieve these goals (as described further in the sections below). By leveraging Stratified 10-Fold Cross Validation, we expect to be able to derive dependable and accurate user type and ride duration prediction models for which accuracy and performance will be discussed in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "##### Compiling Multiple Data Sources\n",
    "To begin our analysis, we need to load the data from our source .csv files. Steps taken to pull data from the various source files are as follows:\n",
    "- For each file from CitiBike, we process each line appending manually computed columns [LinearDistance, DayOfWeek, TimeOfDay, & HolidayFlag]. \n",
    "- Similarly, we load our weather data .csv file.\n",
    "- With both source file variables gathered, we append the weather data to our CitiBike data by matching on the date.\n",
    "- To avoid a 2 hour run-time in our analysis every execution, we load the final version of the data into .CSV files. Each file consists of 250,000 records to reduce file size for GitHub loads.\n",
    "- All above logic is skipped if the file \"Compiled Data/dataset1.csv\" already exists.\n",
    "\n",
    "Below you will see this process, as well as import/options for needed python modules throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.distance import vincenty\n",
    "import holidays\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Load & Merge Data from Source Files\n",
    "# Parse into Compiled Files\n",
    "############################################################\n",
    "\n",
    "starttime = datetime.now()\n",
    "print('Starting Source Data Load & Merge Process. \\n'\n",
    "      'Start Time: ' + str(starttime))\n",
    "\n",
    "if os.path.isfile(\"Compiled Data/dataset1.csv\"):\n",
    "    print(\"Found the File!\")\n",
    "else:\n",
    "    citiBikeDataDirectory = \"Citi Bike Data\"\n",
    "    citiBikeDataFileNames = [\n",
    "        \"2013-07 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-07 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-08 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-09 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-10 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 1.csv\",\n",
    "        \"2013-11 - Citi Bike trip data - 2.csv\",\n",
    "        \"2013-12 - Citi Bike trip data.csv\",\n",
    "        \"2014-01 - Citi Bike trip data.csv\",\n",
    "        \"2014-02 - Citi Bike trip data.csv\"\n",
    "    ]\n",
    "\n",
    "    weatherDataFile = \"Weather Data/NY305801_9255_edited.txt\"\n",
    "\n",
    "    citiBikeDataRaw = []\n",
    "\n",
    "    for file in citiBikeDataFileNames:\n",
    "        print(file)\n",
    "        filepath = citiBikeDataDirectory + \"/\" + file\n",
    "        with open(filepath) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lines.pop(0)  # get rid of the first line that contains the column names\n",
    "            for line in lines:\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.split(\",\")\n",
    "                sLatLong = (line[5], line[6])\n",
    "                eLatLong = (line[9], line[10])\n",
    "\n",
    "                distance = vincenty(sLatLong, eLatLong).miles\n",
    "                line.extend([distance])\n",
    "\n",
    "                ## Monday       = 0\n",
    "                ## Tuesday      = 1\n",
    "                ## Wednesday    = 2\n",
    "                ## Thursday     = 3\n",
    "                ## Friday       = 4\n",
    "                ## Saturday     = 5\n",
    "                ## Sunday       = 6\n",
    "                if parse(line[1]).weekday() == 0:\n",
    "                    DayOfWeek = \"Monday\"\n",
    "                elif parse(line[1]).weekday() == 1:\n",
    "                    DayOfWeek = \"Tuesday\"\n",
    "                elif parse(line[1]).weekday() == 2:\n",
    "                    DayOfWeek = \"Wednesday\"\n",
    "                elif parse(line[1]).weekday() == 3:\n",
    "                    DayOfWeek = \"Thursday\"\n",
    "                elif parse(line[1]).weekday() == 4:\n",
    "                    DayOfWeek = \"Friday\"\n",
    "                elif parse(line[1]).weekday() == 5:\n",
    "                    DayOfWeek = \"Saturday\"\n",
    "                else:\n",
    "                    DayOfWeek = \"Sunday\"\n",
    "                line.extend([DayOfWeek])\n",
    "\n",
    "                ##Morning       5AM-10AM\n",
    "                ##Midday        10AM-2PM\n",
    "                ##Afternoon     2PM-5PM\n",
    "                ##Evening       5PM-10PM\n",
    "                ##Night         10PM-5AM\n",
    "\n",
    "                if parse(line[1]).hour >= 5 and parse(line[1]).hour < 10:\n",
    "                    TimeOfDay = 'Morning'\n",
    "                elif parse(line[1]).hour >= 10 and parse(line[1]).hour < 14:\n",
    "                    TimeOfDay = 'Midday'\n",
    "                elif parse(line[1]).hour >= 14 and parse(line[1]).hour < 17:\n",
    "                    TimeOfDay = 'Afternoon'\n",
    "                elif parse(line[1]).hour >= 17 and parse(line[1]).hour < 22:\n",
    "                    TimeOfDay = 'Evening'\n",
    "                else:\n",
    "                    TimeOfDay = 'Night'\n",
    "                line.extend([TimeOfDay])\n",
    "\n",
    "                ## 1 = Yes\n",
    "                ## 0 = No\n",
    "                if parse(line[1]) in holidays.UnitedStates():\n",
    "                    holidayFlag = \"1\"\n",
    "                else:\n",
    "                    holidayFlag = \"0\"\n",
    "                line.extend([holidayFlag])\n",
    "\n",
    "                citiBikeDataRaw.append(line)\n",
    "            del lines\n",
    "\n",
    "    with open(weatherDataFile) as f:\n",
    "        weatherDataRaw = f.read().splitlines()\n",
    "        weatherDataRaw.pop(0)  # again, get rid of the column names\n",
    "        for c in range(len(weatherDataRaw)):\n",
    "            weatherDataRaw[c] = weatherDataRaw[c].split(\",\")\n",
    "            # Adjust days and months to have a leading zero so we can capture all the data\n",
    "            if len(weatherDataRaw[c][2]) < 2:\n",
    "                weatherDataRaw[c][2] = \"0\" + weatherDataRaw[c][2]\n",
    "            if len(weatherDataRaw[c][0]) < 2:\n",
    "                weatherDataRaw[c][0] = \"0\" + weatherDataRaw[c][0]\n",
    "\n",
    "    citiBikeData = []\n",
    "\n",
    "    while (citiBikeDataRaw):\n",
    "        instance = citiBikeDataRaw.pop()\n",
    "        date = instance[1].split(\" \")[0].split(\"-\")  # uses the start date of the loan\n",
    "        for record in weatherDataRaw:\n",
    "            if (str(date[0]) == str(record[4]) and str(date[1]) == str(record[2]) and str(date[2]) == str(record[0])):\n",
    "                instance.extend([record[5], record[6], record[7], record[8], record[9]])\n",
    "                citiBikeData.append(instance)\n",
    "\n",
    "    del citiBikeDataRaw\n",
    "    del weatherDataRaw\n",
    "\n",
    "    # Final Columns:\n",
    "    #  0 tripduration\n",
    "    #  1 starttime\n",
    "    #  2 stoptime\n",
    "    #  3 start station id\n",
    "    #  4 start station name\n",
    "    #  5 start station latitude\n",
    "    #  6 start station longitude\n",
    "    #  7 end station id\n",
    "    #  8 end station name\n",
    "    #  9 end station latitude\n",
    "    # 10 end station longitude\n",
    "    # 11 bikeid\n",
    "    # 12 usertype\n",
    "    # 13 birth year\n",
    "    # 14 gender\n",
    "    # 15 start/end station distance\n",
    "    # 16 DayOfWeek\n",
    "    # 17 TimeOfDay\n",
    "    # 18 HolidayFlag\n",
    "    # 19 PRCP\n",
    "    # 20 SNOW\n",
    "    # 21 TAVE\n",
    "    # 22 TMAX\n",
    "    # 23 TMIN\n",
    "\n",
    "    maxLineCount = 250000\n",
    "    lineCounter = 1\n",
    "    fileCounter = 1\n",
    "    outputDirectoryFilename = \"Compiled Data/dataset\"\n",
    "    f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "    for line in citiBikeData:\n",
    "        if lineCounter == 250000:\n",
    "            print(f)\n",
    "            f.close()\n",
    "            lineCounter = 1\n",
    "            fileCounter = fileCounter + 1\n",
    "            f = open(outputDirectoryFilename + str(fileCounter) + \".csv\", \"w\")\n",
    "        f.write(\",\".join(map(str, line)) + \"\\n\")\n",
    "        lineCounter = lineCounter + 1\n",
    "\n",
    "    del citiBikeData\n",
    "\n",
    "endtime = datetime.now()\n",
    "print('Ending Source Data Load & Merge Process. \\n'\n",
    "      'End Time: ' + str(starttime) + '\\n'\n",
    "                                      'Total RunTime: ' + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Compiled Data from CSV\n",
    "\n",
    "Now that we have compiled data files from both CitiBike and the weather data, we want to load that data into a Pandas dataframe for analysis. We iterate and load each file produced above, then assign each column with their appropriate data types. Additionally, we compute the Age Column after producing a default value for missing \"Birth Year\" values. This is discussed further in the Data Preparation 1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "############################################################\n",
    "# Load the Compiled Data from CSV\n",
    "############################################################\n",
    "\n",
    "# Create CSV Reader Function and assign column headers\n",
    "def reader(f, columns):\n",
    "    d = pd.read_csv(f)\n",
    "    d.columns = columns\n",
    "    return d\n",
    "\n",
    "\n",
    "# Identify All CSV FileNames needing to be loaded\n",
    "path = r'Compiled Data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Define File Columns\n",
    "columns = [\"tripduration\", \"starttime\", \"stoptime\", \"start_station_id\", \"start_station_name\",\n",
    "           \"start_station_latitude\",\n",
    "           \"start_station_longitude\", \"end_station_id\", \"end_station_name\", \"end_station_latitude\",\n",
    "           \"end_station_longitude\", \"bikeid\", \"usertype\", \"birth year\", \"gender\", \"LinearDistance\", \"DayOfWeek\",\n",
    "           \"TimeOfDay\", \"HolidayFlag\", \"PRCP\", \"SNOW\", \"TAVE\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "# Load Data\n",
    "CitiBikeDataCompiled = pd.concat([reader(f, columns) for f in all_files])\n",
    "\n",
    "# Replace '\\N' Birth Years with Zero Values\n",
    "CitiBikeDataCompiled[\"birth year\"] = CitiBikeDataCompiled[\"birth year\"].replace(r'\\N', '0')\n",
    "\n",
    "# Convert Columns to Numerical Values\n",
    "CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX', 'TMIN']] \\\n",
    "    = CitiBikeDataCompiled[['tripduration', 'birth year', 'LinearDistance', 'PRCP', 'SNOW', 'TAVE', 'TMAX',\n",
    "                            'TMIN']].apply(pd.to_numeric)\n",
    "\n",
    "# Convert Columns to Date Values\n",
    "CitiBikeDataCompiled[['starttime', 'stoptime']] \\\n",
    "    = CitiBikeDataCompiled[['starttime', 'stoptime']].apply(pd.to_datetime)\n",
    "\n",
    "# Compute Age: 0 Birth Year = 0 Age ELSE Compute Start Time Year Minus Birth Year\n",
    "CitiBikeDataCompiled[\"Age\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 0,\n",
    "                                       CitiBikeDataCompiled[\"starttime\"].dt.year - CitiBikeDataCompiled[\n",
    "                                           \"birth year\"])\n",
    "\n",
    "# Convert Columns to Str Values\n",
    "CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']] \\\n",
    "    = CitiBikeDataCompiled[['start_station_id', 'end_station_id', 'bikeid', 'HolidayFlag', 'gender']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(len(CitiBikeDataCompiled))\n",
    "display(CitiBikeDataCompiled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1 - Define and prepare class variables\n",
    "\n",
    "##### Measurable Data Quality Factors\n",
    "When analyzing our final dataset for accurate measures, there are a few key factors we can easily verify/research:\n",
    "- Computational Accuracy: Ensure data attributes added by computation are correct\n",
    "    + TimeOfDay\n",
    "    + DayOfWeek        \n",
    "    + HolidayFlag\n",
    "    \n",
    "- Missing Data from Source\n",
    "- Duplicate Data from Source\n",
    "- Outlier Detection\n",
    "- Sampling to 500,000 Records for further analysis\n",
    "\n",
    "##### Immesurable Data Quality Factors\n",
    "Although we are able to research these many factors, one computation may still be lacking information in this dataset. Our LinearDistance attribute computes the distance from  one lat/long coordinate to another. This attribute does not however tell us the 'true' distance a biker traveled before returning the bike. Some bikers may be biking for exercise around the city with various turns and loops, whereas others travel the quickest path to their destination. Because our dataset limits us to start and end locations, we do not have enough information to accurately compute distance traveled. Because of this, we have named the attribute \"LinearDistance\" rather than \"DistanceTraveled\".\n",
    "\n",
    "Below we will walk through the process of researching the 'Measureable' data quality factors mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:TimeOfDay\n",
    "To help mitigate challenges with time series data, we have chosen to break TimeOfDay into 5 categories.\n",
    "These Categories are broken down below:\n",
    "- Morning       5  AM  -  10 AM\n",
    "- Midday        10 AM  -  2  PM\n",
    "- Afternoon     2  PM  -  5  PM\n",
    "- Evening       5  PM  -  10 PM\n",
    "- Night         10 PM  -  5  AM\n",
    "\n",
    "To ensure that these breakdowns are accurately computed, we pulled the distinct list of TimeOfDay assignments by starttime hour. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Compute StartHour from StartTime\n",
    "CitiBikeDataCompiled[\"StartHour\"] = CitiBikeDataCompiled[\"starttime\"].dt.hour\n",
    "\n",
    "    # Compute Distinct Combinations of StartHour and TimeOfDay\n",
    "DistinctTimeOfDayByHour = CitiBikeDataCompiled[[\"StartHour\", \"TimeOfDay\"]].drop_duplicates().sort_values(\"StartHour\")\n",
    "\n",
    "    # Print\n",
    "display(DistinctTimeOfDayByHour)\n",
    "\n",
    "    #Clean up Variables\n",
    "del CitiBikeDataCompiled[\"StartHour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:DayOfWeek\n",
    "In order to verify our computed DayOfWeek column, we have chosen one full week from 12/22/2013 - 12/28/2013 to validate. Below is a calendar image of this week to baseline our expected results:\n",
    "\n",
    "<img src=\"https://github.com/msmith-ds/DataMining/blob/master/Project2_Full/Images/Dec_2013_Calendar.png?raw=true\" width=\"300\">\n",
    "\n",
    "To verify these 7 days, we pulled the distinct list of DayOfWeek assignments by StartDate (No Time). If we can verify one full week, we may justify that the computation is correct across the entire dataset. Looking at the results below, we can verify that this categorization is correctly being assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, DayOfWeek within Date Threshold\n",
    "CitiBikeDayOfWeekTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"DayOfWeek\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeDayOfWeekTest[\"StartFloorDate\"] = CitiBikeDayOfWeekTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctDayOfWeek = CitiBikeDayOfWeekTest[[\"StartFloorDate\", \"DayOfWeek\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "\n",
    "    #Print\n",
    "display(DistinctDayOfWeek)\n",
    "\n",
    "    # Clean up Variables\n",
    "del CitiBikeDayOfWeekTest\n",
    "del DistinctDayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computational Accuracy:HolidayFlag\n",
    "Using the same week as was used to verify DayOfWeek, w can test whether HolidayFlag is set correctly for the Christmas Holiday. We pulled the distinct list of HolidayFlag assignments by StartDate (No Time). If we can verify one holiday, we may justify that the computation is correct across the entire dataset. Looking at the results below, we expect to see HolidayFlag = 1 only for 12/25/2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    # Create DataFrame for StartTime, HolidayFlag within Date Threshold\n",
    "CitiBikeHolidayFlagTest = CitiBikeDataCompiled[(CitiBikeDataCompiled['starttime'].dt.year == 2013)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.month == 12)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day >= 22)\n",
    "                                             & (CitiBikeDataCompiled['starttime'].dt.day <= 28)][\n",
    "    [\"starttime\", \"HolidayFlag\"]]\n",
    "\n",
    "    # Create FloorDate Variable as StartTime without the timestamp\n",
    "CitiBikeHolidayFlagTest[\"StartFloorDate\"] = CitiBikeHolidayFlagTest[\"starttime\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Compute Distinct combinations\n",
    "DistinctHolidayFlag = CitiBikeHolidayFlagTest[[\"StartFloorDate\", \"HolidayFlag\"]].drop_duplicates().sort_values(\n",
    "    \"StartFloorDate\")\n",
    "    \n",
    "    #Print\n",
    "display(DistinctHolidayFlag)\n",
    "    \n",
    "    # Clean up Variables\n",
    "del CitiBikeHolidayFlagTest\n",
    "del DistinctHolidayFlag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing Data from Source\n",
    "Accounting for missing data is a crucial part of our analysis. At first glance, it is very apparent that we have a large amount of missing data in the Gender and Birth Year attributes from our source CitiBike Data. We have already had to handle for missing Birth Year attributes while computing \"Age\" in our Data Load from CSV section of this paper. This was done to create a DEFAULT value of (0), such that future computations do not result in NA values as well. Gender has also already accounted for missing values with a default value of (0) by the source data. Although we have handled these missing values with a default, we want to ensure that we 'need' these records for further analysis - or if we may remove them from the dataset. Below you will see a table showing the frequency of missing values(or forced default values) by usertype. We noticed that of the 4,881,384 Subscribing Members in our dataset, only 295 of them were missing Gender information, whereas out of the  680,909 Customer Users (Non-Subscribing), there was only one observation where we had complete information for both Gender and Birth Year. This quickly told us that removing records with missing values is NOT an option, since we would lose data for our entire Customer Usertype. These attributes, as well as Age (Computed from birth year) will serve as difficult for use in a classification model attempting to predict usertype. \n",
    "\n",
    "We have also looked at all other attributes, and verified that there are no additional missing values in our dataset. A missing value matrix was produced to identify if there were any gaps in our data across all attributes. Due to the conclusive results in our data, no missing values present, we removed this lackluster visualization from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NADatatestData = CitiBikeDataCompiled[[\"usertype\",\"gender\", \"birth year\"]]\n",
    "\n",
    "NADatatestData[\"GenderISNA\"] = np.where(CitiBikeDataCompiled[\"gender\"] == '0', 1, 0)\n",
    "NADatatestData[\"BirthYearISNA\"] = np.where(CitiBikeDataCompiled[\"birth year\"] == 0, 1,0)\n",
    "\n",
    "NAAggs = pd.DataFrame({'count' : NADatatestData.groupby([\"usertype\",\"GenderISNA\", \"BirthYearISNA\"]).size()}).reset_index()\n",
    "\n",
    "display(NAAggs)\n",
    "\n",
    "del NAAggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Duplicate Data from Source\n",
    "To ensure that there are no duplicate records in our datasets, we ensured that the number of records before and after removing potential duplicates were equal to each other. This test passed, thus we did not need any alterations to the dataset based on duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "len(CitiBikeDataCompiled) == len(CitiBikeDataCompiled.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outlier Detection\n",
    "\n",
    "**Trip Duration**\n",
    "\n",
    "In analyzing a Box Plot on trip duration values, we find extreme outliers present. With durations reaching up to 72 days in the most extreme instance, our team decided to rule out any observation with a duration greater than a 24 hour period. The likelihood of an individual sleeping overnight after their trip with the bike still checked out is much higher after the 24 hour period. This fact easily skews the results of this value, potentially hurting any analysis done. We move forward with removing a total of 457 observations based on trip duration greater than 24 hours (86,400 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "#CitiBikeDataCompiledBackup = CitiBikeDataCompiled\n",
    "#CitiBikeDataCompiled = CitiBikeDataCompiledBackup\n",
    "\n",
    "    # BoxPlot tripDuration - Heavy Outliers!\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 24 hours?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]>86400]))\n",
    "\n",
    "    # Remove > 24 Hours\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"tripduration\"]<86400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once outliers are removed, we run the boxplot again, still seeing skewness in results. To try to mitigate this left-skew distribution, we decide to take a log transform on this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Trip Duration AFTER removal of outliers\n",
    "sns.boxplot(y = \"tripduration\", data = CitiBikeDataCompiled)\n",
    "sns.despine()\n",
    "\n",
    "    # Log Transform Column Added\n",
    "CitiBikeDataCompiled[\"tripdurationLog\"] = CitiBikeDataCompiled[\"tripduration\"].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot TripDurationLog\n",
    "sns.boxplot(y = \"tripdurationLog\", data = CitiBikeDataCompiled)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Similarly, we look at the distribution of Age in our dataset. Interestingly, it seems we have several outlier observations logging their birth year far enough back to cause their age to compute as 115 years old. Possible reasons for these outlier ages could be data entry errors by those who do not enjoy disclosing personal information, or possibly account sharing between a parent and a child - rendering an inaccurate data point to those actually taking the trip. Our target demographic for this study are those individuals under 65 years of age, given that they are the likely age groups to be in better physical condition for the bike share service. Given this target demographic, and the poor entries causing extreme outliers, we have chosen to limit out dataset to observations up to 65 years of age. This change removed an additional 53824 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()\n",
    "    \n",
    "    # How Many Greater than 65 years old?\n",
    "print(len(CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]>65]))\n",
    "\n",
    "    # Remove > 65 years old\n",
    "CitiBikeDataCompiled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]<=65]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "    # BoxPlot Age - removed Outliers!\n",
    "sns.boxplot(y = \"Age\", data = CitiBikeDataCompiled[CitiBikeDataCompiled[\"Age\"]!= 0])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Record Sampling to 500,000 Records\n",
    "Given the extremely large volume of data collected, we have have decided to try to sample down to ~ 1/10th of the original dataset for a total of 500,000 records. Before taking this action, however, we wanted to ensure that we keep data proportions reasonable for analysis and ensure we do not lose any important demographic in our data.\n",
    "\n",
    "Below we compute the percentage of our Dataset that comprises of Customers vs. Subscribers. We note, that 87.6% of the data consists of Subscriber users whereas the remaining 12.4% resemble Customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataCompiled.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Sample Dataset for this analysis, we have chosen to oversample the Customer observations to force a 50/50 split between the two classifications. This will help reduce bias in the model towards Subscribers simply due to the distribution of data in the sample.\n",
    "\n",
    "We are able to compute the sample size for each usertype and then take a random sample within each group. Below you will see that our sampled distribution matches the chosen 50/50 split between Customers and Subscriber Usertypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SampleSize = 500000\n",
    "\n",
    "CustomerSampleSize_Seed   = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "SubscriberSampleSize_Seed = int(round(SampleSize * 50.0 / 100.0,0))\n",
    "\n",
    "CitiBikeCustomerDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Customer'].sample(n=CustomerSampleSize_Seed, replace = False, random_state = CustomerSampleSize_Seed)\n",
    "CitiBikeSubscriberDataSampled = CitiBikeDataCompiled[CitiBikeDataCompiled[\"usertype\"] == 'Subscriber'].sample(n=SubscriberSampleSize_Seed, replace = False, random_state = SubscriberSampleSize_Seed)\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat([CitiBikeCustomerDataSampled,CitiBikeSubscriberDataSampled])\n",
    "\n",
    "print(len(CitiBikeDataSampled_5050))\n",
    "\n",
    "UserTypeDist = pd.DataFrame({'count' : CitiBikeDataSampled_5050.groupby([\"usertype\"]).size()}).reset_index()\n",
    "display(UserTypeDist)\n",
    "\n",
    "UserTypeDist.plot.pie(y = 'count', labels = ['Customer', 'Subscriber'], autopct='%1.1f%%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepping Data for Analysis\n",
    "\n",
    "Now that we have the dataset sampled, we still have some legwork necessary to convert our categorical attributes into integer values. Below we walk through this process for the following Attributes:\n",
    "- start_station_name\n",
    "- end_station_name\n",
    "- gender\n",
    "- DayOfWeek\n",
    "- TimeOfDay\n",
    "\n",
    "Once these 5 attributes have been encoded using OneHotEncoding, we have added 79 attributes into our dataset for analysis in our model.\n",
    "\n",
    "***Start Station Name***\n",
    "\n",
    "Initially including all start (and end) locations resulted in excessive system resource loading, later during randomized principal component computations, that froze our personal workstations and eventually ended with Python 'MemoryError' messaging. Therefore, due to the extremely large quantity of start stations in our dataset (330 stations), we were required to reduce this dimension down to a manageable size manually. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 start stations for Subscribers / Customers separately, we found that there were 9 overlapping stations, producing a final list of 31 stations. While encoding our start_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "    \n",
    "    #How many Start Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"start_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "startstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubStartStations = startstationsubfreq.head(20)\n",
    "\n",
    "del startstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "startstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"start_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustStartStations = startstationcustfreq.head(20)\n",
    "\n",
    "del startstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopStartStations = pd.DataFrame(pd.concat([TopSubStartStations,TopCustStartStations])[\"start_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopStartStations))\n",
    "display(TopStartStations[[\"start_station_name\"]])\n",
    "\n",
    "del TopStartStations\n",
    "del TopSubStartStations\n",
    "del TopCustStartStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.start_station_name,prefix='start_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"start_station_name_Pershing Square N\", \"start_station_name_E 17 St & Broadway\", \"start_station_name_8 Ave & W 31 St\", \"start_station_name_Lafayette St & E 8 St\", \"start_station_name_W 21 St & 6 Ave\", \"start_station_name_8 Ave & W 33 St\", \"start_station_name_W 20 St & 11 Ave\", \"start_station_name_Broadway & E 14 St\", \"start_station_name_Broadway & E 22 St\", \"start_station_name_W 41 St & 8 Ave\", \"start_station_name_Cleveland Pl & Spring St\", \"start_station_name_University Pl & E 14 St\", \"start_station_name_West St & Chambers St\", \"start_station_name_E 43 St & Vanderbilt Ave\", \"start_station_name_Broadway & W 24 St\", \"start_station_name_Greenwich Ave & 8 Ave\", \"start_station_name_W 18 St & 6 Ave\", \"start_station_name_Broadway & W 60 St\", \"start_station_name_Pershing Square S\", \"start_station_name_W 33 St & 7 Ave\", \"start_station_name_Central Park S & 6 Ave\", \"start_station_name_Centre St & Chambers St\", \"start_station_name_Grand Army Plaza & Central Park S\", \"start_station_name_Vesey Pl & River Terrace\", \"start_station_name_Broadway & W 58 St\", \"start_station_name_West Thames St\", \"start_station_name_12 Ave & W 40 St\", \"start_station_name_9 Ave & W 14 St\", \"start_station_name_W 14 St & The High Line\", \"start_station_name_State St\", \"start_station_name_Broadway & Battery Pl\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End Station Name***\n",
    "\n",
    "Similarly, we have an extremely large quantity of end stations in our dataset (330 stations) and including all of them resulted in system crashes during principal component analysis later in our lab. We were required to reduce this dimension down to a manageable size. Through trial and error on top frequency stations, we have chosen to reduce this number down to ~ 10% its original number. By identifying the top 20 end stations for Subscribers / Customers separately, we found that there were 7 overlapping stations, producing a final list of 33 stations. While encoding our end_station_name integer columns, we limit the number of columns to these stations identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "    #How many End Stations are there?\n",
    "print(len(CitiBikeDataSampled_5050[\"end_station_name\"].drop_duplicates()))\n",
    "\n",
    "    # Top 15 Start Station for Subscriber Users \n",
    "endstationsubfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Subscriber'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopSubendStations = endstationsubfreq.head(20)\n",
    "\n",
    "del endstationsubfreq\n",
    "\n",
    "    # Top 15 Start Station for Customer Users \n",
    "endstationcustfreq = pd.DataFrame({'count' : CitiBikeDataSampled_5050[CitiBikeDataSampled_5050[\"usertype\"] == 'Customer'].groupby([\"end_station_name\"]).size()}).reset_index().sort_values('count',ascending = False)\n",
    "TopCustendStations = endstationcustfreq.head(20)\n",
    "\n",
    "del endstationcustfreq\n",
    "\n",
    "    #Concat Subscribers and Customers\n",
    "TopendStations = pd.DataFrame(pd.concat([TopSubendStations,TopCustendStations])[\"end_station_name\"].drop_duplicates()).reset_index()    \n",
    "print(len(TopendStations))\n",
    "display(TopendStations[[\"end_station_name\"]])\n",
    "\n",
    "del TopendStations\n",
    "del TopSubendStations\n",
    "del TopCustendStations\n",
    "\n",
    "    #Split Start Station Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.end_station_name,prefix='end_station_name')\n",
    "\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit[[\"end_station_name_E 17 St & Broadway\", \"end_station_name_Lafayette St & E 8 St\", \"end_station_name_8 Ave & W 31 St\", \"end_station_name_W 21 St & 6 Ave\", \"end_station_name_Pershing Square N\", \"end_station_name_W 20 St & 11 Ave\", \"end_station_name_Broadway & E 14 St\", \"end_station_name_Broadway & E 22 St\", \"end_station_name_University Pl & E 14 St\", \"end_station_name_W 41 St & 8 Ave\", \"end_station_name_West St & Chambers St\", \"end_station_name_Cleveland Pl & Spring St\", \"end_station_name_Greenwich Ave & 8 Ave\", \"end_station_name_E 43 St & Vanderbilt Ave\", \"end_station_name_Broadway & W 24 St\", \"end_station_name_W 18 St & 6 Ave\", \"end_station_name_MacDougal St & Prince St\", \"end_station_name_Carmine St & 6 Ave\", \"end_station_name_8 Ave & W 33 St\", \"end_station_name_2 Ave & E 31 St\", \"end_station_name_Central Park S & 6 Ave\", \"end_station_name_Centre St & Chambers St\", \"end_station_name_Grand Army Plaza & Central Park S\", \"end_station_name_Broadway & W 60 St\", \"end_station_name_Broadway & W 58 St\", \"end_station_name_12 Ave & W 40 St\", \"end_station_name_Vesey Pl & River Terrace\", \"end_station_name_W 14 St & The High Line\", \"end_station_name_9 Ave & W 14 St\", \"end_station_name_West Thames St\", \"end_station_name_State St\", \"end_station_name_Old Fulton St\", \"end_station_name_South End Ave & Liberty St\"]]),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender, DayOfWeek, and TimeOfDay**\n",
    "\n",
    "The rest of our encoding attributes {Gender, DayOfWeek, and TimeOfDay} have the following value permutations. These permutations will be encoded as individual integer columns as well.\n",
    "\n",
    "- Gender:    {0 = unknown, 1 = male, 2 = female}\n",
    "- DayOfWeek: {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}\n",
    "- TimeOfDay: {Morning, Midday, Afternoon, Evening, Night}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    #Split gender Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.gender,prefix='gender')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split DayOfWeek Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.DayOfWeek,prefix='DayOfWeek')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "    #Split TimeOfDay Values for 50/50 dataset\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.TimeOfDay,prefix='TimeOfDay')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "display(CitiBikeDataSampled_5050.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these encodings complete, our final dataset to cross-validate on test/train datasets would appear to be complete. However, given the large number of attributes now present in our dataset, it would be wise to investigate a means of dimensionality reduction to not only speed up model generation, but to also improve accuracy by removing variable redundancy and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2 - Describe the final dataset that is used for classification/regression\n",
    "\n",
    "##### Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "With our data split evenly among customer and subscriber user types and OneHotEncoding complete, we are ready to consider the new dimensions of our dataset. Because our processed data is comprised of 105 various attributes ranging from weather and distance data to location and user type data across all 500,000 sample observations, and some variables such as weather attributes and even some start and end stations correlate to one another as depicted in the correlation matrix below, we feel it would be wise to reduce our number of attributes considered during model generation. After considering a variety of feature selection techniques, we've opted for dimensionality reduction via principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(CitiBikeDataSampled_5050.shape)\n",
    "\n",
    "sns.set(font_scale=0.5)\n",
    "sns.heatmap(CitiBikeDataSampled_5050.corr(), \n",
    "            xticklabels=CitiBikeDataSampled_5050.corr().columns.values,\n",
    "            yticklabels=CitiBikeDataSampled_5050.corr().columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing PCA, we are able to shrink the number of variables required for prediction purposes, replacing our latest variables derived up to this point with linear combinations of themselves. Each of these linear combinations makes up a component and comprises an eigenvector. When these eigenvectors are ordered by largest eigenvalues, representing the extent of variability explained by each vector, the vectors with largest eigenvalues may be identified as the principal components of the analysis. Selecting only those components which describe the most variance will help us reduce processing times and improve model performance and accuracy by avoiding variable inflation and overfitting associated with high dimensionality.\n",
    "\n",
    "Since this lab consists of two primary tasks (the first being to classify user types and the second being to predict trip duration), this section outlines two different PCA's, one for each task, in order to identify the appropriate number of components to include during model generation in subsequent sections. The appropriate number of components must be derived separately since user type classification will include trip duration predictors whereas trip duration prediction will require user type predictors. The expectation is that the number of components should be similar for both tasks, but it is not enough to make this assumption alone; both will be reviewed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA for Classification\n",
    "Our first objective is to identify the number of components to be used for user type classification. In order to do so, we first exclude redundant and non-value variables up front. Non-value variables include gender, birth year, and age since these data were missing for most Customer user types and were replaced with filler values as discussed in previous sections. We will exclude these since they misrepresent correlation with user type. Remaining attribute data will then be standardized so that all variables are on the same scale given that our explanatory variables are comprised of many different measures.\n",
    "\n",
    "*(Note: PCA code steps adapted and modified from https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class_att = CitiBikeDataSampled_5050.drop(['starttime', 'stoptime', 'start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'usertype', 'gender', 'gender_0', 'gender_1', 'gender_2', 'birth year', 'Age', 'tripduration', 'DayOfWeek', 'TimeOfDay'], axis=1)\n",
    "myData = class_att.as_matrix() \n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "myData_scaled_classification = min_max_scaler.fit_transform(myData)\n",
    "\n",
    "del myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excluding duplicate and non-value attributes and scaling our data, we verify that 89 attributes remain. The maximum number of components to be produced will match this number. For this reason, we will identify 89 to be the number of components produced by our PCA and will review each component's explained variance further to determine the proper number of components to be included later during model generation. Note randomized PCA was chosen in order to use singular value decomposition in our dimensionality reduction efforts due to the large size of our data set. Using full PCA required unacceptable lengths of time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(myData_scaled_classification.shape)\n",
    "pca_class = PCA(n_components=89, svd_solver='randomized')\n",
    "\n",
    "pca_class.fit(myData_scaled_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the resulting components have been ordered by eigenvector value and these values portrayed as ratios of variance explained by each component. In order to identify the principal components to be included during model generation, we review the rate at which explained variance decreases in significance from one principal component to the next. Accompanying these proportion values is a scree plot representing these same values in visual form. By plotting the scree plot, it is easier to judge where this rate of decreasing explained variance occurs. Note the rate of change in explained variance among the first 14 principal components – the change is rather steep through the 14th component. After the 1% drop between components 14 and 15, the rate of decreasing explained variance begins to somewhat flatten out, reducing to a 0.2% change or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_class.explained_variance_ratio_\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.plot(range(1,90), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, 90, -0.1, 12])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now referring to the cumulative variance values and associated plot below, it may be seen that the cumulative variance arguably begins to plateau around the 14th principal component and that the first 14 components together explain 77.92% of variance in the data set. For this reason, 14 principal components may be selected as being the most appropriate for user type classification modeling given the variables among these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_class.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,90), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([0, 90, 10, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pca_classification = PCA(n_components=14, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA for Regression\n",
    "\n",
    "Now that 14 principal components have been identified for Customer/Subscriber classification, we want to determine the correct number of principal components to use when predicting trip duration. For regression, we will need to interchange user type and log trip duration (which will be what we are actually predicting later before back transforming to regular trip duration) inclusion/exclusion. Before including user type, however, OneHotEncoding this variable is required since this has not been done as user type has always been discussed in terms of response up to this point (classification).\n",
    "\n",
    "We will also be removing end station names for regression since, contextually, CitiBike does not know the end location of a rider at the time the rider checks out a bike. Yes, this information becomes available at checkin... but if the company is to counterbalance the shortage effects of having too many bikes checked out during the same period of time, the model needs to exclude trip end location details. This concept will be discussed further later in this writeup. Again, all attributes are standardized before eigenvectors are computed for reasons stated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate usertype dummies for use in regression PCA\n",
    "AttSplit = pd.get_dummies(CitiBikeDataSampled_5050.usertype,prefix='usertype')\n",
    "CitiBikeDataSampled_5050 = pd.concat((CitiBikeDataSampled_5050,AttSplit),axis=1) # add back into the dataframe\n",
    "\n",
    "del AttSplit\n",
    "\n",
    "# Include usertype but exclude trip duration\n",
    "reg_att = CitiBikeDataSampled_5050.drop(['starttime', 'stoptime', 'start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'usertype','tripdurationLog', 'gender', 'gender_0', 'gender_1', 'gender_2', 'birth year', 'Age', 'tripduration', 'DayOfWeek', 'TimeOfDay', 'LinearDistance'], axis=1)\n",
    "myData = reg_att.columns.values.tolist() \n",
    "myData = [i for i in myData if \"end_station_name_\" not in i]\n",
    "reg_att = CitiBikeDataSampled_5050[myData]\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "myData_scaled_regression = min_max_scaler.fit_transform(reg_att.as_matrix())\n",
    "\n",
    "del myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After OneHotEncoding the user type variable, including only appropriate variables, and standardizing, we are left with 56 attributes. Therefore we can expect to produce a total of 56 linear combinations in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(myData_scaled_regression.shape)\n",
    "pca_reg = PCA(n_components=56, svd_solver='randomized')\n",
    "\n",
    "pca_reg.fit(myData_scaled_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering the resulting eigenvectors by variance proportion explained and plotting the scree plot below reveals dramatic change in explained variance through component 15. The 0.85% change in explained variance between components 15 and 16 flattens out to changes less than 0.33% between components thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca_reg.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1,57), var*100, marker = '.', color = 'red', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.axis([0, 60, -0.1, 20])\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(var, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative variance values and plot that follow indicate that total variance begins to plateau around the 15th principal component, supporting our previous conclusions. The plot also indicates the first 15 components explain 89.35% of data set variance. Based on these results, we will use 15 principal components for our trip duration prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca_reg.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(range(1,57), var1, marker = '.', color = 'green', markerfacecolor = 'black')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance (Sum %)')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.axis([-1, 60, 15, 101])\n",
    "\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pca_regression = PCA(n_components=15, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Set Summary\n",
    "\n",
    "At this stage, we've converted our original 30 variables into 107 attributes after creating dummy variables for categorical data such as day of the week, time of day, station names, gender, etc. These 107 attributes and their data types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_type = []\n",
    "for idx, col in enumerate(CitiBikeDataSampled_5050.columns):\n",
    "    data_type.append(CitiBikeDataSampled_5050.dtypes[idx])\n",
    "\n",
    "summary_df = {'Attribute Name' : pd.Series(CitiBikeDataSampled_5050.columns, index = range(len(CitiBikeDataSampled_5050.columns))), 'Data Type' : pd.Series(data_type, index = range(len(CitiBikeDataSampled_5050.columns)))}\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "del data_type, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with our data cleaned and prepped using OneHotEncoding, there is the innate need to reduce this number of attributes to a more manageable size before classification and regression predictions are made. For this reason, we've performed randomized PCA to compute linear combinations of the data and have chosen to use the first 14 principal components for the *n_components* parameter in our classification PCA and the first 15 principal components for regression PCA, based on explained variance and cumulative variance measures.\n",
    "\n",
    "Eigenvectors, proportions of explained variance, and cumulative proportions of explained variance are provided for the classification principal components and then again for regression principal components below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Classification Task Principal Component Breakdown\n",
    "pca_classification.fit(myData_scaled_classification)\n",
    "var = pca_classification.explained_variance_\n",
    "var1 = pca_classification.explained_variance_ratio_\n",
    "var2 = np.cumsum(np.round(pca_classification.explained_variance_ratio_, decimals=4))\n",
    "idx = ['PC'+str(i) for i in range(1,15)]\n",
    "\n",
    "PCA_df = {'...Eigenvector...' : pd.Series(var, index = idx),\n",
    "          '..Variance Proportion..' : pd.Series(var1, index = idx),\n",
    "          '.Variance Cumulative Proportion.' : pd.Series(var2, index = idx)}\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_df)\n",
    "print('Classification Task Principal Component Breakdown')\n",
    "display(PCA_df)\n",
    "\n",
    "del var, var1, var2, idx\n",
    "pca_classification = PCA(n_components=14, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Regression Task Principal Component Breakdown\n",
    "pca_regression.fit(myData_scaled_regression)\n",
    "var = pca_regression.explained_variance_\n",
    "var1 = pca_regression.explained_variance_ratio_\n",
    "var2 = np.cumsum(np.round(pca_regression.explained_variance_ratio_, decimals=4))\n",
    "idx = ['PC'+str(i) for i in range(1,16)]\n",
    "\n",
    "PCA_df = {'...Eigenvector...' : pd.Series(var, index = idx),\n",
    "          '..Variance Proportion..' : pd.Series(var1, index = idx),\n",
    "          '.Variance Cumulative Proportion.' : pd.Series(var2, index = idx)}\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_df)\n",
    "print('Regression Task Principal Component Breakdown')\n",
    "display(PCA_df)\n",
    "\n",
    "del var, var1, var2, idx\n",
    "pca_regression = PCA(n_components=15, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our discussions revolving data preparation and principal component derivations address our current interests for variable understanding moving forward into modeling and evaluation, there is still more to be understood regarding principal component loadings. Loadings will be described as we discuss the most important attributes for our tasks later in the Modeling and Evaluation 6 section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation\n",
    "\n",
    "##### Choose and explain the evaluation metrics that will be used\n",
    "\n",
    "### Michael for Eval Metrics\n",
    "*XXXXXX (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions. XXXXXX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test & Train Datasets\n",
    "\n",
    "#### Classification\n",
    "With our final encoded dataset complete, and PCA analysis performed to identify our principal components we begin splitting the data into Test vs Train datasets. We have chosen to utilize Stratified KFold Cross Validation for our classification analysis, with 10 folds. This means, that from our original sample size of 500,000, each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations all while keeping the ratio of classes equal amongst customers and subscribers. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We have utilized a random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Create CV Object for StratifiedKFold with 10 Folds, seeded at the length of our sample size\n",
    "seed = len(CitiBikeDataSampled_5050)\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = seed)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "Alternatively, we have chosen to utilize standard KFold Cross Validation for our regresssion analysis, with 10 folds. This is because our regression target is a continuous attribute, and we have already done stratification of the original dataset in general. We have chosen to shuffle the data before splitting into batches to prevent any potential issues with the order of our dataset. Once again, this means, that from our original sample size of 500,000, data will be randomly shuffled then each \"fold\" will save off approximately 10% as test observations utilizing the rest as training observations. This process will occur through 10 iterations, or folds, to allow us to cross validate our results amongst different test/train combinations. We continue to utilize the previously established random_state seed equal to the length of the original sampled dataset to ensure reproducible results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Create CV Object for KFold with 10 Folds, seeded at the length of our sample size\n",
    "\n",
    "cvReg = StratifiedKFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "print(cvReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build/Tune Three Classification Models (usertype predictions)\n",
    "#### Logistic Regression\n",
    "cost winner: .01\n",
    "\n",
    "Accuracy matches .01 and 5.0. We would not expect to need a high cost factor due to the scaler data prior to running our model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def lr_explor(cost,  \n",
    "              ScaledData,\n",
    "              PCA         = pca_classification,\n",
    "              Data        = CitiBikeDataSampled_5050,\n",
    "              cv          = cv,\n",
    "              seed        = seed):\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None, random_state=seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',lr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#cost    = [.01, .05, 1.0, 5.0]\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(cost)):\n",
    "#    acclist.append(lr_explor(cost       = cost[i],\n",
    "#                             ScaledData = myData_scaled_classification))\n",
    "#\n",
    "#LRcostdf = pd.DataFrame(pd.concat([pd.DataFrame(cost),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#LRcostdf.columns = ['Cost','Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(LRcostdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Cost</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0.01</td>\n",
    "      <td>0.68128</td>\n",
    "      <td>0.68198</td>\n",
    "      <td>0.68054</td>\n",
    "      <td>0.68190</td>\n",
    "      <td>0.68584</td>\n",
    "      <td>0.67858</td>\n",
    "      <td>0.68522</td>\n",
    "      <td>0.67878</td>\n",
    "      <td>0.68100</td>\n",
    "      <td>0.68374</td>\n",
    "      <td>0.681886</td>\n",
    "      <td>00:00:55.301026</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>0.05</td>\n",
    "      <td>0.68122</td>\n",
    "      <td>0.68212</td>\n",
    "      <td>0.68052</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68588</td>\n",
    "      <td>0.67842</td>\n",
    "      <td>0.68508</td>\n",
    "      <td>0.67870</td>\n",
    "      <td>0.68080</td>\n",
    "      <td>0.68372</td>\n",
    "      <td>0.681854</td>\n",
    "      <td>00:00:54.031372</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1.00</td>\n",
    "      <td>0.68136</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68050</td>\n",
    "      <td>0.68208</td>\n",
    "      <td>0.68586</td>\n",
    "      <td>0.67848</td>\n",
    "      <td>0.68504</td>\n",
    "      <td>0.67882</td>\n",
    "      <td>0.68080</td>\n",
    "      <td>0.68374</td>\n",
    "      <td>0.681876</td>\n",
    "      <td>00:00:53.148202</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>5.00</td>\n",
    "      <td>0.68138</td>\n",
    "      <td>0.68202</td>\n",
    "      <td>0.68048</td>\n",
    "      <td>0.68210</td>\n",
    "      <td>0.68586</td>\n",
    "      <td>0.67846</td>\n",
    "      <td>0.68508</td>\n",
    "      <td>0.67876</td>\n",
    "      <td>0.68082</td>\n",
    "      <td>0.68376</td>\n",
    "      <td>0.681872</td>\n",
    "      <td>00:00:52.870310</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "winner: Last Iteration (12)\n",
    "\tmax_depth\tmax_features\tmin_samples_leaf\tmin_samples_split\tn_estimators\n",
    "\t1000.0\t    14\t            25\t                50\t                15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def rfc_explor(ScaledData,\n",
    "               n_estimators,\n",
    "               max_features,\n",
    "               max_depth, \n",
    "               min_samples_split,\n",
    "               min_samples_leaf, \n",
    "               PCA         = pca_classification,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    rfc_clf = RandomForestClassifier(n_estimators=n_estimators, max_features = max_features, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, n_jobs=-1, random_state = seed) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',rfc_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_estimators       =  [10    , 10     , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10    , 10  , 5    , 15   ]  \n",
    "#max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "#max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "#min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "#min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "#\n",
    "#for i in range(0,len(n_estimators)):\n",
    "#    acclist.append(rfc_explor(ScaledData        = myData_scaled_classification,\n",
    "#                              n_estimators      = n_estimators[i],\n",
    "#                              max_features      = max_features[i],\n",
    "#                              max_depth         = max_depth[i],\n",
    "#                              min_samples_split = min_samples_split[i],\n",
    "#                              min_samples_leaf  = min_samples_leaf[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_estimators\": n_estimators,          \n",
    "#                                                \"max_features\": max_features,         \n",
    "#                                                \"max_depth\": max_depth,        \n",
    "#                                                \"min_samples_split\": min_samples_split,\n",
    "#                                                \"min_samples_leaf\": min_samples_leaf   \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['max_depth', 'max_features', 'min_samples_leaf','min_samples_split', 'n_estimators', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table border=\\1\\ class=\\dataframe\\>\n",
    " <thead>\n",
    "   <tr style=\\text-align: right;\\>\n",
    "     <th></th>\n",
    "     <th>max_depth</th>\n",
    "     <th>max_features</th>\n",
    "     <th>min_samples_leaf</th>\n",
    "     <th>min_samples_split</th>\n",
    "     <th>n_estimators</th>\n",
    "     <th>Iteration 0</th>\n",
    "     <th>Iteration 1</th>\n",
    "     <th>Iteration 2</th>\n",
    "     <th>Iteration 3</th>\n",
    "     <th>Iteration 4</th>\n",
    "     <th>Iteration 5</th>\n",
    "     <th>Iteration 6</th>\n",
    "     <th>Iteration 7</th>\n",
    "     <th>Iteration 8</th>\n",
    "     <th>Iteration 9</th>\n",
    "     <th>MeanAccuracy</th>\n",
    "     <th>RunTime</th>\n",
    "   </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "   <tr>\n",
    "     <th>0</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>1</td>\n",
    "     <td>2</td>\n",
    "     <td>10</td>\n",
    "     <td>0.70206</td>\n",
    "     <td>0.70432</td>\n",
    "     <td>0.70548</td>\n",
    "     <td>0.70046</td>\n",
    "     <td>0.70300</td>\n",
    "     <td>0.69678</td>\n",
    "     <td>0.70606</td>\n",
    "     <td>0.70146</td>\n",
    "     <td>0.70256</td>\n",
    "     <td>0.70450</td>\n",
    "     <td>0.702668</td>\n",
    "     <td>00:02:22.858960</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>1</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>4</td>\n",
    "     <td>8</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71310</td>\n",
    "     <td>0.71480</td>\n",
    "     <td>0.71486</td>\n",
    "     <td>0.71398</td>\n",
    "     <td>0.71414</td>\n",
    "     <td>0.70878</td>\n",
    "     <td>0.71756</td>\n",
    "     <td>0.71150</td>\n",
    "     <td>0.71694</td>\n",
    "     <td>0.71564</td>\n",
    "     <td>0.714130</td>\n",
    "     <td>00:02:19.138254</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>2</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>6</td>\n",
    "     <td>12</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71576</td>\n",
    "     <td>0.71660</td>\n",
    "     <td>0.72088</td>\n",
    "     <td>0.71724</td>\n",
    "     <td>0.71766</td>\n",
    "     <td>0.71410</td>\n",
    "     <td>0.71964</td>\n",
    "     <td>0.71730</td>\n",
    "     <td>0.71990</td>\n",
    "     <td>0.71808</td>\n",
    "     <td>0.717716</td>\n",
    "     <td>00:02:18.481832</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>3</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>8</td>\n",
    "     <td>16</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72052</td>\n",
    "     <td>0.72110</td>\n",
    "     <td>0.72216</td>\n",
    "     <td>0.71840</td>\n",
    "     <td>0.71940</td>\n",
    "     <td>0.71478</td>\n",
    "     <td>0.72130</td>\n",
    "     <td>0.71560</td>\n",
    "     <td>0.71952</td>\n",
    "     <td>0.71852</td>\n",
    "     <td>0.719130</td>\n",
    "     <td>00:02:17.359507</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>4</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>10</td>\n",
    "     <td>20</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71878</td>\n",
    "     <td>0.72088</td>\n",
    "     <td>0.72286</td>\n",
    "     <td>0.71678</td>\n",
    "     <td>0.72022</td>\n",
    "     <td>0.71692</td>\n",
    "     <td>0.72426</td>\n",
    "     <td>0.71810</td>\n",
    "     <td>0.72134</td>\n",
    "     <td>0.72066</td>\n",
    "     <td>0.720080</td>\n",
    "     <td>00:02:15.995898</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>5</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72098</td>\n",
    "     <td>0.72240</td>\n",
    "     <td>0.72164</td>\n",
    "     <td>0.71710</td>\n",
    "     <td>0.72258</td>\n",
    "     <td>0.71856</td>\n",
    "     <td>0.72334</td>\n",
    "     <td>0.71908</td>\n",
    "     <td>0.72254</td>\n",
    "     <td>0.72320</td>\n",
    "     <td>0.721142</td>\n",
    "     <td>00:02:11.538186</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>6</th>\n",
    "     <td>NaN</td>\n",
    "     <td>auto</td>\n",
    "     <td>40</td>\n",
    "     <td>80</td>\n",
    "     <td>10</td>\n",
    "     <td>0.71874</td>\n",
    "     <td>0.71902</td>\n",
    "     <td>0.72102</td>\n",
    "     <td>0.71716</td>\n",
    "     <td>0.72050</td>\n",
    "     <td>0.71686</td>\n",
    "     <td>0.72410</td>\n",
    "     <td>0.71760</td>\n",
    "     <td>0.72158</td>\n",
    "     <td>0.72104</td>\n",
    "     <td>0.719762</td>\n",
    "     <td>00:02:06.851562</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>7</th>\n",
    "     <td>NaN</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72230</td>\n",
    "     <td>0.72428</td>\n",
    "     <td>0.72382</td>\n",
    "     <td>0.71876</td>\n",
    "     <td>0.72684</td>\n",
    "     <td>0.71958</td>\n",
    "     <td>0.72524</td>\n",
    "     <td>0.72160</td>\n",
    "     <td>0.72436</td>\n",
    "     <td>0.72328</td>\n",
    "     <td>0.723006</td>\n",
    "     <td>00:06:37.091719</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>8</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72260</td>\n",
    "     <td>0.72488</td>\n",
    "     <td>0.72530</td>\n",
    "     <td>0.72070</td>\n",
    "     <td>0.72446</td>\n",
    "     <td>0.71846</td>\n",
    "     <td>0.72462</td>\n",
    "     <td>0.72124</td>\n",
    "     <td>0.72546</td>\n",
    "     <td>0.72346</td>\n",
    "     <td>0.723118</td>\n",
    "     <td>00:06:33.179886</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>9</th>\n",
    "     <td>500.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72228</td>\n",
    "     <td>0.72430</td>\n",
    "     <td>0.72430</td>\n",
    "     <td>0.71970</td>\n",
    "     <td>0.72520</td>\n",
    "     <td>0.71920</td>\n",
    "     <td>0.72702</td>\n",
    "     <td>0.72076</td>\n",
    "     <td>0.72326</td>\n",
    "     <td>0.72404</td>\n",
    "     <td>0.723006</td>\n",
    "     <td>00:06:35.014423</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>10</th>\n",
    "     <td>100.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>10</td>\n",
    "     <td>0.72234</td>\n",
    "     <td>0.72360</td>\n",
    "     <td>0.72392</td>\n",
    "     <td>0.71976</td>\n",
    "     <td>0.72480</td>\n",
    "     <td>0.71958</td>\n",
    "     <td>0.72520</td>\n",
    "     <td>0.72080</td>\n",
    "     <td>0.72534</td>\n",
    "     <td>0.72464</td>\n",
    "     <td>0.722998</td>\n",
    "     <td>00:06:33.892724</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>11</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>5</td>\n",
    "     <td>0.71642</td>\n",
    "     <td>0.71852</td>\n",
    "     <td>0.71884</td>\n",
    "     <td>0.71458</td>\n",
    "     <td>0.71964</td>\n",
    "     <td>0.71342</td>\n",
    "     <td>0.71864</td>\n",
    "     <td>0.71534</td>\n",
    "     <td>0.71868</td>\n",
    "     <td>0.71980</td>\n",
    "     <td>0.717388</td>\n",
    "     <td>00:04:21.575723</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "     <th>12</th>\n",
    "     <td>1000.0</td>\n",
    "     <td>14</td>\n",
    "     <td>25</td>\n",
    "     <td>50</td>\n",
    "     <td>15</td>\n",
    "     <td>0.72432</td>\n",
    "     <td>0.72640</td>\n",
    "     <td>0.72618</td>\n",
    "     <td>0.72126</td>\n",
    "     <td>0.72712</td>\n",
    "     <td>0.72128</td>\n",
    "     <td>0.72728</td>\n",
    "     <td>0.72448</td>\n",
    "     <td>0.72708</td>\n",
    "     <td>0.72652</td>\n",
    "     <td>0.725192</td>\n",
    "     <td>00:08:59.111516</td>\n",
    "   </tr>\n",
    " </tbody>\n",
    "</table>\n",
    "</div>\n",
    "Wall time: 55min 32s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "Winner: \n",
    "    algorithm\tleaf_size\tn_neighbors\n",
    "    kd_tree\t    50\t        150\n",
    "\n",
    "Note: we lose 00.04% accuracy between KD_tree and ball_tree, but we gain 7min 25 secs in runtime - pushing KD_tree as our top choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def knn_explor(ScaledData,\n",
    "               n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               PCA         = pca_classification,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cv,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',knn_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Below Code is commented out due to its long run-time. \n",
    "##The Rendered HTML Table output has been hardcoded in the next cell block for interpretations \n",
    "\n",
    "\n",
    "#%%time\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  ['ball_tree'     , 'ball_tree'     , 'ball_tree'     , 'ball_tree'     , 'ball_tree'       , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      , 'ball_tree'      ] \n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knn_explor(ScaledData  = myData_scaled_classification,\n",
    "#                              n_neighbors = n_neighbors[i],\n",
    "#                              algorithm   = algorithm[i],\n",
    "#                              leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#acclist = [] \n",
    "#\n",
    "#n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "#algorithm   =  ['kd_tree'     , 'kd_tree'     , 'kd_tree'     , 'kd_tree'     , 'kd_tree'       , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      , 'kd_tree'      ] \n",
    "#leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "#\n",
    "#\n",
    "#\n",
    "#for i in range(0,len(n_neighbors)):\n",
    "#    acclist.append(knn_explor(ScaledData  = myData_scaled_classification,\n",
    "#                              n_neighbors = n_neighbors[i],\n",
    "#                              algorithm   = algorithm[i],\n",
    "#                              leaf_size   = leaf_size[i]\n",
    "#                             )\n",
    "#                  )\n",
    "#\n",
    "#rfcdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "#                                                \"n_neighbors\": n_neighbors,          \n",
    "#                                                \"algorithm\": algorithm,         \n",
    "#                                                \"leaf_size\": leaf_size  \n",
    "#                                              }),\n",
    "#                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "#rfcdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanAccuracy', 'RunTime']\n",
    "#display(rfcdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_subarea output_html rendered_html\"><div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66732</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66590</td>\n",
    "      <td>0.66382</td>\n",
    "      <td>0.66776</td>\n",
    "      <td>0.66150</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66194</td>\n",
    "      <td>0.66566</td>\n",
    "      <td>0.66724</td>\n",
    "      <td>0.665266</td>\n",
    "      <td>00:06:53.743385</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66722</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66406</td>\n",
    "      <td>0.66784</td>\n",
    "      <td>0.66154</td>\n",
    "      <td>0.66562</td>\n",
    "      <td>0.66188</td>\n",
    "      <td>0.66534</td>\n",
    "      <td>0.66700</td>\n",
    "      <td>0.665216</td>\n",
    "      <td>00:05:40.484082</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66716</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66606</td>\n",
    "      <td>0.66422</td>\n",
    "      <td>0.66790</td>\n",
    "      <td>0.66162</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66182</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.665328</td>\n",
    "      <td>00:06:54.453049</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66748</td>\n",
    "      <td>0.66598</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66386</td>\n",
    "      <td>0.66786</td>\n",
    "      <td>0.66178</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66192</td>\n",
    "      <td>0.66540</td>\n",
    "      <td>0.66708</td>\n",
    "      <td>0.665288</td>\n",
    "      <td>00:09:07.276291</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.66592</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66398</td>\n",
    "      <td>0.66806</td>\n",
    "      <td>0.66152</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66190</td>\n",
    "      <td>0.66542</td>\n",
    "      <td>0.66698</td>\n",
    "      <td>0.665258</td>\n",
    "      <td>00:06:54.266485</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.68034</td>\n",
    "      <td>0.67936</td>\n",
    "      <td>0.67816</td>\n",
    "      <td>0.67758</td>\n",
    "      <td>0.68036</td>\n",
    "      <td>0.67388</td>\n",
    "      <td>0.67840</td>\n",
    "      <td>0.67480</td>\n",
    "      <td>0.68068</td>\n",
    "      <td>0.67866</td>\n",
    "      <td>0.678222</td>\n",
    "      <td>00:07:03.431483</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.68648</td>\n",
    "      <td>0.68584</td>\n",
    "      <td>0.68636</td>\n",
    "      <td>0.68364</td>\n",
    "      <td>0.68574</td>\n",
    "      <td>0.68160</td>\n",
    "      <td>0.68682</td>\n",
    "      <td>0.68154</td>\n",
    "      <td>0.68738</td>\n",
    "      <td>0.68630</td>\n",
    "      <td>0.685170</td>\n",
    "      <td>00:07:14.175179</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.68922</td>\n",
    "      <td>0.68816</td>\n",
    "      <td>0.68828</td>\n",
    "      <td>0.68702</td>\n",
    "      <td>0.68972</td>\n",
    "      <td>0.68486</td>\n",
    "      <td>0.69022</td>\n",
    "      <td>0.68466</td>\n",
    "      <td>0.69014</td>\n",
    "      <td>0.68880</td>\n",
    "      <td>0.688108</td>\n",
    "      <td>00:07:45.081930</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.69556</td>\n",
    "      <td>0.69452</td>\n",
    "      <td>0.69446</td>\n",
    "      <td>0.69290</td>\n",
    "      <td>0.69572</td>\n",
    "      <td>0.69068</td>\n",
    "      <td>0.69476</td>\n",
    "      <td>0.69210</td>\n",
    "      <td>0.69590</td>\n",
    "      <td>0.69468</td>\n",
    "      <td>0.694128</td>\n",
    "      <td>00:08:10.106731</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.69504</td>\n",
    "      <td>0.69628</td>\n",
    "      <td>0.69582</td>\n",
    "      <td>0.69370</td>\n",
    "      <td>0.69856</td>\n",
    "      <td>0.69170</td>\n",
    "      <td>0.69632</td>\n",
    "      <td>0.69318</td>\n",
    "      <td>0.69630</td>\n",
    "      <td>0.69670</td>\n",
    "      <td>0.695360</td>\n",
    "      <td>00:08:54.870863</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.69618</td>\n",
    "      <td>0.69692</td>\n",
    "      <td>0.69596</td>\n",
    "      <td>0.69322</td>\n",
    "      <td>0.69690</td>\n",
    "      <td>0.69150</td>\n",
    "      <td>0.69764</td>\n",
    "      <td>0.69344</td>\n",
    "      <td>0.69672</td>\n",
    "      <td>0.69680</td>\n",
    "      <td>0.695528</td>\n",
    "      <td>00:09:27.100008</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>ball_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.69592</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.69374</td>\n",
    "      <td>0.69716</td>\n",
    "      <td>0.69184</td>\n",
    "      <td>0.69722</td>\n",
    "      <td>0.69380</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.69588</td>\n",
    "      <td>0.695366</td>\n",
    "      <td>00:09:52.138668</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "Wall time: 1h 33min 57s\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>algorithm</th>\n",
    "      <th>leaf_size</th>\n",
    "      <th>n_neighbors</th>\n",
    "      <th>Iteration 0</th>\n",
    "      <th>Iteration 1</th>\n",
    "      <th>Iteration 2</th>\n",
    "      <th>Iteration 3</th>\n",
    "      <th>Iteration 4</th>\n",
    "      <th>Iteration 5</th>\n",
    "      <th>Iteration 6</th>\n",
    "      <th>Iteration 7</th>\n",
    "      <th>Iteration 8</th>\n",
    "      <th>Iteration 9</th>\n",
    "      <th>MeanAccuracy</th>\n",
    "      <th>RunTime</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>30</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66726</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66568</td>\n",
    "      <td>0.66414</td>\n",
    "      <td>0.66804</td>\n",
    "      <td>0.66150</td>\n",
    "      <td>0.66570</td>\n",
    "      <td>0.66168</td>\n",
    "      <td>0.66564</td>\n",
    "      <td>0.66714</td>\n",
    "      <td>0.665266</td>\n",
    "      <td>00:01:13.676624</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>15</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66732</td>\n",
    "      <td>0.66598</td>\n",
    "      <td>0.66574</td>\n",
    "      <td>0.66376</td>\n",
    "      <td>0.66764</td>\n",
    "      <td>0.66182</td>\n",
    "      <td>0.66572</td>\n",
    "      <td>0.66194</td>\n",
    "      <td>0.66548</td>\n",
    "      <td>0.66706</td>\n",
    "      <td>0.665246</td>\n",
    "      <td>00:01:10.890032</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66726</td>\n",
    "      <td>0.66576</td>\n",
    "      <td>0.66590</td>\n",
    "      <td>0.66402</td>\n",
    "      <td>0.66802</td>\n",
    "      <td>0.66158</td>\n",
    "      <td>0.66562</td>\n",
    "      <td>0.66180</td>\n",
    "      <td>0.66518</td>\n",
    "      <td>0.66734</td>\n",
    "      <td>0.665248</td>\n",
    "      <td>00:01:12.039968</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>75</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66738</td>\n",
    "      <td>0.66600</td>\n",
    "      <td>0.66600</td>\n",
    "      <td>0.66366</td>\n",
    "      <td>0.66794</td>\n",
    "      <td>0.66162</td>\n",
    "      <td>0.66586</td>\n",
    "      <td>0.66160</td>\n",
    "      <td>0.66542</td>\n",
    "      <td>0.66720</td>\n",
    "      <td>0.665268</td>\n",
    "      <td>00:01:17.640469</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>5</td>\n",
    "      <td>0.66718</td>\n",
    "      <td>0.66580</td>\n",
    "      <td>0.66588</td>\n",
    "      <td>0.66394</td>\n",
    "      <td>0.66802</td>\n",
    "      <td>0.66152</td>\n",
    "      <td>0.66578</td>\n",
    "      <td>0.66184</td>\n",
    "      <td>0.66560</td>\n",
    "      <td>0.66700</td>\n",
    "      <td>0.665256</td>\n",
    "      <td>00:01:17.354629</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>10</td>\n",
    "      <td>0.68052</td>\n",
    "      <td>0.67930</td>\n",
    "      <td>0.67830</td>\n",
    "      <td>0.67748</td>\n",
    "      <td>0.68034</td>\n",
    "      <td>0.67394</td>\n",
    "      <td>0.67810</td>\n",
    "      <td>0.67488</td>\n",
    "      <td>0.68094</td>\n",
    "      <td>0.67860</td>\n",
    "      <td>0.678240</td>\n",
    "      <td>00:01:19.506095</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>15</td>\n",
    "      <td>0.68658</td>\n",
    "      <td>0.68566</td>\n",
    "      <td>0.68624</td>\n",
    "      <td>0.68360</td>\n",
    "      <td>0.68588</td>\n",
    "      <td>0.68146</td>\n",
    "      <td>0.68650</td>\n",
    "      <td>0.68156</td>\n",
    "      <td>0.68738</td>\n",
    "      <td>0.68594</td>\n",
    "      <td>0.685080</td>\n",
    "      <td>00:01:23.414543</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>20</td>\n",
    "      <td>0.68918</td>\n",
    "      <td>0.68810</td>\n",
    "      <td>0.68830</td>\n",
    "      <td>0.68724</td>\n",
    "      <td>0.68992</td>\n",
    "      <td>0.68492</td>\n",
    "      <td>0.69012</td>\n",
    "      <td>0.68480</td>\n",
    "      <td>0.69018</td>\n",
    "      <td>0.68854</td>\n",
    "      <td>0.688130</td>\n",
    "      <td>00:01:22.808268</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>50</td>\n",
    "      <td>0.69550</td>\n",
    "      <td>0.69440</td>\n",
    "      <td>0.69404</td>\n",
    "      <td>0.69292</td>\n",
    "      <td>0.69560</td>\n",
    "      <td>0.69072</td>\n",
    "      <td>0.69500</td>\n",
    "      <td>0.69222</td>\n",
    "      <td>0.69570</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.694064</td>\n",
    "      <td>00:01:34.127069</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>100</td>\n",
    "      <td>0.69496</td>\n",
    "      <td>0.69624</td>\n",
    "      <td>0.69570</td>\n",
    "      <td>0.69372</td>\n",
    "      <td>0.69846</td>\n",
    "      <td>0.69174</td>\n",
    "      <td>0.69630</td>\n",
    "      <td>0.69314</td>\n",
    "      <td>0.69638</td>\n",
    "      <td>0.69678</td>\n",
    "      <td>0.695342</td>\n",
    "      <td>00:01:49.578879</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>150</td>\n",
    "      <td>0.69610</td>\n",
    "      <td>0.69696</td>\n",
    "      <td>0.69588</td>\n",
    "      <td>0.69322</td>\n",
    "      <td>0.69702</td>\n",
    "      <td>0.69148</td>\n",
    "      <td>0.69770</td>\n",
    "      <td>0.69356</td>\n",
    "      <td>0.69670</td>\n",
    "      <td>0.69662</td>\n",
    "      <td>0.695524</td>\n",
    "      <td>00:02:02.480648</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>kd_tree</td>\n",
    "      <td>50</td>\n",
    "      <td>200</td>\n",
    "      <td>0.69592</td>\n",
    "      <td>0.69694</td>\n",
    "      <td>0.69454</td>\n",
    "      <td>0.69374</td>\n",
    "      <td>0.69726</td>\n",
    "      <td>0.69158</td>\n",
    "      <td>0.69724</td>\n",
    "      <td>0.69384</td>\n",
    "      <td>0.69672</td>\n",
    "      <td>0.69586</td>\n",
    "      <td>0.695364</td>\n",
    "      <td>00:02:11.007620</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "Wall time: 17min 54s\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build/Tune Three Regression Models (TripDurationLog predictions)\n",
    "#### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def knnr_explor(ScaledData,\n",
    "               n_neighbors,\n",
    "               algorithm ,\n",
    "               leaf_size,\n",
    "               PCA         = pca_regression,\n",
    "               Data        = CitiBikeDataSampled_5050,\n",
    "               cv          = cvReg,\n",
    "               seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    knnr_clf = KNeighborsRegressor(n_neighbors = n_neighbors, algorithm = algorithm, leaf_size = leaf_size, n_jobs=-1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',knnr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, scoring = 'neg_mean_squared_error', cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    accuracy = accuracy * -1\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "algorithm   =  'ball_tree'\n",
    "leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knnr_explor(ScaledData  = myData_scaled_regression,\n",
    "                               n_neighbors = n_neighbors[i],\n",
    "                               algorithm   = algorithm,\n",
    "                               leaf_size   = leaf_size[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "knnrdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "display(knnrdf)\n",
    "\n",
    "\n",
    "acclist = []\n",
    "\n",
    "n_neighbors =  [5          , 5          , 5          , 5          , 5            , 10          , 15          , 20          , 50          , 100         , 150         , 200         ] \n",
    "algorithm   =  'kd_tree'\n",
    "leaf_size   =  [30         , 15         , 50         , 75         , 50           , 50          , 50          , 50          , 50          , 50          , 50          , 50          ] \n",
    "\n",
    "for i in range(0,len(n_neighbors)):\n",
    "    acclist.append(knnr_explor(ScaledData  = myData_scaled_regression,\n",
    "                               n_neighbors = n_neighbors[i],\n",
    "                               algorithm   = algorithm,\n",
    "                               leaf_size   = leaf_size[i]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"n_neighbors\": n_neighbors,          \n",
    "                                                \"algorithm\": algorithm,         \n",
    "                                                \"leaf_size\": leaf_size  \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "knnrdf.columns = ['algorithm', 'leaf_size','n_neighbors', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "display(rfcdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DecisionTree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def dtr_explor(ScaledData,\n",
    "                splitter,\n",
    "                max_features ,\n",
    "                max_depth,\n",
    "                min_samples_split,\n",
    "                min_samples_leaf,\n",
    "                PCA         = pca_regression,\n",
    "                Data        = CitiBikeDataSampled_5050,\n",
    "                cv          = cvReg,\n",
    "                seed        = seed):\n",
    "    startTime = datetime.now()\n",
    "    y = Data['tripdurationLog'].values # get the labels we want\n",
    "    \n",
    "    X = ScaledData\n",
    "    \n",
    "    dtr_clf = DecisionTreeRegressor(splitter           = splitter, \n",
    "                                    max_features       = max_features, \n",
    "                                    max_depth          = max_depth, \n",
    "                                    min_samples_split  = min_samples_split, \n",
    "                                    min_samples_leaf   = min_samples_leaf, \n",
    "                                    random_state       = seed, \n",
    "                                    n_jobs             = -1) # get object\n",
    "    \n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',dtr_clf)]\n",
    "    )\n",
    "\n",
    "    accuracy = cross_val_score(clf_pipe, X, y, scoring = 'neg_mean_squared_error', cv=cv.split(X, y)) # this also can help with parallelism\n",
    "    accuracy = accuracy * -1\n",
    "    MeanAccuracy =  sum(accuracy)/len(accuracy)\n",
    "    accuracy = np.append(accuracy, MeanAccuracy)\n",
    "    endTime = datetime.now()\n",
    "    TotalTime = endTime - startTime\n",
    "    accuracy = np.append(accuracy, TotalTime)\n",
    "    \n",
    "    #print(TotalTime)\n",
    "    #print(accuracy)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acclist = [] \n",
    "\n",
    "splitter           =  'best' \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(max_features)):\n",
    "    acclist.append(knnr_explor(ScaledData         = myData_scaled_regression,\n",
    "                               splitter           = splitter, \n",
    "                               max_features       = max_features[i], \n",
    "                               max_depth          = max_depth[i], \n",
    "                               min_samples_split  = min_samples_split[i],\n",
    "                               min_samples_leaf   = min_samples_leaf[i],\n",
    "                              )\n",
    "                  )\n",
    "\n",
    "knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"splitter         \": splitter         ,\n",
    "                                                \"max_features     \": max_features     ,\n",
    "                                                \"max_depth        \": max_depth        ,\n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf \": min_samples_leaf \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "knnrdf.columns = ['max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split', 'splitter', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "display(knnrdf)\n",
    "\n",
    "\n",
    "acclist = []\n",
    "\n",
    "splitter           =  'random' \n",
    "max_features       =  ['auto', 'auto' , 'auto', 'auto', 'auto', 'auto', 'auto', 14    , 14    , 14    , 14  , 14   , 14   ] \n",
    "max_depth          =  [None  , None   , None  , None  , None  , None  , None  , None  , 1000  , 500   , 100 , 1000 , 1000 ] \n",
    "min_samples_split  =  [2     , 8      , 12    , 16    , 20    , 50    , 80    , 50    , 50    , 50    , 50  , 50   , 50   ] \n",
    "min_samples_leaf   =  [1     , 4      , 6     , 8     , 10    , 25    , 40    , 25    , 25    , 25    , 25  , 25   , 25   ]\n",
    "\n",
    "for i in range(0,len(max_features)):\n",
    "    acclist.append(knnr_explor(ScaledData         = myData_scaled_regression,\n",
    "                               splitter           = splitter, \n",
    "                               max_features       = max_features[i], \n",
    "                               max_depth          = max_depth[i], \n",
    "                               min_samples_split  = min_samples_split[i],\n",
    "                               min_samples_leaf   = min_samples_leaf[i],\n",
    "                              )\n",
    "                  )\n",
    "\n",
    "knnrdf = pd.DataFrame(pd.concat([pd.DataFrame({\n",
    "                                                \"splitter         \": splitter         ,\n",
    "                                                \"max_features     \": max_features     ,\n",
    "                                                \"max_depth        \": max_depth        ,\n",
    "                                                \"min_samples_split\": min_samples_split,\n",
    "                                                \"min_samples_leaf \": min_samples_leaf \n",
    "                                              }),\n",
    "                               pd.DataFrame(acclist)], axis = 1).reindex())\n",
    "knnrdf.columns = ['max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split', 'splitter', 'Iteration 0', 'Iteration 1', 'Iteration 2', 'Iteration 3', 'Iteration 4', 'Iteration 5', 'Iteration 6', 'Iteration 7', 'Iteration 8', 'Iteration 9', 'MeanSquaredError', 'RunTime']\n",
    "display(knnrdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results using a chosen method of evaluation\n",
    "#### Classification\n",
    "\n",
    "We have created a function to be re-used for our cross-validation Accuracy Scores. Inputs of PCA components, Model CLF object, original sample data, and a CV containing our test/train splits allow us to easily produce an array of Accuracy Scores, and log_loss values for the different permutations of models tested. A ROC Curve plot is also displayed depicting a stacked view of the ROC_AUC Values for each iteration. Finally, a confusion matrix is displayed for the last test/train iteration for further interpretation on results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ROC_curve(X, y, mean_tpr, mean_fpr, cv = cv, ):\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "             label='Luck')\n",
    "\n",
    "    mean_tpr /= cv.get_n_splits(X, y)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_kfold_scores_Classification( clf,  \n",
    "                                         ScaledData,\n",
    "                                         PCA      = pca_classification,\n",
    "                                         Data     = CitiBikeDataSampled_5050,\n",
    "                                         cv       = cv):\n",
    "    \n",
    "    y = Data['usertype'].values # get the labels we want\n",
    "    y = np.where(y == 'Subscriber', 1, 0)    \n",
    "    X = ScaledData\n",
    "\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "\n",
    "    # setup pipeline to take PCA, then fit a clf model\n",
    "    clf_pipe = Pipeline(\n",
    "        [('PCA',PCA),\n",
    "         ('CLF',clf)]\n",
    "    )\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange', 'pink', 'darkred', 'dimgray', 'maroon', 'coral'])\n",
    "    lw = 2\n",
    "    i = 0\n",
    "    accuracy = []\n",
    "    logloss = []\n",
    "    \n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        clf_pipe.fit(X[train],y[train])  # train object\n",
    "        y_hat = clf_pipe.predict(X[test]) # get test set preditions\n",
    "        \n",
    "        a = float(mt.accuracy_score(y[test],y_hat))\n",
    "        l = float(mt.log_loss(y[test], y_hat))\n",
    "        \n",
    "        accuracy.append(round(a,5)) \n",
    "\n",
    "        logloss.append(round(l,5)) \n",
    "\n",
    "\n",
    "        probas_ = clf_pipe.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        \n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = (12, 6)\n",
    "        \n",
    "        plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "                 label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Accuracy Ratings across all iterations: {0}\\n\\n\\\n",
    "Average Accuracy: {1}\\n\\n\\\n",
    "Log Loss Values across all iterations: {2}\\n\\n\\\n",
    "Average Log Loss: {3}\\n\".format(accuracy, round(sum(accuracy)/len(accuracy),5), logloss,round(sum(logloss)/len(logloss),5)))\n",
    "    \n",
    "    plot_ROC_curve(X, y, mean_tpr, mean_fpr)\n",
    "    \n",
    "    ytestnames = np.where(y[test] == 1, 'Subscriber', 'Customer')\n",
    "    yhatnames  = np.where(y_hat == 1, 'Subscriber', 'Customer')\n",
    "    print(\"confusion matrix\\n{0}\\n\".format(pd.crosstab(ytestnames, yhatnames, rownames = ['True'], colnames = ['Predicted'], margins = True)))\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(y[test], y_hat), \n",
    "                          classes   =[\"Customer\", \"Subscriber\"], \n",
    "                          normalize =True,\n",
    "                          title     ='Confusion matrix, with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=.01, class_weight=None, random_state=seed) # get object\n",
    "\n",
    "compute_kfold_scores_Classification(clf         = lr_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest  - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_clf = RandomForestClassifier(n_estimators       = 15, \n",
    "                                 max_features       = 14, \n",
    "                                 max_depth          = 1000.0, \n",
    "                                 min_samples_split  = 50, \n",
    "                                 min_samples_leaf   = 25, \n",
    "                                 n_jobs             = -1, \n",
    "                                 random_state       = seed) # get object\n",
    "    \n",
    "compute_kfold_scores_Classification(clf         = rfc_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 150, algorithm = 'kd_tree', leaf_size = 50, n_jobs=-1) # get object\n",
    "\n",
    "compute_kfold_scores_Classification(clf         = knn_clf,\n",
    "                                    ScaledData  = myData_scaled_classification\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "Build a function text...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function code for detailed regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Regression - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Regression - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Layer Perceptron Regression - Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the advantages of each model\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Who is the Winner????\n",
    "\n",
    "#### Regression\n",
    "\n",
    "Who is the Winner????\n",
    "\n",
    "### Anyone, do research here for python versions of 2ds unit 7.5 material\n",
    "\n",
    "*XXXXXX If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course. XXXXXX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation Part 6 - Which attributes from the analysis are most important?\n",
    "*XXXXXX Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task. XXXXXX*\n",
    "\n",
    "With our models compared and contrasted, it is time to discuss attribute importance in helping us make our classification and regression predictions. It is important to remember that our variables used were not the original data set features themselves but rather linear combinations of our data constituting our principal components. We will be reviewing the importance of these principal components in our models in the form of model coefficients.\n",
    "\n",
    "Another facet of attribute importance we will discuss is that of PCA loadings. As mentioned when performing our in depth review of PCA for both prediction tasks, loadings are a means by which we may understand the underlying factors each principal component represents.\n",
    "\n",
    "These things being said, this section consists of four subsections:\n",
    "\n",
    "1. Classification PCA Importance\n",
    "2. Classification PCA Loadings\n",
    "3. Regression PCA Importance\n",
    "4. Regression PCA Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Importance\n",
    "\n",
    "When performing our detailed PCA for our Customer/Subscriber classifications earlier, we identified the first 14 components as being all that were necessary to describe 80% of the variance in the classification data set. When performing logistic regression using these 14 principal components as inputs, we generated coefficient values by which our components are weighted. The larger the coefficient, the greater the component's influence on positive model response. The larger the value in the negative direction, the greater the component's influence on the negative model response.\n",
    "\n",
    "The coefficient values for each of our principal components are plotted in the horizontal bar chart below. It is immediately apparent that principal components 3 and 10 are weighted heaviest at about 1.75 and 1.65 respectively. Next greatest weights are those of principal components 4 (0.52), 1 (0.49), and 9 (0.42). Interestingly enough, some components are negatively weighted, the greatest of which is principal component 8 at -0.25. While reviewing our logistic regression explanatory variables' (principal components in this case) coefficient values is helpful, it does not help us contextually understand the significance of our original attributes. For this reason, it is important follow up with a review and interpretation of our top weighted principal components' loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=10) \n",
    "matplotlib.rc('ytick', labelsize=10)\n",
    "\n",
    "idx = ['PC'+str(i) for i in range(1,15)]\n",
    "\n",
    "plt.barh(range(len(lr_clf.coef_[0])), coefs[0], color = 'DarkTurquoise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification PCA Loadings\n",
    "\n",
    "Now at the individual principal component level, each classification principal component has 89 loadings which may be esteemed as weights or coefficients representing each original attribute. These loadings represent to what extent each attribute fabricates a given principal component, and the relationship between these attributes in context of the principal component under review. Another perspective is that each principal component is describing an underlying factor which is comprised of the heaviest loadings.\n",
    "\n",
    "Rather than discuss all 14 principal components, we will instead focus on the three principal components with the largest positive coefficients (PC3, PC10, and PC4) as described in the PCA importance plot above as well as the component with the largest coefficient value in the negative direction (PC8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 3 - Early Birds, Saturdays, and General Travel Days/Times *\n",
    "\n",
    "The 3rd principal component, and component with the largest positive coefficient in the logistic regression model, seems to mostly describe the relationship between day of the week and time of the day and how it relates to rider type. Given PC3's large positive coefficient value in the model, it is interesting to see the negative correlation between Saturday rides (-0.58) and Morning rides (0.58). We could hypothesize that if riders are spending their time riding during the morning hours (5am-10am), they are commuting more for work than they are weekend joy riding. Supporting this arguement are the modest positive loadings for the weekdays and the negative loading for Sunday as well. Interestingly enough however, evening, midday, and afternoon rides are also negatively loaded. While more midday and afternoon riding among Customer vs. Subscribers might be expected, the negative loading for evening riding is a bit of a surprise. It would be good to keep an eye on this attribute's loading in the other principal components as well.\n",
    "\n",
    "Note that while the general signs are arbitrary from one principal component to the next, the sign relationship between attribute loadings within the same principal component are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=8)\n",
    "\n",
    "weightsplot = pd.Series(pca_class.components_[2], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 10 - Environmental Conditions and the Days of the Week / Time of Day*\n",
    "\n",
    "We admit we were excited to see the influence of weather on Customer/Subscriber classifications via PC10. Min, max, and average temperatures appear to play a significant role in determining who is a Subscriber and who is only a Customer (temperature loadings approximately -0.50 to -0.51). Not only are the strong negative temperature loadings interesting, but they are negatively correlated with Saturday and Sunday activity (~0.15 and 0.205 respectively). This might suggest that, for example, as the temperature outside increases, subscriber activity decreases on the weekends but increases weekdays (except Fridays). Not only that but night time Subscriber activity (loading of approximately -0.16) increases as well. Remember, the overall signs are arbitrary while signed relationships between attributes are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[9], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DeepSkyBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 4 - Leisure vs. Purpose-Centered Weekend Rides*\n",
    "\n",
    "PC4 is unique in that it requires a respectable amount of domain knowledge to interpret correctly. The most notable loadings are Saturday's 0.6 and Sunday's approximately -0.76 loadings. These are in direct contrast to one another but both occur on the weekend. Noteworthy is also the fact that Morning riding (~0.21) is positively correlated with Saturday riding this time (It was negatively correlated in PC3, but that PC compared weekend vs. weekday riding whereas PC4 focuses primarily on the weekend alone). We might hypothesize that Subscribers might still use the bike share system on Saturdays to run errands, some of which may tend to take place earlier in the day, while Customers tend to joy ride on Sundays, a day when many people tend to relax and take part in recreational activities and other amusements. What isn't surprising is that the day of the week still plays a signficant role in predicting Subscribers vs. Customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[3], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 8 - Beginning vs. End of the Week*\n",
    "\n",
    "The last principal component for which we will be discussing loadings is PC8. Unlike the previous three PCs reviewed, this component had a negative coefficient in the logistic regression model. The contrast in Tuesday (approximately -0.5) and Thursday (~0.62) loadings is a bit perplexing at first glance. When taking into account the other days of the week and even time of day, however, it's easier to put these relationships into perspective. For example, Thursdays through Sundays all have positive loadings whereas Mondays through Wednesdays all have negative loadings. Accompanying these day of the week correlations are time of day. Given this first half of the work week vs. second half relationship it would appear that individuals riding more in the beginning of the week ride more midday, afternoon, and evening than those riding late in the week and on the weekends. Those individuals might tend to ride more in the morning (Thursdays especially) and at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[7], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important detail that is hard to miss is that start and end stations play a very small role in these top principal components. None of these locations portray any significant loadings within these PCs. Start and end location coordinates played little role as well, though larger than individual station names (See PC3 and PC10). Therefore, we believe it safe to say that in terms of CitiBike user types classification, these attributes are less important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELETE REMAINING PCA LOADING SECTIONS IN CLASSIFICATION\n",
    "\n",
    "*Principal Component 1 - Peak vs. Non Peak Commute Times*\n",
    "\n",
    "The first principal component appears to describe the relationships between peak vs. non-peak ride times when people would likely be riding for commutes between work and home vs. leisure rides midday, afternoon, and on the weekends. Specifically, attribute *TimeOfDay_Midday* has a very high loading value around +0.83 while *TimeOfDay_Midday* and *TimeOfDay_Afternoon* have values around -0.35 and -0.35 respectively. While the signs in general are not important from one principal component to the next, the sign relationship between attribute loadings within a component are important. In the case of PC1 it would seem that midday and afternoon ride times are negatively correlated with evening ride times. This is an interesting but expected relationship that would suggest midday, afternoon, and evening ride behaviors (the original TimeOfDay attribute in general really) are important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "weightsplot = pd.Series(pca_class.components_[0], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()\n",
    "\n",
    "print(weightsplot.max() - weightsplot.max()*0.1)\n",
    "print(weightsplot.index[0])\n",
    "\n",
    "display([i for i in weightsplot.index if i > weightsplot.max() - weightsplot.max()*0.1])\n",
    "#pca_classification.fit(myData_scaled_classification)\n",
    "#var = pca_classification.explained_variance_\n",
    "#var1 = pca_classification.explained_variance_ratio_\n",
    "#var2 = np.cumsum(np.round(pca_classification.explained_variance_ratio_, decimals=4))\n",
    "#idx = ['PC'+str(i) for i in range(1,15)]\n",
    "#\n",
    "#PCA_df = {'...Eigenvector...' : pd.Series(var, index = idx),\n",
    "#          '..Variance Proportion..' : pd.Series(var1, index = idx),\n",
    "#          '.Variance Cumulative Proportion.' : pd.Series(var2, index = idx)}\n",
    "#\n",
    "#PCA_df = pd.DataFrame(PCA_df)\n",
    "#print('Classification Task Principal Component Breakdown')\n",
    "#display(PCA_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 2 - Lunch and Afternoon Rides*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[1], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 5 - TGIF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[4], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Gold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 6 - Monday Go Getters, Mid-Week Hump, and Morning Riders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[5], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 7 - Mid-Week Madness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[6], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 8 - T Days*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Principal Component 9 - *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[8], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'RoyalBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[10], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'SlateBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[11], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Indigo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[12], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DarkOrchid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_class.components_[13], index=class_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'Magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression PCA Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_variable_names = CitiBikeDataSampled_5050.drop(['starttime', 'stoptime', 'start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'usertype','tripdurationLog', 'gender', 'gender_0', 'gender_1', 'gender_2', 'birth year', 'Age', 'tripduration', 'DayOfWeek', 'TimeOfDay', 'LinearDistance'], axis=1).columns.values.tolist() \n",
    "lr_variable_names = [i for i in lr_variable_names if \"end_station_name_\" not in i]\n",
    "\n",
    "weightsplot = pd.Series(pca_reg.components_[0], index=reg_att.columns)\n",
    "weightsplot.plot(kind='bar', color = 'DarkRed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[1], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[2], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[3], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[4], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Gold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[5], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[6], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[7], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[8], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'RoyalBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[9], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'DeepSkyBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[10], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'SlateBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[11], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Indigo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[12], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'DarkOrchid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[13], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weightsplot = pd.Series(pca_reg.components_[14], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar', color = 'Gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "##### How useful is the model for interested parties?\n",
    "*XXXXXX (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? XXXXXX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Work for Reuse. Please Delete this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Feature Weightings Explained\n",
    "\n",
    "With our model settled upon for our Logistic Regression, we need to explore the weights associated with each feature to get an idea of how each component impacts the ultimate prediction. Looking at our logistic model directly, we found, interestingly enough, that the highest weight was placed on the latitude of the station where renters began their trips. Having a high positive weight means that subscribers were more likely located at lower latitudes, but based on our heat maps, this seemed to contradict our initial data exploration. Moreover, the next highest weight in terms of magnitude was a specific station, and while it appeared in the top ten percent of our most visited stations, it's odd that it would be so important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "lr_weights_raw = lr_clf_RemABYG.coef_.T # take transpose to make a column vector\n",
    "lr_variable_names = CitiBikeDataSampled_5050.drop(['starttime','stoptime','start_station_id','start_station_name','end_station_id','end_station_name','usertype','gender','DayOfWeek','TimeOfDay','tripduration','Age','gender_0','gender_1','gender_2','birth year'], axis = 1).columns\n",
    "lr_weights = zip(lr_weights_raw,lr_variable_names)\n",
    "\n",
    "lr_weights_top_15_sorted = sorted(lr_weights, key=lambda x: abs(x[0]), reverse=True)[:15]\n",
    "\n",
    "lr_weights_top_15_sorted_coef = []\n",
    "lr_weights_top_15_sorted_names = []\n",
    "for coef, name in lr_weights_top_15_sorted:\n",
    "    lr_weights_top_15_sorted_coef.append(coef[0])\n",
    "    lr_weights_top_15_sorted_names.append(name)\n",
    "\n",
    "weightsplot = pd.Series(lr_clf_RemABYG.coef_[0], index=lr_variable_names)\n",
    "weightsplot.plot(kind='bar')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "\n",
    "top_15_weights_plot = pd.Series(lr_weights_top_15_sorted_coef, index=lr_weights_top_15_sorted_names)\n",
    "top_15_weights_plot.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#for coef, name in zip(lr_weights_raw,lr_variable_names):\n",
    "#    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing our dataset once more, we realized that it was likely our weights were compromised due to the fact that we were dealing with a mix of values and quantities that weren't directly comparable. To get a better understanding of how these features were interacting, we decided to attempt to normalize the features, improving our ability to compare these features across their vastly different scales.\n",
    "\n",
    "This required a refit using the sklearn Standard Scaler training object, but because we're only concerned with the weights and their interactions, we can easily reuse the sets defined in our ShuffleSplit earlier in our regression. However, because we're dealing with scaled and normalized values, we'll use a lower cost value, in this case 0.05 to reflect the expected decrease in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=10) \n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_Train_RemABYG)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_Train_RemABYG)\n",
    "X_test_scaled = scl_obj.transform(x_Test_RemABYG)\n",
    "\n",
    "scalar_lr_clf = LogisticRegression(penalty='l2', C=0.05) # Earlier this cost was 1.0, because we've normalized the dataset, we now can use a lower cost\n",
    "scalar_lr_clf.fit(X_train_scaled,y_Train_RemABYG)\n",
    "\n",
    "scalar_y_hat = scalar_lr_clf.predict(X_test_scaled)\n",
    "\n",
    "scalar_acc = mt.accuracy_score(y_Test_RemABYG, y_hat_RemABYG)\n",
    "scalar_conf = mt.confusion_matrix(y_Test_RemABYG, y_hat_RemABYG)\n",
    "print('accuracy:', scalar_acc)\n",
    "print(scalar_conf)\n",
    "\n",
    "\n",
    "scalar_weights = zip(scalar_lr_clf.coef_[0],lr_variable_names)\n",
    "\n",
    "scalar_weights_top_15_sorted = sorted(scalar_weights, key=lambda x: abs(x[0]), reverse=True)[:15]\n",
    "\n",
    "scalar_weights_top_15_sorted_coef = []\n",
    "scalar_weights_top_15_sorted_names = []\n",
    "for coef, name in scalar_weights_top_15_sorted:\n",
    "    scalar_weights_top_15_sorted_coef.append(coef)\n",
    "    scalar_weights_top_15_sorted_names.append(name)\n",
    "\n",
    "scalar_weights = pd.Series(scalar_lr_clf.coef_[0],index=lr_variable_names)\n",
    "scalar_weights.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and prediction matrix closely resembles our earlier regression and SVM models, meaning that the act of normalization did not greatly impact our model, however it did make understanding the weights much easier and, surprisingly, closely resembled some of our previous hypotheses we posited during our inital data gathering.\n",
    "\n",
    "Perhaps one unique component of these new weights is that the log transform of the trip duration has a negative weight, nearly 1, with the subscriber class. This implies that subscribers spend very little time on their trip, despite the linear distance between stations having a high positive weight. These two features seem to correlate with our earlier observations that subscribers were typically seen commuting rather than touring using the bikes.\n",
    "\n",
    "Another interesting aspect is the temperature component created after merging Citi Bike rental data with historical weather data. Both the maximum temperature and the minimum temperature have a negative weight with the subscriber class, however the average temperature has a positive weight. If subscribers are commuting, then it might make sense that as the average temperature for the day rises, they're much more likely to make use of their subscription. As it's possible they commute regardless of weather, extreme weathers are not as likely to impact their riding habits and behaviors.\n",
    "\n",
    "Finally, a few more quick observations based entirely on weight - saturday and sunday rentals both have negative weights in relationship to subscribers, further building upon the idea that subscribers are also generally commuters, and mornings also have a positive weight in regards to subscriber use. This might mean that subscribers are more likely to ride earlier in the morning on weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "\n",
    "scalar_top_15_weights_plot = pd.Series(scalar_weights_top_15_sorted_coef, index=scalar_weights_top_15_sorted_names)\n",
    "scalar_top_15_weights_plot.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "#zip_vars = zip(scalar_lr_clf.coef_.T,lr_variable_names) # combine attributes\n",
    "#zip_vars = sorted(zip_vars)\n",
    "\n",
    "#for coef, name in zip_vars:\n",
    "#    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of LR vs. SVM\n",
    "Both Logisitic Regression and Support Vector Machines present advantages as well as disadvantages. As might be expected, which approach is best depends on one's data set and problem statement. Our use of both model types with the same data sample set allows us to break down these pros and cons in detail and determine which produces a better fit given our situation. With this in mind, each model type's general advantages and disadvantages are outlined and discussed below.\n",
    "\n",
    "**General Advantages**\n",
    "\n",
    "*Logisitic Regression*\n",
    "- *Simplicity* - The logistic regression model is built in such a way that independent variables are plotted along an x-axis and single-class classification is represented by 0 or 1 on the y-axis. The logit and cost functions are tuned to accurately fit a logistic curve to the data on this x-y plane. By evaluating P(X) via the logit function, we are able to easily determine whether the class is 0 (Customer), in cases when P < 0.5, or 1 (Subscriber), in cases in which P > 0.5. In other words, logistic regression fits all data points as though they were positioned along a continuous function.\n",
    "\n",
    "- *Small Dimensionality Applications* - Logistic regression works well when there are a smaller number of dimensions and when these attributes do not necessarily determine the response without considering probability of response.\n",
    "\n",
    "*SVM*\n",
    "- *Flexible* - Since SVM is capable of fitting a hyperplane among an infinite number of dimensions to create a boundary between classes of various data points, it allows for seperation of data from varying classes even when data sets are very large or contain many attributes.\n",
    "\n",
    "- *Robust* - In general, SVM is more robust against outliers since it only considers points near the margin as support vectors.\n",
    "\n",
    "- *Speed (SVM training with Stochastic Gradient Descent)* - Because stochastic gradient descent processes the data in batches and is less precise in it's batch-by-batch approach to deepest descent but still arrives there nonetheless in the end, it is significantly faster than logistic regression and SVM on its own (less resource instensive).\n",
    "\n",
    "**General Disadvantages**\n",
    "\n",
    "*Logistic Regression*\n",
    "- *Speed* - Logistic regression, though faster than straightforward SVM for our data set, is slower in comparison to training a classfier model using stochastic gradient descent.\n",
    "\n",
    "- *Sensitivity* - In cases when P = ~0.5, logistic regression has difficulty determining classification.\n",
    "\n",
    "*SVM*\n",
    "- *Speed (SVM by itself)* - Fitting a SVM model on its own was very slow for our data set. Even after 14 hours of run-time on a quad-core i7 workstation, not even a single model was completely generated.\n",
    "\n",
    "**The Winner**\n",
    "\n",
    "Given the general advantages and disadvantages above, we'd now like to discuss both model approaches with respect to our data. These discussions are centered around the data set version in which only Age, Birth Year, and Gender are removed from the originally modeled set. As mentioned above, linear SVM training with stochastic gradient descent for 3x iterations was much faster for our selected data set (~39s) vs. native logistic regression training (~1 min 24s). This time difference is expected to increase exponentially as the number of rides analyzed increases, giving SVM with stochastic gradient descent the upper edge. <span style=\"color:green\">*+1 for SVM with stochastic gradient descent*</span>\n",
    "\n",
    "On the other hand, however, logistic regression produced accuracy results of {74.52, 74.49, 74.73} % across the three test/train iterations whereas SVM with stochastic gradient descent exhibited accuracy results of {74.121, 74.339, 74.44} %. This slight increase in accuracy among logistic regression models is an interesting observation, especially given logistic regression's added simplicity vs. SVM modelling. A second measure of accuracy employed throughout this analysis has been log-loss. When comparing log-loss among both model types, we expect log-loss values to correspond with model accuracy measures also. As such, logistic regression log-loss values were {8.80, 8.75, 8.73} for each of the three iterations whereas SVM with stochastic gradient values were {8.94, 8.86, 8.83}. As expected, these results correlate with the accuracy scores discussed. <span style=\"color:blue\">*+1 for logistic regression*</span>\n",
    "\n",
    "The final comparison worth further discussion is that of efficiency. As mentioned in the general advantages and disadvantages above, logistic regression consists of a simpler model than SVM, especially when factoring in stochastic gradient descent to SVM model training. Logistic regression classifies our CitiBike user types by optimizing the log likelihood function; SVM tries to maximize the margin between the closest data points of two classification regions. The fact that stochastic gradient descent is required given our data set dimensions and large number of observations means the model generation process may be deemed less efficient. In the end, the principles of Occam's Razor apply. Therefore, logistic regression without stochastic gradient descent takes the win for our data set.<span style=\"color:blue\">*+1 for logistic regression*</span>\n",
    "\n",
    "Given its increased accuracy and efficiency/simplicity, we rate logistic regression as being the better model for user type classification using our sample data set of 500,000 rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Py3]",
   "language": "python",
   "name": "conda-env-Py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
